{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffa235c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 22, 92]           1,792\n",
      "       BatchNorm2d-2           [-1, 64, 22, 92]             128\n",
      "              ReLU-3           [-1, 64, 22, 92]               0\n",
      "         MaxPool3d-4           [-1, 64, 20, 90]               0\n",
      "            Conv2d-5           [-1, 32, 20, 90]           2,080\n",
      "              ReLU-6           [-1, 32, 20, 90]               0\n",
      "            Conv2d-7           [-1, 32, 20, 90]           3,104\n",
      "              ReLU-8           [-1, 32, 20, 90]               0\n",
      "            Conv2d-9           [-1, 32, 20, 90]           3,104\n",
      "             ReLU-10           [-1, 32, 20, 90]               0\n",
      "           Conv2d-11          [-1, 128, 20, 90]           4,224\n",
      "small_basic_block-12          [-1, 128, 20, 90]               0\n",
      "      BatchNorm2d-13          [-1, 128, 20, 90]             256\n",
      "             ReLU-14          [-1, 128, 20, 90]               0\n",
      "        MaxPool3d-15           [-1, 64, 18, 44]               0\n",
      "           Conv2d-16           [-1, 64, 18, 44]           4,160\n",
      "             ReLU-17           [-1, 64, 18, 44]               0\n",
      "           Conv2d-18           [-1, 64, 18, 44]          12,352\n",
      "             ReLU-19           [-1, 64, 18, 44]               0\n",
      "           Conv2d-20           [-1, 64, 18, 44]          12,352\n",
      "             ReLU-21           [-1, 64, 18, 44]               0\n",
      "           Conv2d-22          [-1, 256, 18, 44]          16,640\n",
      "small_basic_block-23          [-1, 256, 18, 44]               0\n",
      "      BatchNorm2d-24          [-1, 256, 18, 44]             512\n",
      "             ReLU-25          [-1, 256, 18, 44]               0\n",
      "           Conv2d-26           [-1, 64, 18, 44]          16,448\n",
      "             ReLU-27           [-1, 64, 18, 44]               0\n",
      "           Conv2d-28           [-1, 64, 18, 44]          12,352\n",
      "             ReLU-29           [-1, 64, 18, 44]               0\n",
      "           Conv2d-30           [-1, 64, 18, 44]          12,352\n",
      "             ReLU-31           [-1, 64, 18, 44]               0\n",
      "           Conv2d-32          [-1, 256, 18, 44]          16,640\n",
      "small_basic_block-33          [-1, 256, 18, 44]               0\n",
      "      BatchNorm2d-34          [-1, 256, 18, 44]             512\n",
      "             ReLU-35          [-1, 256, 18, 44]               0\n",
      "        MaxPool3d-36           [-1, 64, 16, 21]               0\n",
      "          Dropout-37           [-1, 64, 16, 21]               0\n",
      "           Conv2d-38          [-1, 256, 16, 18]          65,792\n",
      "      BatchNorm2d-39          [-1, 256, 16, 18]             512\n",
      "             ReLU-40          [-1, 256, 16, 18]               0\n",
      "          Dropout-41          [-1, 256, 16, 18]               0\n",
      "           Conv2d-42            [-1, 37, 4, 18]         123,173\n",
      "      BatchNorm2d-43            [-1, 37, 4, 18]              74\n",
      "             ReLU-44            [-1, 37, 4, 18]               0\n",
      "           Conv2d-45            [-1, 37, 4, 18]          17,982\n",
      "================================================================\n",
      "Total params: 326,541\n",
      "Trainable params: 326,541\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.03\n",
      "Forward/backward pass size (MB): 33.57\n",
      "Params size (MB): 1.25\n",
      "Estimated Total Size (MB): 34.84\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from model.LPRNet import build_lprnet\n",
    "from torchsummary import summary\n",
    "\n",
    "model = build_lprnet()\n",
    "summary(model, input_size=(3, 24, 94))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2709cf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import string\n",
    "\n",
    "class ImageFolderCTCDataset(Dataset):\n",
    "    def __init__(self, folder_path, image_shape=(3, 24, 94)):\n",
    "        self.folder_path = folder_path\n",
    "        self.image_files = os.listdir(folder_path)\n",
    "        _, height, width = image_shape\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((height, width)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        # Build dictionary\n",
    "        self.chars = list(string.digits + string.ascii_uppercase)\n",
    "        self.chars.append('-')  # blank for CTC\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.folder_path, img_name)\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # Extract label from filename (remove ext, cut at \"_\")\n",
    "        label_str = img_name.split('.')[0].split('_')[0]\n",
    "\n",
    "        # Encode string -> list of indices\n",
    "        label_encoded = [self.char_to_idx[ch] for ch in label_str]\n",
    "\n",
    "        return image, torch.tensor(label_encoded, dtype=torch.long), len(label_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70ae9664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 24, 94])\n",
      "tensor([ 0,  0, 21, 17,  2,  8,  7,  7])\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SHAPE = (3, 24, 94)\n",
    "train_ds = ImageFolderCTCDataset(r\"lprds\\train\", image_shape=IMAGE_SHAPE)\n",
    "val_ds = ImageFolderCTCDataset(r\"lprds\\val\", image_shape=IMAGE_SHAPE)\n",
    "test_ds = ImageFolderCTCDataset(r\"lprds\\test\", image_shape=IMAGE_SHAPE)\n",
    "\n",
    "img, label_encoded, label_length = train_ds[0]\n",
    "\n",
    "print(img.shape)           # torch.Size([3, 24, 94])\n",
    "print(label_encoded)       # tensor([ 0,  0, 21, 17,  2,  8,  7,  7])\n",
    "print(label_length)        # 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df72ba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def ctc_collate_fn(batch):\n",
    "    images, labels, lengths = zip(*batch)  # unzip list of tuples\n",
    "\n",
    "    # Stack images [B, C, H, W]\n",
    "    images = torch.stack(images, dim=0)\n",
    "\n",
    "    # Concatenate all labels into one flat tensor\n",
    "    labels = torch.cat(labels)\n",
    "\n",
    "    # Convert lengths to tensor\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "\n",
    "    return images, labels, lengths\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=ctc_collate_fn)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=ctc_collate_fn)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=ctc_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35cbe796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 24, 94])\n",
      "torch.Size([227])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=ctc_collate_fn)\n",
    "val_loader   = DataLoader(val_ds, batch_size=32, shuffle=False, collate_fn=ctc_collate_fn)\n",
    "test_loader  = DataLoader(test_ds, batch_size=32, shuffle=False, collate_fn=ctc_collate_fn)\n",
    "\n",
    "# check a batch\n",
    "images, labels, lengths = next(iter(train_loader))\n",
    "print(images.shape)   # [32, 3, 24, 94]\n",
    "print(labels.shape)   # flat 1D tensor, e.g. torch.Size([180])\n",
    "print(lengths.shape)  # [32], lengths of each label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3d41681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_lengths_from_logits(logits):\n",
    "    # logits: (N, class_num, T)\n",
    "    T = logits.size(2)\n",
    "    batch_size = logits.size(0)\n",
    "    # CTC expects input_lengths per sample (length of T for each sample)\n",
    "    return torch.full((batch_size,), T, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6420c562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Step [10/19], Loss: 5.6086\n",
      "Epoch [1] Validation Loss: 52.4369\n",
      "✅ Saved best model at epoch 1 with val_loss=52.4369\n",
      "Epoch [2], Step [10/19], Loss: 3.7520\n",
      "Epoch [2] Validation Loss: 13.6140\n",
      "✅ Saved best model at epoch 2 with val_loss=13.6140\n",
      "Epoch [3], Step [10/19], Loss: 3.6394\n",
      "Epoch [3] Validation Loss: 7.6765\n",
      "✅ Saved best model at epoch 3 with val_loss=7.6765\n",
      "Epoch [4], Step [10/19], Loss: 3.5888\n",
      "Epoch [4] Validation Loss: 5.5754\n",
      "✅ Saved best model at epoch 4 with val_loss=5.5754\n",
      "Epoch [5], Step [10/19], Loss: 3.5460\n",
      "Epoch [5] Validation Loss: 3.6545\n",
      "✅ Saved best model at epoch 5 with val_loss=3.6545\n",
      "Epoch [6], Step [10/19], Loss: 3.4927\n",
      "Epoch [6] Validation Loss: 3.8135\n",
      "Epoch [7], Step [10/19], Loss: 3.3886\n",
      "Epoch [7] Validation Loss: 3.7435\n",
      "Epoch [8], Step [10/19], Loss: 3.3040\n",
      "Epoch [8] Validation Loss: 3.5553\n",
      "✅ Saved best model at epoch 8 with val_loss=3.5553\n",
      "Epoch [9], Step [10/19], Loss: 3.0602\n",
      "Epoch [9] Validation Loss: 3.2087\n",
      "✅ Saved best model at epoch 9 with val_loss=3.2087\n",
      "Epoch [10], Step [10/19], Loss: 2.8720\n",
      "Epoch [10] Validation Loss: 3.6453\n",
      "Epoch [11], Step [10/19], Loss: 2.6615\n",
      "Epoch [11] Validation Loss: 2.8090\n",
      "✅ Saved best model at epoch 11 with val_loss=2.8090\n",
      "Epoch [12], Step [10/19], Loss: 2.2180\n",
      "Epoch [12] Validation Loss: 2.8165\n",
      "Epoch [13], Step [10/19], Loss: 1.9919\n",
      "Epoch [13] Validation Loss: 2.1699\n",
      "✅ Saved best model at epoch 13 with val_loss=2.1699\n",
      "Epoch [14], Step [10/19], Loss: 1.6250\n",
      "Epoch [14] Validation Loss: 1.8648\n",
      "✅ Saved best model at epoch 14 with val_loss=1.8648\n",
      "Epoch [15], Step [10/19], Loss: 1.4626\n",
      "Epoch [15] Validation Loss: 1.7674\n",
      "✅ Saved best model at epoch 15 with val_loss=1.7674\n",
      "Epoch [16], Step [10/19], Loss: 1.2015\n",
      "Epoch [16] Validation Loss: 1.3265\n",
      "✅ Saved best model at epoch 16 with val_loss=1.3265\n",
      "Epoch [17], Step [10/19], Loss: 1.0717\n",
      "Epoch [17] Validation Loss: 1.1918\n",
      "✅ Saved best model at epoch 17 with val_loss=1.1918\n",
      "Epoch [18], Step [10/19], Loss: 0.8995\n",
      "Epoch [18] Validation Loss: 1.1038\n",
      "✅ Saved best model at epoch 18 with val_loss=1.1038\n",
      "Epoch [19], Step [10/19], Loss: 0.8266\n",
      "Epoch [19] Validation Loss: 1.2020\n",
      "Epoch [20], Step [10/19], Loss: 0.8558\n",
      "Epoch [20] Validation Loss: 1.4833\n",
      "Epoch [21], Step [10/19], Loss: 0.9072\n",
      "Epoch [21] Validation Loss: 1.1082\n",
      "Epoch [22], Step [10/19], Loss: 0.6874\n",
      "Epoch [22] Validation Loss: 1.0357\n",
      "✅ Saved best model at epoch 22 with val_loss=1.0357\n",
      "Epoch [23], Step [10/19], Loss: 0.6211\n",
      "Epoch [23] Validation Loss: 1.0452\n",
      "Epoch [24], Step [10/19], Loss: 0.6713\n",
      "Epoch [24] Validation Loss: 0.9314\n",
      "✅ Saved best model at epoch 24 with val_loss=0.9314\n",
      "Epoch [25], Step [10/19], Loss: 0.5361\n",
      "Epoch [25] Validation Loss: 0.9814\n",
      "Epoch [26], Step [10/19], Loss: 0.5302\n",
      "Epoch [26] Validation Loss: 0.9185\n",
      "✅ Saved best model at epoch 26 with val_loss=0.9185\n",
      "Epoch [27], Step [10/19], Loss: 0.5371\n",
      "Epoch [27] Validation Loss: 0.8195\n",
      "✅ Saved best model at epoch 27 with val_loss=0.8195\n",
      "Epoch [28], Step [10/19], Loss: 0.4078\n",
      "Epoch [28] Validation Loss: 0.6895\n",
      "✅ Saved best model at epoch 28 with val_loss=0.6895\n",
      "Epoch [29], Step [10/19], Loss: 0.4665\n",
      "Epoch [29] Validation Loss: 0.8764\n",
      "Epoch [30], Step [10/19], Loss: 0.4425\n",
      "Epoch [30] Validation Loss: 0.9371\n",
      "Epoch [31], Step [10/19], Loss: 0.4300\n",
      "Epoch [31] Validation Loss: 1.0418\n",
      "Epoch [32], Step [10/19], Loss: 0.4213\n",
      "Epoch [32] Validation Loss: 0.7273\n",
      "Epoch [33], Step [10/19], Loss: 0.3530\n",
      "Epoch [33] Validation Loss: 0.8264\n",
      "Epoch [34], Step [10/19], Loss: 0.3121\n",
      "Epoch [34] Validation Loss: 0.8253\n",
      "Epoch [35], Step [10/19], Loss: 0.4121\n",
      "Epoch [35] Validation Loss: 0.7161\n",
      "Epoch [36], Step [10/19], Loss: 0.3546\n",
      "Epoch [36] Validation Loss: 0.7679\n",
      "Epoch [37], Step [10/19], Loss: 0.2665\n",
      "Epoch [37] Validation Loss: 0.7830\n",
      "Epoch [38], Step [10/19], Loss: 0.2982\n",
      "Epoch [38] Validation Loss: 0.8260\n",
      "Epoch [39], Step [10/19], Loss: 0.3264\n",
      "Epoch [39] Validation Loss: 0.7556\n",
      "Epoch [40], Step [10/19], Loss: 0.2631\n",
      "Epoch [40] Validation Loss: 0.7114\n",
      "Epoch [41], Step [10/19], Loss: 0.2983\n",
      "Epoch [41] Validation Loss: 0.7224\n",
      "Epoch [42], Step [10/19], Loss: 0.3265\n",
      "Epoch [42] Validation Loss: 0.9134\n",
      "Epoch [43], Step [10/19], Loss: 0.2301\n",
      "Epoch [43] Validation Loss: 0.7527\n",
      "Epoch [44], Step [10/19], Loss: 0.2149\n",
      "Epoch [44] Validation Loss: 0.7050\n",
      "Epoch [45], Step [10/19], Loss: 0.1950\n",
      "Epoch [45] Validation Loss: 0.7736\n",
      "Epoch [46], Step [10/19], Loss: 0.2002\n",
      "Epoch [46] Validation Loss: 0.7747\n",
      "Epoch [47], Step [10/19], Loss: 0.2513\n",
      "Epoch [47] Validation Loss: 0.8554\n",
      "Epoch [48], Step [10/19], Loss: 0.2101\n",
      "Epoch [48] Validation Loss: 0.6734\n",
      "✅ Saved best model at epoch 48 with val_loss=0.6734\n",
      "Epoch [49], Step [10/19], Loss: 0.2093\n",
      "Epoch [49] Validation Loss: 0.7973\n",
      "Epoch [50], Step [10/19], Loss: 0.1994\n",
      "Epoch [50] Validation Loss: 0.7159\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# -------- Config ----------\n",
    "CLASS_NUM = 37              # number of classes (0-9, A-Z, plus blank)\n",
    "MAX_LABEL_LEN = 10          # max characters per sample (fake, for model design)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LR = 1e-3\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "# --------------------------\n",
    "\n",
    "# ===== Dataset & DataLoader =====\n",
    "def ctc_collate_fn(batch):\n",
    "    images, labels, lengths = zip(*batch)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    labels = torch.cat(labels)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "    return images, labels, lengths\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=ctc_collate_fn)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=ctc_collate_fn)\n",
    "\n",
    "# ===== Model, Loss, Optimizer =====\n",
    "model = build_lprnet(MAX_LABEL_LEN, CLASS_NUM).to(DEVICE)\n",
    "criterion = nn.CTCLoss(blank=train_ds.char_to_idx['-'], reduction='mean')\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "# ===== Training & Validation =====\n",
    "def train_one_epoch(epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (images, labels, target_lengths) in enumerate(train_loader):\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        target_lengths = target_lengths.to(DEVICE)\n",
    "\n",
    "        # Forward\n",
    "        logits = model(images)                     # [N, C, T]\n",
    "        logits = logits.permute(2, 0, 1)           # [T, N, C]\n",
    "        log_probs = logits.log_softmax(2)\n",
    "\n",
    "        # Input lengths = all T\n",
    "        input_lengths = torch.full(size=(images.size(0),), \n",
    "                                   fill_value=logits.size(0), \n",
    "                                   dtype=torch.long).to(DEVICE)\n",
    "\n",
    "        # Loss\n",
    "        loss = criterion(log_probs, labels, input_lengths, target_lengths)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (batch_idx+1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}], Step [{batch_idx+1}/{len(train_loader)}], \"\n",
    "                  f\"Loss: {running_loss/10:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "def validate(epoch):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, target_lengths in val_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            target_lengths = target_lengths.to(DEVICE)\n",
    "\n",
    "            logits = model(images)\n",
    "            logits = logits.permute(2, 0, 1)\n",
    "            log_probs = logits.log_softmax(2)\n",
    "\n",
    "            input_lengths = torch.full(size=(images.size(0),), \n",
    "                                       fill_value=logits.size(0), \n",
    "                                       dtype=torch.long).to(DEVICE)\n",
    "\n",
    "            loss = criterion(log_probs, labels, input_lengths, target_lengths)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Epoch [{epoch+1}] Validation Loss: {val_loss:.4f}\")\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "# ===== Main Loop =====\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_one_epoch(epoch)\n",
    "    val_loss = validate(epoch)\n",
    "\n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"lprnet_best.pth\")\n",
    "        print(f\"✅ Saved best model at epoch {epoch+1} with val_loss={val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2688ad0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6498\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.649839868148168"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate_test():\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, target_lengths in test_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            target_lengths = target_lengths.to(DEVICE)\n",
    "\n",
    "            logits = model(images)\n",
    "            logits = logits.permute(2, 0, 1)\n",
    "            log_probs = logits.log_softmax(2)\n",
    "\n",
    "            input_lengths = torch.full(size=(images.size(0),), \n",
    "                                       fill_value=logits.size(0), \n",
    "                                       dtype=torch.long).to(DEVICE)\n",
    "\n",
    "            loss = criterion(log_probs, labels, input_lengths, target_lengths)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Test Loss: {val_loss:.4f}\")\n",
    "    return val_loss\n",
    "validate_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff615b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 18])\n"
     ]
    }
   ],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(\"lprnet_best.pth\", map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=ctc_collate_fn)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels, target_lengths in test_loader:\n",
    "        images = images.to(DEVICE)\n",
    "        logits = model(images)                     # [N, C, T]\n",
    "        preds = logits.argmax(1)                   # simple greedy decode (still needs CTC decoding)\n",
    "        print(preds.shape)  # torch.Size([32, 18])\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39ac89d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(logits, idx_to_char, blank_idx):\n",
    "    \"\"\"\n",
    "    logits: [T, N, C] tensor (log probs or raw logits)\n",
    "    idx_to_char: dictionary mapping int -> char\n",
    "    blank_idx: index of blank symbol\n",
    "    \"\"\"\n",
    "    preds = logits.argmax(2).permute(1, 0)   # [N, T]\n",
    "\n",
    "    results = []\n",
    "    for pred in preds:\n",
    "        string = \"\"\n",
    "        prev = None\n",
    "        for p in pred.cpu().numpy():\n",
    "            if p != prev and p != blank_idx:   # collapse + remove blank\n",
    "                string += idx_to_char[p]\n",
    "            prev = p\n",
    "        results.append(string)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4df15a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(model, image, dataset):\n",
    "    \"\"\"\n",
    "    model   : trained model\n",
    "    image   : tensor [3, 24, 94]\n",
    "    dataset : dataset object (for idx_to_char, blank index)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image = image.unsqueeze(0).to(DEVICE)        # add batch dim [1, 3, 24, 94]\n",
    "        logits = model(image)                        # [N, C, T]\n",
    "        logits = logits.permute(2, 0, 1)             # [T, N, C]\n",
    "\n",
    "        preds = greedy_decode(logits, dataset.idx_to_char, dataset.char_to_idx['-'])\n",
    "        return preds[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b06d5e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth: 75N1960G.png\n",
      "Prediction  : 75N19600\n"
     ]
    }
   ],
   "source": [
    "# Take a sample from your test set\n",
    "i = 9\n",
    "img, _, _ = test_ds[i]\n",
    "print(\"Ground truth:\", test_ds.image_files[i])\n",
    "\n",
    "pred = predict_image(model, img, test_ds)\n",
    "print(\"Prediction  :\", pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab907b93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lprnet-v1 (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
