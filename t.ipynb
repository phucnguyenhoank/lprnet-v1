{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa235c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 8, 18, 88]             400\n",
      "         MaxPool2d-2             [-1, 8, 9, 44]               0\n",
      "              ReLU-3             [-1, 8, 9, 44]               0\n",
      "            Conv2d-4            [-1, 10, 5, 40]           2,010\n",
      "         MaxPool2d-5            [-1, 10, 2, 20]               0\n",
      "              ReLU-6            [-1, 10, 2, 20]               0\n",
      "            Linear-7                   [-1, 32]          12,832\n",
      "              ReLU-8                   [-1, 32]               0\n",
      "            Linear-9                    [-1, 6]             198\n",
      "           Conv2d-10            [-1, 3, 24, 94]               6\n",
      "             ReLU-11            [-1, 3, 24, 94]               0\n",
      "           Conv2d-12           [-1, 64, 22, 92]           1,792\n",
      "      BatchNorm2d-13           [-1, 64, 22, 92]             128\n",
      "             ReLU-14           [-1, 64, 22, 92]               0\n",
      "        MaxPool3d-15           [-1, 64, 20, 90]               0\n",
      "           Conv2d-16           [-1, 32, 20, 90]           2,080\n",
      "             ReLU-17           [-1, 32, 20, 90]               0\n",
      "           Conv2d-18           [-1, 32, 20, 90]           3,104\n",
      "             ReLU-19           [-1, 32, 20, 90]               0\n",
      "           Conv2d-20           [-1, 32, 20, 90]           3,104\n",
      "             ReLU-21           [-1, 32, 20, 90]               0\n",
      "           Conv2d-22          [-1, 128, 20, 90]           4,224\n",
      "small_basic_block-23          [-1, 128, 20, 90]               0\n",
      "      BatchNorm2d-24          [-1, 128, 20, 90]             256\n",
      "             ReLU-25          [-1, 128, 20, 90]               0\n",
      "        MaxPool3d-26           [-1, 64, 18, 44]               0\n",
      "           Conv2d-27           [-1, 64, 18, 44]           4,160\n",
      "             ReLU-28           [-1, 64, 18, 44]               0\n",
      "           Conv2d-29           [-1, 64, 18, 44]          12,352\n",
      "             ReLU-30           [-1, 64, 18, 44]               0\n",
      "           Conv2d-31           [-1, 64, 18, 44]          12,352\n",
      "             ReLU-32           [-1, 64, 18, 44]               0\n",
      "           Conv2d-33          [-1, 256, 18, 44]          16,640\n",
      "small_basic_block-34          [-1, 256, 18, 44]               0\n",
      "      BatchNorm2d-35          [-1, 256, 18, 44]             512\n",
      "             ReLU-36          [-1, 256, 18, 44]               0\n",
      "           Conv2d-37           [-1, 64, 18, 44]          16,448\n",
      "             ReLU-38           [-1, 64, 18, 44]               0\n",
      "           Conv2d-39           [-1, 64, 18, 44]          12,352\n",
      "             ReLU-40           [-1, 64, 18, 44]               0\n",
      "           Conv2d-41           [-1, 64, 18, 44]          12,352\n",
      "             ReLU-42           [-1, 64, 18, 44]               0\n",
      "           Conv2d-43          [-1, 256, 18, 44]          16,640\n",
      "small_basic_block-44          [-1, 256, 18, 44]               0\n",
      "      BatchNorm2d-45          [-1, 256, 18, 44]             512\n",
      "             ReLU-46          [-1, 256, 18, 44]               0\n",
      "        MaxPool3d-47           [-1, 64, 16, 21]               0\n",
      "          Dropout-48           [-1, 64, 16, 21]               0\n",
      "           Conv2d-49          [-1, 256, 16, 18]          65,792\n",
      "      BatchNorm2d-50          [-1, 256, 16, 18]             512\n",
      "             ReLU-51          [-1, 256, 16, 18]               0\n",
      "          Dropout-52          [-1, 256, 16, 18]               0\n",
      "           Conv2d-53            [-1, 37, 4, 18]         123,173\n",
      "      BatchNorm2d-54            [-1, 37, 4, 18]              74\n",
      "             ReLU-55            [-1, 37, 4, 18]               0\n",
      "           Conv2d-56            [-1, 37, 4, 18]          17,982\n",
      "================================================================\n",
      "Total params: 341,987\n",
      "Trainable params: 341,987\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 33.84\n",
      "Params size (MB): 1.30\n",
      "Estimated Total Size (MB): 35.16\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from model.LPRNet import build_lprnet\n",
    "from torchsummary import summary\n",
    "\n",
    "model = build_lprnet()\n",
    "summary(model, input_size=(1, 24, 94))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2709cf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import string\n",
    "\n",
    "class ImageFolderCTCDataset(Dataset):\n",
    "    def __init__(self, folder_path, image_shape=(1, 24, 94), augment=False):\n",
    "        self.folder_path = folder_path\n",
    "        self.image_files = os.listdir(folder_path)\n",
    "        _, height, width = image_shape\n",
    "\n",
    "        if augment:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((height, width)),\n",
    "                transforms.RandomAffine(\n",
    "                    degrees=5,\n",
    "                    translate=(0.05, 0.05),\n",
    "                    scale=(0.9, 1.1),\n",
    "                    shear=0\n",
    "                ),\n",
    "                transforms.Grayscale(num_output_channels=1),\n",
    "                transforms.ToTensor()   # grayscale tensor in [0,1]\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((height, width)),\n",
    "                transforms.Grayscale(num_output_channels=1),\n",
    "                transforms.ToTensor()   # grayscale tensor in [0,1]\n",
    "            ])\n",
    "\n",
    "        # dictionary build\n",
    "        self.chars = list(string.digits + string.ascii_uppercase)\n",
    "        self.chars.append('-')\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.folder_path, img_name)\n",
    "\n",
    "        image = Image.open(img_path).convert(\"L\")  # grayscale\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # Extract label\n",
    "        label_str = img_name.split('.')[0].split('_')[0]\n",
    "        label_encoded = [self.char_to_idx[ch] for ch in label_str]\n",
    "\n",
    "        return image, torch.tensor(label_encoded, dtype=torch.long), len(label_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ae9664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 24, 94])\n",
      "tensor([ 0,  0, 21, 17,  2,  8,  7,  7])\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SHAPE = (3, 24, 94)\n",
    "data_set_folder = r\"LPRNet_Pytorch/data/split/\"\n",
    "train_ds = ImageFolderCTCDataset(data_set_folder + \"/train\", image_shape=IMAGE_SHAPE, augment=True)\n",
    "val_ds = ImageFolderCTCDataset(data_set_folder + \"/val\", image_shape=IMAGE_SHAPE)\n",
    "test_ds = ImageFolderCTCDataset(data_set_folder + \"/test\", image_shape=IMAGE_SHAPE)\n",
    "\n",
    "img, label_encoded, label_length = train_ds[0]\n",
    "\n",
    "print(img.shape)           # torch.Size([3, 24, 94])\n",
    "print(label_encoded)       # tensor([ 0,  0, 21, 17,  2,  8,  7,  7])\n",
    "print(label_length)        # 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df72ba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def ctc_collate_fn(batch):\n",
    "    images, labels, lengths = zip(*batch)  # unzip list of tuples\n",
    "\n",
    "    # Stack images [B, C, H, W]\n",
    "    images = torch.stack(images, dim=0)\n",
    "\n",
    "    # Concatenate all labels into one flat tensor\n",
    "    labels = torch.cat(labels)\n",
    "\n",
    "    # Convert lengths to tensor\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "\n",
    "    return images, labels, lengths\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=ctc_collate_fn)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=ctc_collate_fn)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=ctc_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35cbe796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 24, 94])\n",
      "torch.Size([219])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# check a batch\n",
    "images, labels, lengths = next(iter(train_loader))\n",
    "print(images.shape)   # [32, 3, 24, 94]\n",
    "print(labels.shape)   # flat 1D tensor, e.g. torch.Size([180])\n",
    "print(lengths.shape)  # [32], lengths of each label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a66da7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# -------- Config ----------\n",
    "CLASS_NUM = 37              # number of classes (0-9, A-Z, plus blank)\n",
    "MAX_LABEL_LEN = 10          # max characters per sample (fake, for model design)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LR = 1e-3\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "# --------------------------\n",
    "\n",
    "# ===== Dataset & DataLoader =====\n",
    "def ctc_collate_fn(batch):\n",
    "    images, labels, lengths = zip(*batch)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    labels = torch.cat(labels)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "    return images, labels, lengths\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=ctc_collate_fn)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=ctc_collate_fn)\n",
    "\n",
    "# ===== Model, Loss, Optimizer =====\n",
    "model = build_lprnet(MAX_LABEL_LEN, CLASS_NUM).to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"lprnet_best.pth\", map_location=DEVICE))\n",
    "\n",
    "criterion = nn.CTCLoss(blank=train_ds.char_to_idx['-'], reduction='mean')\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "# ===== Training & Validation =====\n",
    "def train_one_epoch(epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (images, labels, target_lengths) in enumerate(train_loader):\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        target_lengths = target_lengths.to(DEVICE)\n",
    "\n",
    "        # Forward\n",
    "        logits = model(images)                     # [N, C, T]\n",
    "        logits = logits.permute(2, 0, 1)           # [T, N, C]\n",
    "        log_probs = logits.log_softmax(2)\n",
    "\n",
    "        # Input lengths = all T\n",
    "        input_lengths = torch.full(size=(images.size(0),), \n",
    "                                   fill_value=logits.size(0), \n",
    "                                   dtype=torch.long).to(DEVICE)\n",
    "\n",
    "        # Loss\n",
    "        loss = criterion(log_probs, labels, input_lengths, target_lengths)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (batch_idx+1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}], Step [{batch_idx+1}/{len(train_loader)}], \"\n",
    "                  f\"Loss: {running_loss/10:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "def validate(epoch):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, target_lengths in val_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            target_lengths = target_lengths.to(DEVICE)\n",
    "\n",
    "            logits = model(images)\n",
    "            logits = logits.permute(2, 0, 1)\n",
    "            log_probs = logits.log_softmax(2)\n",
    "\n",
    "            input_lengths = torch.full(size=(images.size(0),), \n",
    "                                       fill_value=logits.size(0), \n",
    "                                       dtype=torch.long).to(DEVICE)\n",
    "\n",
    "            loss = criterion(log_probs, labels, input_lengths, target_lengths)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Epoch [{epoch+1}] Validation Loss: {val_loss:.4f}\")\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6420c562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Step [10/19], Loss: 0.3170\n",
      "Epoch [1] Validation Loss: 0.5419\n",
      "✅ Saved best model at epoch 1 with val_loss=0.5419\n",
      "Epoch [2], Step [10/19], Loss: 0.3155\n",
      "Epoch [2] Validation Loss: 0.3242\n",
      "✅ Saved best model at epoch 2 with val_loss=0.3242\n",
      "Epoch [3], Step [10/19], Loss: 0.2842\n",
      "Epoch [3] Validation Loss: 0.3150\n",
      "✅ Saved best model at epoch 3 with val_loss=0.3150\n",
      "Epoch [4], Step [10/19], Loss: 0.2652\n",
      "Epoch [4] Validation Loss: 0.3646\n",
      "Epoch [5], Step [10/19], Loss: 0.2988\n",
      "Epoch [5] Validation Loss: 0.4781\n",
      "Epoch [6], Step [10/19], Loss: 0.3177\n",
      "Epoch [6] Validation Loss: 0.3450\n",
      "Epoch [7], Step [10/19], Loss: 0.2559\n",
      "Epoch [7] Validation Loss: 0.5174\n",
      "Epoch [8], Step [10/19], Loss: 0.3038\n",
      "Epoch [8] Validation Loss: 0.4544\n",
      "Epoch [9], Step [10/19], Loss: 0.2997\n",
      "Epoch [9] Validation Loss: 0.3179\n",
      "Epoch [10], Step [10/19], Loss: 0.2734\n",
      "Epoch [10] Validation Loss: 0.4233\n",
      "Epoch [11], Step [10/19], Loss: 0.2697\n",
      "Epoch [11] Validation Loss: 0.3415\n",
      "Epoch [12], Step [10/19], Loss: 0.2870\n",
      "Epoch [12] Validation Loss: 0.4468\n",
      "Epoch [13], Step [10/19], Loss: 0.3136\n",
      "Epoch [13] Validation Loss: 0.3546\n",
      "Epoch [14], Step [10/19], Loss: 0.2514\n",
      "Epoch [14] Validation Loss: 0.3567\n",
      "Epoch [15], Step [10/19], Loss: 0.2085\n",
      "Epoch [15] Validation Loss: 0.4055\n",
      "Epoch [16], Step [10/19], Loss: 0.2810\n",
      "Epoch [16] Validation Loss: 0.3246\n",
      "Epoch [17], Step [10/19], Loss: 0.2711\n",
      "Epoch [17] Validation Loss: 0.4348\n",
      "Epoch [18], Step [10/19], Loss: 0.2935\n",
      "Epoch [18] Validation Loss: 0.4862\n",
      "Epoch [19], Step [10/19], Loss: 0.3704\n",
      "Epoch [19] Validation Loss: 0.5770\n",
      "Epoch [20], Step [10/19], Loss: 0.3223\n",
      "Epoch [20] Validation Loss: 0.3877\n",
      "Epoch [21], Step [10/19], Loss: 0.2527\n",
      "Epoch [21] Validation Loss: 0.3464\n",
      "Epoch [22], Step [10/19], Loss: 0.2626\n",
      "Epoch [22] Validation Loss: 0.3942\n",
      "Epoch [23], Step [10/19], Loss: 0.2561\n",
      "Epoch [23] Validation Loss: 0.3622\n",
      "Epoch [24], Step [10/19], Loss: 0.3060\n",
      "Epoch [24] Validation Loss: 0.3872\n",
      "Epoch [25], Step [10/19], Loss: 0.2382\n",
      "Epoch [25] Validation Loss: 0.3950\n",
      "Epoch [26], Step [10/19], Loss: 0.2844\n",
      "Epoch [26] Validation Loss: 0.3803\n",
      "Epoch [27], Step [10/19], Loss: 0.2567\n",
      "Epoch [27] Validation Loss: 0.3228\n",
      "Epoch [28], Step [10/19], Loss: 0.2451\n",
      "Epoch [28] Validation Loss: 0.3142\n",
      "✅ Saved best model at epoch 28 with val_loss=0.3142\n",
      "Epoch [29], Step [10/19], Loss: 0.2989\n",
      "Epoch [29] Validation Loss: 0.4679\n",
      "Epoch [30], Step [10/19], Loss: 0.2713\n",
      "Epoch [30] Validation Loss: 0.4078\n",
      "Epoch [31], Step [10/19], Loss: 0.3037\n",
      "Epoch [31] Validation Loss: 0.3593\n",
      "Epoch [32], Step [10/19], Loss: 0.3741\n",
      "Epoch [32] Validation Loss: 0.4128\n",
      "Epoch [33], Step [10/19], Loss: 0.3465\n",
      "Epoch [33] Validation Loss: 0.4023\n",
      "Epoch [34], Step [10/19], Loss: 0.2942\n",
      "Epoch [34] Validation Loss: 0.3214\n",
      "Epoch [35], Step [10/19], Loss: 0.2238\n",
      "Epoch [35] Validation Loss: 0.3309\n",
      "Epoch [36], Step [10/19], Loss: 0.1818\n",
      "Epoch [36] Validation Loss: 0.3668\n",
      "Epoch [37], Step [10/19], Loss: 0.2001\n",
      "Epoch [37] Validation Loss: 0.2752\n",
      "✅ Saved best model at epoch 37 with val_loss=0.2752\n",
      "Epoch [38], Step [10/19], Loss: 0.2210\n",
      "Epoch [38] Validation Loss: 0.2762\n",
      "Epoch [39], Step [10/19], Loss: 0.1984\n",
      "Epoch [39] Validation Loss: 0.3258\n",
      "Epoch [40], Step [10/19], Loss: 0.2197\n",
      "Epoch [40] Validation Loss: 0.2879\n",
      "Epoch [41], Step [10/19], Loss: 0.1981\n",
      "Epoch [41] Validation Loss: 0.2846\n",
      "Epoch [42], Step [10/19], Loss: 0.1924\n",
      "Epoch [42] Validation Loss: 0.3029\n",
      "Epoch [43], Step [10/19], Loss: 0.2124\n",
      "Epoch [43] Validation Loss: 0.2916\n",
      "Epoch [44], Step [10/19], Loss: 0.2180\n",
      "Epoch [44] Validation Loss: 0.2797\n",
      "Epoch [45], Step [10/19], Loss: 0.2390\n",
      "Epoch [45] Validation Loss: 0.4038\n",
      "Epoch [46], Step [10/19], Loss: 0.1909\n",
      "Epoch [46] Validation Loss: 0.3432\n",
      "Epoch [47], Step [10/19], Loss: 0.1880\n",
      "Epoch [47] Validation Loss: 0.3029\n",
      "Epoch [48], Step [10/19], Loss: 0.2137\n",
      "Epoch [48] Validation Loss: 0.3445\n",
      "Epoch [49], Step [10/19], Loss: 0.1863\n",
      "Epoch [49] Validation Loss: 0.3735\n",
      "Epoch [50], Step [10/19], Loss: 0.2144\n",
      "Epoch [50] Validation Loss: 0.3419\n",
      "Epoch [51], Step [10/19], Loss: 0.1924\n",
      "Epoch [51] Validation Loss: 0.2866\n",
      "Epoch [52], Step [10/19], Loss: 0.2127\n",
      "Epoch [52] Validation Loss: 0.2570\n",
      "✅ Saved best model at epoch 52 with val_loss=0.2570\n",
      "Epoch [53], Step [10/19], Loss: 0.1914\n",
      "Epoch [53] Validation Loss: 0.2421\n",
      "✅ Saved best model at epoch 53 with val_loss=0.2421\n",
      "Epoch [54], Step [10/19], Loss: 0.1723\n",
      "Epoch [54] Validation Loss: 0.3438\n",
      "Epoch [55], Step [10/19], Loss: 0.2177\n",
      "Epoch [55] Validation Loss: 0.3534\n",
      "Epoch [56], Step [10/19], Loss: 0.1908\n",
      "Epoch [56] Validation Loss: 0.3446\n",
      "Epoch [57], Step [10/19], Loss: 0.2208\n",
      "Epoch [57] Validation Loss: 0.4940\n",
      "Epoch [58], Step [10/19], Loss: 0.3135\n",
      "Epoch [58] Validation Loss: 0.5634\n",
      "Epoch [59], Step [10/19], Loss: 0.2389\n",
      "Epoch [59] Validation Loss: 0.3200\n",
      "Epoch [60], Step [10/19], Loss: 0.2335\n",
      "Epoch [60] Validation Loss: 0.2966\n",
      "Epoch [61], Step [10/19], Loss: 0.1901\n",
      "Epoch [61] Validation Loss: 0.3099\n",
      "Epoch [62], Step [10/19], Loss: 0.1688\n",
      "Epoch [62] Validation Loss: 0.2547\n",
      "Epoch [63], Step [10/19], Loss: 0.1650\n",
      "Epoch [63] Validation Loss: 0.2868\n",
      "Epoch [64], Step [10/19], Loss: 0.1605\n",
      "Epoch [64] Validation Loss: 0.4134\n",
      "Epoch [65], Step [10/19], Loss: 0.3129\n",
      "Epoch [65] Validation Loss: 0.3314\n",
      "Epoch [66], Step [10/19], Loss: 0.1908\n",
      "Epoch [66] Validation Loss: 0.3006\n",
      "Epoch [67], Step [10/19], Loss: 0.1591\n",
      "Epoch [67] Validation Loss: 0.3097\n",
      "Epoch [68], Step [10/19], Loss: 0.2002\n",
      "Epoch [68] Validation Loss: 0.3408\n",
      "Epoch [69], Step [10/19], Loss: 0.2080\n",
      "Epoch [69] Validation Loss: 0.2776\n",
      "Epoch [70], Step [10/19], Loss: 0.2033\n",
      "Epoch [70] Validation Loss: 0.3276\n",
      "Epoch [71], Step [10/19], Loss: 0.2115\n",
      "Epoch [71] Validation Loss: 0.3149\n",
      "Epoch [72], Step [10/19], Loss: 0.1598\n",
      "Epoch [72] Validation Loss: 0.3381\n",
      "Epoch [73], Step [10/19], Loss: 0.1518\n",
      "Epoch [73] Validation Loss: 0.2917\n",
      "Epoch [74], Step [10/19], Loss: 0.1576\n",
      "Epoch [74] Validation Loss: 0.2911\n",
      "Epoch [75], Step [10/19], Loss: 0.1631\n",
      "Epoch [75] Validation Loss: 0.3536\n",
      "Epoch [76], Step [10/19], Loss: 0.2418\n",
      "Epoch [76] Validation Loss: 0.4630\n",
      "Epoch [77], Step [10/19], Loss: 0.2118\n",
      "Epoch [77] Validation Loss: 0.4262\n",
      "Epoch [78], Step [10/19], Loss: 0.2458\n",
      "Epoch [78] Validation Loss: 0.3459\n",
      "Epoch [79], Step [10/19], Loss: 0.2346\n",
      "Epoch [79] Validation Loss: 0.4346\n",
      "Epoch [80], Step [10/19], Loss: 0.3157\n",
      "Epoch [80] Validation Loss: 0.4976\n",
      "Epoch [81], Step [10/19], Loss: 0.2552\n",
      "Epoch [81] Validation Loss: 0.4122\n",
      "Epoch [82], Step [10/19], Loss: 0.2298\n",
      "Epoch [82] Validation Loss: 0.2943\n",
      "Epoch [83], Step [10/19], Loss: 0.2142\n",
      "Epoch [83] Validation Loss: 0.3482\n",
      "Epoch [84], Step [10/19], Loss: 0.1766\n",
      "Epoch [84] Validation Loss: 0.3687\n",
      "Epoch [85], Step [10/19], Loss: 0.1426\n",
      "Epoch [85] Validation Loss: 0.3348\n",
      "Epoch [86], Step [10/19], Loss: 0.1919\n",
      "Epoch [86] Validation Loss: 0.3327\n",
      "Epoch [87], Step [10/19], Loss: 0.2427\n",
      "Epoch [87] Validation Loss: 0.4208\n",
      "Epoch [88], Step [10/19], Loss: 0.2349\n",
      "Epoch [88] Validation Loss: 0.5421\n",
      "Epoch [89], Step [10/19], Loss: 0.2220\n",
      "Epoch [89] Validation Loss: 0.3503\n",
      "Epoch [90], Step [10/19], Loss: 0.1946\n",
      "Epoch [90] Validation Loss: 0.2935\n",
      "Epoch [91], Step [10/19], Loss: 0.1611\n",
      "Epoch [91] Validation Loss: 0.2854\n",
      "Epoch [92], Step [10/19], Loss: 0.1542\n",
      "Epoch [92] Validation Loss: 0.2747\n",
      "Epoch [93], Step [10/19], Loss: 0.1781\n",
      "Epoch [93] Validation Loss: 0.4256\n",
      "Epoch [94], Step [10/19], Loss: 0.1699\n",
      "Epoch [94] Validation Loss: 0.3094\n",
      "Epoch [95], Step [10/19], Loss: 0.1634\n",
      "Epoch [95] Validation Loss: 0.3215\n",
      "Epoch [96], Step [10/19], Loss: 0.1873\n",
      "Epoch [96] Validation Loss: 0.2930\n",
      "Epoch [97], Step [10/19], Loss: 0.1624\n",
      "Epoch [97] Validation Loss: 0.3035\n",
      "Epoch [98], Step [10/19], Loss: 0.1611\n",
      "Epoch [98] Validation Loss: 0.3008\n",
      "Epoch [99], Step [10/19], Loss: 0.1450\n",
      "Epoch [99] Validation Loss: 0.3352\n",
      "Epoch [100], Step [10/19], Loss: 0.1784\n",
      "Epoch [100] Validation Loss: 0.4139\n"
     ]
    }
   ],
   "source": [
    "# ===== Main Loop =====\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_one_epoch(epoch)\n",
    "    val_loss = validate(epoch)\n",
    "\n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"chinese_lprnet_best.pth\")\n",
    "        print(f\"✅ Saved best model at epoch {epoch+1} with val_loss={val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2688ad0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1634\n"
     ]
    }
   ],
   "source": [
    "def validate_test():\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, target_lengths in test_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            target_lengths = target_lengths.to(DEVICE)\n",
    "\n",
    "            logits = model(images)\n",
    "            logits = logits.permute(2, 0, 1)\n",
    "            log_probs = logits.log_softmax(2)\n",
    "\n",
    "            input_lengths = torch.full(size=(images.size(0),), \n",
    "                                       fill_value=logits.size(0), \n",
    "                                       dtype=torch.long).to(DEVICE)\n",
    "\n",
    "            loss = criterion(log_probs, labels, input_lengths, target_lengths)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    return val_loss\n",
    "\n",
    "print(f\"Test Loss: {validate_test():.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ff615b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 18])\n"
     ]
    }
   ],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(\"lprnet_best.pth\", map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=ctc_collate_fn)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels, target_lengths in test_loader:\n",
    "        images = images.to(DEVICE)\n",
    "        logits = model(images)                     # [N, C, T]\n",
    "        preds = logits.argmax(1)                   # simple greedy decode (still needs CTC decoding)\n",
    "        print(preds.shape)  # torch.Size([32, 18])\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39ac89d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(logits, idx_to_char, blank_idx):\n",
    "    \"\"\"\n",
    "    logits: [T, N, C] tensor (log probs or raw logits)\n",
    "    idx_to_char: dictionary mapping int -> char\n",
    "    blank_idx: index of blank symbol\n",
    "    \"\"\"\n",
    "    preds = logits.argmax(2).permute(1, 0)   # [N, T]\n",
    "\n",
    "    results = []\n",
    "    for pred in preds:\n",
    "        string = \"\"\n",
    "        prev = None\n",
    "        for p in pred.cpu().numpy():\n",
    "            if p != prev and p != blank_idx:   # collapse + remove blank\n",
    "                string += idx_to_char[p]\n",
    "            prev = p\n",
    "        results.append(string)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4df15a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(model, image, dataset):\n",
    "    \"\"\"\n",
    "    model   : trained model\n",
    "    image   : tensor [3, 24, 94]\n",
    "    dataset : dataset object (for idx_to_char, blank index)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image = image.unsqueeze(0).to(DEVICE)        # add batch dim [1, 3, 24, 94]\n",
    "        logits = model(image)                        # [N, C, T]\n",
    "        logits = logits.permute(2, 0, 1)             # [T, N, C]\n",
    "\n",
    "        preds = greedy_decode(logits, dataset.idx_to_char, dataset.char_to_idx['-'])\n",
    "        return preds[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06d5e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth: A09N61.jpg\n",
      "Prediction  : 7M1\n"
     ]
    }
   ],
   "source": [
    "# Take a sample from your test set\n",
    "i = 9\n",
    "img, _, _ = test_ds[i]\n",
    "print(\"Ground truth:\", test_ds.image_files[i])\n",
    "\n",
    "pred = predict_image(model, img, test_ds)\n",
    "print(\"Prediction  :\", pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8001c13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "def evaluate_and_save(model, dataset, output_dir=\"results\"):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = len(dataset)\n",
    "\n",
    "    # Prepare output folders\n",
    "    right_dir = os.path.join(output_dir, \"right\")\n",
    "    wrong_dir = os.path.join(output_dir, \"wrong\")\n",
    "    os.makedirs(right_dir, exist_ok=True)\n",
    "    os.makedirs(wrong_dir, exist_ok=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(total):\n",
    "            img, label_encoded, _ = dataset[i]\n",
    "\n",
    "            # Ground truth string\n",
    "            label_str = \"\".join(dataset.idx_to_char[idx.item()] for idx in label_encoded)\n",
    "\n",
    "            # Prediction\n",
    "            pred_str = predict_image(model, img, dataset)\n",
    "\n",
    "            # Convert tensor -> numpy for saving\n",
    "            if isinstance(img, torch.Tensor):\n",
    "                np_img = img.squeeze().cpu().numpy() * 255.0\n",
    "                np_img = np_img.astype(\"uint8\")\n",
    "            else:\n",
    "                np_img = img\n",
    "\n",
    "            # Build filename: prediction_GT_index.png\n",
    "            filename = f\"{pred_str}_GT-{label_str}_{i}.png\"\n",
    "\n",
    "            if pred_str == label_str:\n",
    "                correct += 1\n",
    "                save_path = os.path.join(right_dir, filename)\n",
    "            else:\n",
    "                save_path = os.path.join(wrong_dir, filename)\n",
    "\n",
    "            # Save with OpenCV\n",
    "            cv2.imwrite(save_path, np_img)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Run\n",
    "\n",
    "test_acc = evaluate_and_save(model, test_ds, output_dir=\"predictions\")\n",
    "print(f\"Test Accuracy: {test_acc:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e1cd37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4110cb64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lprnet-v1 (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
