{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9a96062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(50)\n",
    "\n",
    "batch_size = 8\n",
    "num_classes = 37   # 36 characters + 1 blank at index 0\n",
    "max_label_length = 10\n",
    "time_steps = 284   # flatten H*W from your model output\n",
    "\n",
    "# Random dense labels (padding with 0 at the end if shorter than max_label_length)\n",
    "labels = tf.random.uniform([batch_size, max_label_length],\n",
    "                           minval=1, maxval=num_classes, dtype=tf.int32)\n",
    "\n",
    "# Each sequence has random true length between 3 and 10\n",
    "label_length = tf.random.uniform([batch_size], minval=3,\n",
    "                                 maxval=max_label_length+1, dtype=tf.int32)\n",
    "\n",
    "# Apply mask â†’ zero out padding positions\n",
    "label_mask = tf.sequence_mask(label_length, maxlen=max_label_length,\n",
    "                              dtype=tf.int32)\n",
    "labels *= label_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68163e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 10)\n",
      "tf.Tensor([12 27 15 19  0  0  0  0  0  0], shape=(10,), dtype=int32)\n",
      "tf.Tensor([33 11 24 14  3  0  0  0  0  0], shape=(10,), dtype=int32)\n",
      "tf.Tensor([ 2 10 25  0  0  0  0  0  0  0], shape=(10,), dtype=int32)\n",
      "tf.Tensor([ 2 30 15 12  7 22 36  0  0  0], shape=(10,), dtype=int32)\n",
      "tf.Tensor([16 10  5  9 19 16 34 33  6  8], shape=(10,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(labels.shape)\n",
    "for i in range(5):\n",
    "    print(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc1672f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 284, 37)\n",
      "tf.Tensor(\n",
      "[[0.63302183 0.4549662  0.13450539 ... 0.90973794 0.19173634 0.59932053]\n",
      " [0.72287667 0.58431506 0.8866216  ... 0.34071648 0.6914804  0.91204166]\n",
      " [0.55996966 0.33776295 0.71286213 ... 0.3919481  0.9734224  0.98789   ]\n",
      " ...\n",
      " [0.9272051  0.7476417  0.5433881  ... 0.5210618  0.6206019  0.51927996]\n",
      " [0.00536728 0.6365942  0.5971236  ... 0.14164734 0.597612   0.05568016]\n",
      " [0.37533653 0.5941305  0.9234253  ... 0.6358385  0.49370325 0.598449  ]], shape=(284, 37), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.00923419 0.766767   0.1920495  ... 0.97353864 0.863531   0.14868426]\n",
      " [0.8987088  0.22154188 0.66756797 ... 0.9135784  0.21104515 0.00705957]\n",
      " [0.14524603 0.82534254 0.92810106 ... 0.8524648  0.7753761  0.65505564]\n",
      " ...\n",
      " [0.01978254 0.14131701 0.26184237 ... 0.40653622 0.83232033 0.79178727]\n",
      " [0.74095416 0.6701547  0.6166954  ... 0.226655   0.47121155 0.14657187]\n",
      " [0.6168709  0.16097271 0.93752146 ... 0.22719693 0.54741347 0.3883685 ]], shape=(284, 37), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.30403268 0.857216   0.3955456  ... 0.5393578  0.85164857 0.30824852]\n",
      " [0.24679744 0.05531096 0.4672761  ... 0.03480494 0.06010628 0.11038303]\n",
      " [0.28439796 0.50837743 0.6856744  ... 0.9861897  0.34425986 0.08898365]\n",
      " ...\n",
      " [0.01510847 0.4012891  0.28318775 ... 0.734236   0.56871796 0.6547371 ]\n",
      " [0.39536202 0.714707   0.95552886 ... 0.5705973  0.20038927 0.66755545]\n",
      " [0.6561332  0.13482106 0.9024066  ... 0.90861654 0.4001825  0.98171926]], shape=(284, 37), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[7.0087004e-01 3.1545734e-01 3.1552935e-01 ... 7.8744686e-01\n",
      "  5.6602859e-01 6.9960189e-01]\n",
      " [5.3014994e-02 6.1436093e-01 4.2509198e-02 ... 1.4944482e-01\n",
      "  5.0351059e-01 3.8031316e-01]\n",
      " [4.2913306e-01 6.9467103e-01 8.1812549e-01 ... 7.1173930e-01\n",
      "  5.6631267e-01 4.6906924e-01]\n",
      " ...\n",
      " [9.4405615e-01 3.4114778e-01 2.1796656e-01 ... 3.9875686e-01\n",
      "  2.9922247e-01 6.2118411e-01]\n",
      " [6.6574311e-01 3.3176899e-01 8.0848634e-01 ... 9.2174613e-01\n",
      "  4.9468505e-01 8.1220484e-01]\n",
      " [8.8395047e-01 2.1668875e-01 7.7033043e-04 ... 7.9894006e-01\n",
      "  3.5164952e-02 4.9239659e-01]], shape=(284, 37), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.55575955 0.1650852  0.55067587 ... 0.6813407  0.84610665 0.9464216 ]\n",
      " [0.74411106 0.34259272 0.595044   ... 0.4731779  0.15954435 0.48817575]\n",
      " [0.81232417 0.26383996 0.25149274 ... 0.3162737  0.88671637 0.6877595 ]\n",
      " ...\n",
      " [0.6166992  0.54914105 0.42428374 ... 0.69880164 0.45337546 0.68679416]\n",
      " [0.5419992  0.09231174 0.3524598  ... 0.63904727 0.71080875 0.91567373]\n",
      " [0.20876682 0.2245022  0.2007463  ... 0.7404561  0.11023962 0.08522844]], shape=(284, 37), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Fake logits (batch, time, num_classes)\n",
    "logits = tf.random.uniform([batch_size, time_steps, num_classes], dtype=tf.float32)\n",
    "print(logits.shape)\n",
    "for i in range(5):\n",
    "    print(logits[i])\n",
    "\n",
    "# Each input has 284 time steps\n",
    "logit_length = tf.fill([batch_size], time_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "740ca31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTC loss per example: [ 994.7967   995.81903 1005.3434   978.7675   958.4418   967.2306\n",
      "  999.75867 1011.1817 ]\n",
      "Mean loss: 988.9174\n"
     ]
    }
   ],
   "source": [
    "loss = tf.nn.ctc_loss(\n",
    "    labels=labels,                # dense padded labels\n",
    "    logits=logits,                # [B, T, C]\n",
    "    label_length=label_length,    # [B]\n",
    "    logit_length=logit_length,    # [B]\n",
    "    logits_time_major=False,      # since our logits are [B, T, C]\n",
    "    blank_index=0                 # class 0 is reserved for blank\n",
    ")\n",
    "\n",
    "print(\"CTC loss per example:\", loss.numpy())\n",
    "print(\"Mean loss:\", tf.reduce_mean(loss).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea77052a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m     loss = tf.reduce_mean(loss)\n\u001b[32m     15\u001b[39m grads = tape.gradient(loss, logits)  \u001b[38;5;66;03m# or model.trainable_variables\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m optimizer.apply_gradients(\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# replace with real model vars\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # logits = model(x)  # your real forward pass\n",
    "    loss = tf.nn.ctc_loss(\n",
    "        labels=labels,\n",
    "        logits=logits,\n",
    "        label_length=label_length,\n",
    "        logit_length=logit_length,\n",
    "        logits_time_major=False,\n",
    "        blank_index=0\n",
    "    )\n",
    "    loss = tf.reduce_mean(loss)\n",
    "\n",
    "grads = tape.gradient(loss, logits)  # or model.trainable_variables\n",
    "optimizer.apply_gradients(zip(grads, [logits]))  # replace with real model vars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb0f1bd",
   "metadata": {},
   "source": [
    "## CTC Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66fb7814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTC mean loss: 236.98169\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# ---------- Settings ----------\n",
    "BATCH_SIZE = 4\n",
    "NUM_CLASSES = 37    # include blank in this count\n",
    "MAX_LABEL_LEN = 10  # maximum ground-truth length (you said 10)\n",
    "BLANK_INDEX = 0     # set to 0 OR (NUM_CLASSES-1) depending on how you encoded labels\n",
    "# ---------- end settings ----------\n",
    "\n",
    "# Dummy model_output to simulate your (batch, H, W, C) output:\n",
    "# For real training: model_output = model(inputs)\n",
    "model_output = tf.random.normal([BATCH_SIZE, 4, 71, NUM_CLASSES], dtype=tf.float32)\n",
    "\n",
    "# 1) Collapse height -> use mean over H (same as PyTorch torch.mean(x, dim=2))\n",
    "logits = tf.reduce_mean(model_output, axis=1)          # shape (BATCH_SIZE, 71, NUM_CLASSES)\n",
    "time_steps = tf.shape(logits)[1]                       # 71\n",
    "\n",
    "# 2) Prepare example labels (dense, padded) and their lengths\n",
    "# Example: random labels in [1..NUM_CLASSES-1], 0 reserved for blank if BLANK_INDEX==0\n",
    "# In practice convert strings -> ints with your char2idx\n",
    "labels = tf.random.uniform([BATCH_SIZE, MAX_LABEL_LEN],\n",
    "                           minval=1, maxval=NUM_CLASSES, dtype=tf.int32)\n",
    "\n",
    "# Make random true lengths between 1 and MAX_LABEL_LEN:\n",
    "label_length = tf.random.uniform([BATCH_SIZE], minval=1, maxval=MAX_LABEL_LEN+1, dtype=tf.int32)\n",
    "\n",
    "# Zero out padding positions:\n",
    "label_mask = tf.sequence_mask(label_length, maxlen=MAX_LABEL_LEN, dtype=tf.int32)\n",
    "labels = labels * label_mask\n",
    "\n",
    "# 3) logit lengths: here time dimension is width (71) for every sample\n",
    "logit_length = tf.fill([BATCH_SIZE], time_steps)\n",
    "\n",
    "# 4) Compute CTC loss: (no softmax beforehand)\n",
    "# tf.nn.ctc_loss returns a vector of shape (batch,)\n",
    "loss_per_batch = tf.nn.ctc_loss(\n",
    "    labels=labels,\n",
    "    logits=logits,            # (batch, time, classes)\n",
    "    label_length=label_length,\n",
    "    logit_length=logit_length,\n",
    "    logits_time_major=False,\n",
    "    blank_index=BLANK_INDEX\n",
    ")\n",
    "\n",
    "loss = tf.reduce_mean(loss_per_batch)\n",
    "print(\"CTC mean loss:\", loss.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58b49d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lprnet-v1 (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
