{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ffa235c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 22, 92]           1,792\n",
      "       BatchNorm2d-2           [-1, 64, 22, 92]             128\n",
      "              ReLU-3           [-1, 64, 22, 92]               0\n",
      "         MaxPool3d-4           [-1, 64, 20, 90]               0\n",
      "            Conv2d-5           [-1, 32, 20, 90]           2,080\n",
      "              ReLU-6           [-1, 32, 20, 90]               0\n",
      "            Conv2d-7           [-1, 32, 20, 90]           3,104\n",
      "              ReLU-8           [-1, 32, 20, 90]               0\n",
      "            Conv2d-9           [-1, 32, 20, 90]           3,104\n",
      "             ReLU-10           [-1, 32, 20, 90]               0\n",
      "           Conv2d-11          [-1, 128, 20, 90]           4,224\n",
      "small_basic_block-12          [-1, 128, 20, 90]               0\n",
      "      BatchNorm2d-13          [-1, 128, 20, 90]             256\n",
      "             ReLU-14          [-1, 128, 20, 90]               0\n",
      "        MaxPool3d-15           [-1, 64, 18, 44]               0\n",
      "           Conv2d-16           [-1, 64, 18, 44]           4,160\n",
      "             ReLU-17           [-1, 64, 18, 44]               0\n",
      "           Conv2d-18           [-1, 64, 18, 44]          12,352\n",
      "             ReLU-19           [-1, 64, 18, 44]               0\n",
      "           Conv2d-20           [-1, 64, 18, 44]          12,352\n",
      "             ReLU-21           [-1, 64, 18, 44]               0\n",
      "           Conv2d-22          [-1, 256, 18, 44]          16,640\n",
      "small_basic_block-23          [-1, 256, 18, 44]               0\n",
      "      BatchNorm2d-24          [-1, 256, 18, 44]             512\n",
      "             ReLU-25          [-1, 256, 18, 44]               0\n",
      "           Conv2d-26           [-1, 64, 18, 44]          16,448\n",
      "             ReLU-27           [-1, 64, 18, 44]               0\n",
      "           Conv2d-28           [-1, 64, 18, 44]          12,352\n",
      "             ReLU-29           [-1, 64, 18, 44]               0\n",
      "           Conv2d-30           [-1, 64, 18, 44]          12,352\n",
      "             ReLU-31           [-1, 64, 18, 44]               0\n",
      "           Conv2d-32          [-1, 256, 18, 44]          16,640\n",
      "small_basic_block-33          [-1, 256, 18, 44]               0\n",
      "      BatchNorm2d-34          [-1, 256, 18, 44]             512\n",
      "             ReLU-35          [-1, 256, 18, 44]               0\n",
      "        MaxPool3d-36           [-1, 64, 16, 21]               0\n",
      "          Dropout-37           [-1, 64, 16, 21]               0\n",
      "           Conv2d-38          [-1, 256, 16, 18]          65,792\n",
      "      BatchNorm2d-39          [-1, 256, 16, 18]             512\n",
      "             ReLU-40          [-1, 256, 16, 18]               0\n",
      "          Dropout-41          [-1, 256, 16, 18]               0\n",
      "           Conv2d-42            [-1, 37, 4, 18]         123,173\n",
      "      BatchNorm2d-43            [-1, 37, 4, 18]              74\n",
      "             ReLU-44            [-1, 37, 4, 18]               0\n",
      "           Conv2d-45            [-1, 37, 4, 18]          17,982\n",
      "================================================================\n",
      "Total params: 326,541\n",
      "Trainable params: 326,541\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.03\n",
      "Forward/backward pass size (MB): 33.57\n",
      "Params size (MB): 1.25\n",
      "Estimated Total Size (MB): 34.84\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from model.LPRNet import build_lprnet\n",
    "from torchsummary import summary\n",
    "\n",
    "model = build_lprnet()\n",
    "summary(model, input_size=(3, 24, 94))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2709cf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import string\n",
    "\n",
    "class ImageFolderCTCDataset(Dataset):\n",
    "    def __init__(self, folder_path, image_shape=(3, 24, 94), augment=False):\n",
    "        self.folder_path = folder_path\n",
    "        self.image_files = os.listdir(folder_path)\n",
    "        _, height, width = image_shape\n",
    "\n",
    "        if augment:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((height, width)),\n",
    "                transforms.RandomAffine(\n",
    "                    degrees=5,              # small rotation (±5°)\n",
    "                    translate=(0.05, 0.05), # shift up to 5% horizontally/vertically\n",
    "                    scale=(0.9, 1.1),       # zoom in/out 10%\n",
    "                    shear=0                 # keep shear = 0 (optional)\n",
    "                ),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((height, width)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "            ])\n",
    "\n",
    "        # dictionary build\n",
    "        self.chars = list(string.digits + string.ascii_uppercase)\n",
    "        self.chars.append('-')\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.folder_path, img_name)\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # Extract label\n",
    "        label_str = img_name.split('.')[0].split('_')[0]\n",
    "        label_encoded = [self.char_to_idx[ch] for ch in label_str]\n",
    "\n",
    "        return image, torch.tensor(label_encoded, dtype=torch.long), len(label_encoded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70ae9664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 24, 94])\n",
      "tensor([ 0,  0, 21, 17,  2,  8,  7,  7])\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SHAPE = (3, 24, 94)\n",
    "train_ds = ImageFolderCTCDataset(r\"lprds\\train\", image_shape=IMAGE_SHAPE, augment=True)\n",
    "val_ds = ImageFolderCTCDataset(r\"lprds\\val\", image_shape=IMAGE_SHAPE)\n",
    "test_ds = ImageFolderCTCDataset(r\"lprds\\test\", image_shape=IMAGE_SHAPE)\n",
    "\n",
    "img, label_encoded, label_length = train_ds[0]\n",
    "\n",
    "print(img.shape)           # torch.Size([3, 24, 94])\n",
    "print(label_encoded)       # tensor([ 0,  0, 21, 17,  2,  8,  7,  7])\n",
    "print(label_length)        # 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df72ba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def ctc_collate_fn(batch):\n",
    "    images, labels, lengths = zip(*batch)  # unzip list of tuples\n",
    "\n",
    "    # Stack images [B, C, H, W]\n",
    "    images = torch.stack(images, dim=0)\n",
    "\n",
    "    # Concatenate all labels into one flat tensor\n",
    "    labels = torch.cat(labels)\n",
    "\n",
    "    # Convert lengths to tensor\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "\n",
    "    return images, labels, lengths\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=ctc_collate_fn)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=ctc_collate_fn)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=ctc_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35cbe796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 24, 94])\n",
      "torch.Size([219])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=ctc_collate_fn)\n",
    "val_loader   = DataLoader(val_ds, batch_size=32, shuffle=False, collate_fn=ctc_collate_fn)\n",
    "test_loader  = DataLoader(test_ds, batch_size=32, shuffle=False, collate_fn=ctc_collate_fn)\n",
    "\n",
    "# check a batch\n",
    "images, labels, lengths = next(iter(train_loader))\n",
    "print(images.shape)   # [32, 3, 24, 94]\n",
    "print(labels.shape)   # flat 1D tensor, e.g. torch.Size([180])\n",
    "print(lengths.shape)  # [32], lengths of each label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3d41681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_lengths_from_logits(logits):\n",
    "    # logits: (N, class_num, T)\n",
    "    T = logits.size(2)\n",
    "    batch_size = logits.size(0)\n",
    "    # CTC expects input_lengths per sample (length of T for each sample)\n",
    "    return torch.full((batch_size,), T, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6420c562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Step [10/19], Loss: 0.4647\n",
      "Epoch [1] Validation Loss: 0.5779\n",
      "✅ Saved best model at epoch 1 with val_loss=0.5779\n",
      "Epoch [2], Step [10/19], Loss: 0.3850\n",
      "Epoch [2] Validation Loss: 0.3969\n",
      "✅ Saved best model at epoch 2 with val_loss=0.3969\n",
      "Epoch [3], Step [10/19], Loss: 0.3545\n",
      "Epoch [3] Validation Loss: 0.4347\n",
      "Epoch [4], Step [10/19], Loss: 0.3231\n",
      "Epoch [4] Validation Loss: 0.4081\n",
      "Epoch [5], Step [10/19], Loss: 0.2885\n",
      "Epoch [5] Validation Loss: 0.4109\n",
      "Epoch [6], Step [10/19], Loss: 0.3288\n",
      "Epoch [6] Validation Loss: 0.4906\n",
      "Epoch [7], Step [10/19], Loss: 0.3437\n",
      "Epoch [7] Validation Loss: 0.3639\n",
      "✅ Saved best model at epoch 7 with val_loss=0.3639\n",
      "Epoch [8], Step [10/19], Loss: 0.3210\n",
      "Epoch [8] Validation Loss: 0.5676\n",
      "Epoch [9], Step [10/19], Loss: 0.3426\n",
      "Epoch [9] Validation Loss: 0.5703\n",
      "Epoch [10], Step [10/19], Loss: 0.3144\n",
      "Epoch [10] Validation Loss: 0.4219\n",
      "Epoch [11], Step [10/19], Loss: 0.3038\n",
      "Epoch [11] Validation Loss: 0.4258\n",
      "Epoch [12], Step [10/19], Loss: 0.3096\n",
      "Epoch [12] Validation Loss: 0.4102\n",
      "Epoch [13], Step [10/19], Loss: 0.2957\n",
      "Epoch [13] Validation Loss: 0.4025\n",
      "Epoch [14], Step [10/19], Loss: 0.3186\n",
      "Epoch [14] Validation Loss: 0.4665\n",
      "Epoch [15], Step [10/19], Loss: 0.3018\n",
      "Epoch [15] Validation Loss: 0.3981\n",
      "Epoch [16], Step [10/19], Loss: 0.2823\n",
      "Epoch [16] Validation Loss: 0.3927\n",
      "Epoch [17], Step [10/19], Loss: 0.2806\n",
      "Epoch [17] Validation Loss: 0.3924\n",
      "Epoch [18], Step [10/19], Loss: 0.3012\n",
      "Epoch [18] Validation Loss: 0.3748\n",
      "Epoch [19], Step [10/19], Loss: 0.2604\n",
      "Epoch [19] Validation Loss: 0.4445\n",
      "Epoch [20], Step [10/19], Loss: 0.3149\n",
      "Epoch [20] Validation Loss: 0.4781\n",
      "Epoch [21], Step [10/19], Loss: 0.2887\n",
      "Epoch [21] Validation Loss: 0.5567\n",
      "Epoch [22], Step [10/19], Loss: 0.2796\n",
      "Epoch [22] Validation Loss: 0.3957\n",
      "Epoch [23], Step [10/19], Loss: 0.2690\n",
      "Epoch [23] Validation Loss: 0.4471\n",
      "Epoch [24], Step [10/19], Loss: 0.2923\n",
      "Epoch [24] Validation Loss: 0.4489\n",
      "Epoch [25], Step [10/19], Loss: 0.2878\n",
      "Epoch [25] Validation Loss: 0.4818\n",
      "Epoch [26], Step [10/19], Loss: 0.2502\n",
      "Epoch [26] Validation Loss: 0.3858\n",
      "Epoch [27], Step [10/19], Loss: 0.2844\n",
      "Epoch [27] Validation Loss: 0.4532\n",
      "Epoch [28], Step [10/19], Loss: 0.2730\n",
      "Epoch [28] Validation Loss: 0.4265\n",
      "Epoch [29], Step [10/19], Loss: 0.2905\n",
      "Epoch [29] Validation Loss: 0.4124\n",
      "Epoch [30], Step [10/19], Loss: 0.2691\n",
      "Epoch [30] Validation Loss: 0.4239\n",
      "Epoch [31], Step [10/19], Loss: 0.2363\n",
      "Epoch [31] Validation Loss: 0.3910\n",
      "Epoch [32], Step [10/19], Loss: 0.2355\n",
      "Epoch [32] Validation Loss: 0.4453\n",
      "Epoch [33], Step [10/19], Loss: 0.2432\n",
      "Epoch [33] Validation Loss: 0.4525\n",
      "Epoch [34], Step [10/19], Loss: 0.2875\n",
      "Epoch [34] Validation Loss: 0.4001\n",
      "Epoch [35], Step [10/19], Loss: 0.2925\n",
      "Epoch [35] Validation Loss: 0.4235\n",
      "Epoch [36], Step [10/19], Loss: 0.2366\n",
      "Epoch [36] Validation Loss: 0.4590\n",
      "Epoch [37], Step [10/19], Loss: 0.2911\n",
      "Epoch [37] Validation Loss: 0.4535\n",
      "Epoch [38], Step [10/19], Loss: 0.2695\n",
      "Epoch [38] Validation Loss: 0.3629\n",
      "✅ Saved best model at epoch 38 with val_loss=0.3629\n",
      "Epoch [39], Step [10/19], Loss: 0.2332\n",
      "Epoch [39] Validation Loss: 0.3621\n",
      "✅ Saved best model at epoch 39 with val_loss=0.3621\n",
      "Epoch [40], Step [10/19], Loss: 0.2472\n",
      "Epoch [40] Validation Loss: 0.3991\n",
      "Epoch [41], Step [10/19], Loss: 0.2103\n",
      "Epoch [41] Validation Loss: 0.3919\n",
      "Epoch [42], Step [10/19], Loss: 0.2583\n",
      "Epoch [42] Validation Loss: 0.5036\n",
      "Epoch [43], Step [10/19], Loss: 0.2331\n",
      "Epoch [43] Validation Loss: 0.3977\n",
      "Epoch [44], Step [10/19], Loss: 0.2679\n",
      "Epoch [44] Validation Loss: 0.3951\n",
      "Epoch [45], Step [10/19], Loss: 0.2328\n",
      "Epoch [45] Validation Loss: 0.4658\n",
      "Epoch [46], Step [10/19], Loss: 0.2542\n",
      "Epoch [46] Validation Loss: 0.3656\n",
      "Epoch [47], Step [10/19], Loss: 0.2142\n",
      "Epoch [47] Validation Loss: 0.3898\n",
      "Epoch [48], Step [10/19], Loss: 0.2805\n",
      "Epoch [48] Validation Loss: 0.4279\n",
      "Epoch [49], Step [10/19], Loss: 0.2464\n",
      "Epoch [49] Validation Loss: 0.4976\n",
      "Epoch [50], Step [10/19], Loss: 0.2146\n",
      "Epoch [50] Validation Loss: 0.4263\n",
      "Epoch [51], Step [10/19], Loss: 0.2131\n",
      "Epoch [51] Validation Loss: 0.4152\n",
      "Epoch [52], Step [10/19], Loss: 0.2175\n",
      "Epoch [52] Validation Loss: 0.3915\n",
      "Epoch [53], Step [10/19], Loss: 0.1898\n",
      "Epoch [53] Validation Loss: 0.3647\n",
      "Epoch [54], Step [10/19], Loss: 0.2445\n",
      "Epoch [54] Validation Loss: 0.3767\n",
      "Epoch [55], Step [10/19], Loss: 0.2497\n",
      "Epoch [55] Validation Loss: 0.3584\n",
      "✅ Saved best model at epoch 55 with val_loss=0.3584\n",
      "Epoch [56], Step [10/19], Loss: 0.2095\n",
      "Epoch [56] Validation Loss: 0.3998\n",
      "Epoch [57], Step [10/19], Loss: 0.1922\n",
      "Epoch [57] Validation Loss: 0.4449\n",
      "Epoch [58], Step [10/19], Loss: 0.2404\n",
      "Epoch [58] Validation Loss: 0.3764\n",
      "Epoch [59], Step [10/19], Loss: 0.2094\n",
      "Epoch [59] Validation Loss: 0.4950\n",
      "Epoch [60], Step [10/19], Loss: 0.2164\n",
      "Epoch [60] Validation Loss: 0.4278\n",
      "Epoch [61], Step [10/19], Loss: 0.2125\n",
      "Epoch [61] Validation Loss: 0.3658\n",
      "Epoch [62], Step [10/19], Loss: 0.2140\n",
      "Epoch [62] Validation Loss: 0.4291\n",
      "Epoch [63], Step [10/19], Loss: 0.2439\n",
      "Epoch [63] Validation Loss: 0.5099\n",
      "Epoch [64], Step [10/19], Loss: 0.1985\n",
      "Epoch [64] Validation Loss: 0.3772\n",
      "Epoch [65], Step [10/19], Loss: 0.2239\n",
      "Epoch [65] Validation Loss: 0.4580\n",
      "Epoch [66], Step [10/19], Loss: 0.1760\n",
      "Epoch [66] Validation Loss: 0.3248\n",
      "✅ Saved best model at epoch 66 with val_loss=0.3248\n",
      "Epoch [67], Step [10/19], Loss: 0.2087\n",
      "Epoch [67] Validation Loss: 0.5193\n",
      "Epoch [68], Step [10/19], Loss: 0.2140\n",
      "Epoch [68] Validation Loss: 0.4088\n",
      "Epoch [69], Step [10/19], Loss: 0.2227\n",
      "Epoch [69] Validation Loss: 0.4620\n",
      "Epoch [70], Step [10/19], Loss: 0.1725\n",
      "Epoch [70] Validation Loss: 0.3978\n",
      "Epoch [71], Step [10/19], Loss: 0.2296\n",
      "Epoch [71] Validation Loss: 0.4536\n",
      "Epoch [72], Step [10/19], Loss: 0.1971\n",
      "Epoch [72] Validation Loss: 0.4890\n",
      "Epoch [73], Step [10/19], Loss: 0.2650\n",
      "Epoch [73] Validation Loss: 0.3955\n",
      "Epoch [74], Step [10/19], Loss: 0.2154\n",
      "Epoch [74] Validation Loss: 0.4173\n",
      "Epoch [75], Step [10/19], Loss: 0.1911\n",
      "Epoch [75] Validation Loss: 0.3780\n",
      "Epoch [76], Step [10/19], Loss: 0.1907\n",
      "Epoch [76] Validation Loss: 0.3832\n",
      "Epoch [77], Step [10/19], Loss: 0.1630\n",
      "Epoch [77] Validation Loss: 0.4275\n",
      "Epoch [78], Step [10/19], Loss: 0.1453\n",
      "Epoch [78] Validation Loss: 0.3939\n",
      "Epoch [79], Step [10/19], Loss: 0.1728\n",
      "Epoch [79] Validation Loss: 0.3582\n",
      "Epoch [80], Step [10/19], Loss: 0.1945\n",
      "Epoch [80] Validation Loss: 0.4040\n",
      "Epoch [81], Step [10/19], Loss: 0.2339\n",
      "Epoch [81] Validation Loss: 0.5063\n",
      "Epoch [82], Step [10/19], Loss: 0.2322\n",
      "Epoch [82] Validation Loss: 0.3718\n",
      "Epoch [83], Step [10/19], Loss: 0.2112\n",
      "Epoch [83] Validation Loss: 0.3914\n",
      "Epoch [84], Step [10/19], Loss: 0.2089\n",
      "Epoch [84] Validation Loss: 0.3955\n",
      "Epoch [85], Step [10/19], Loss: 0.1998\n",
      "Epoch [85] Validation Loss: 0.3596\n",
      "Epoch [86], Step [10/19], Loss: 0.1560\n",
      "Epoch [86] Validation Loss: 0.3192\n",
      "✅ Saved best model at epoch 86 with val_loss=0.3192\n",
      "Epoch [87], Step [10/19], Loss: 0.2102\n",
      "Epoch [87] Validation Loss: 0.3750\n",
      "Epoch [88], Step [10/19], Loss: 0.1463\n",
      "Epoch [88] Validation Loss: 0.4228\n",
      "Epoch [89], Step [10/19], Loss: 0.1770\n",
      "Epoch [89] Validation Loss: 0.3413\n",
      "Epoch [90], Step [10/19], Loss: 0.1681\n",
      "Epoch [90] Validation Loss: 0.4197\n",
      "Epoch [91], Step [10/19], Loss: 0.2092\n",
      "Epoch [91] Validation Loss: 0.4279\n",
      "Epoch [92], Step [10/19], Loss: 0.1887\n",
      "Epoch [92] Validation Loss: 0.3759\n",
      "Epoch [93], Step [10/19], Loss: 0.1410\n",
      "Epoch [93] Validation Loss: 0.4463\n",
      "Epoch [94], Step [10/19], Loss: 0.1921\n",
      "Epoch [94] Validation Loss: 0.3603\n",
      "Epoch [95], Step [10/19], Loss: 0.1903\n",
      "Epoch [95] Validation Loss: 0.3790\n",
      "Epoch [96], Step [10/19], Loss: 0.1455\n",
      "Epoch [96] Validation Loss: 0.3669\n",
      "Epoch [97], Step [10/19], Loss: 0.1930\n",
      "Epoch [97] Validation Loss: 0.3697\n",
      "Epoch [98], Step [10/19], Loss: 0.1863\n",
      "Epoch [98] Validation Loss: 0.3409\n",
      "Epoch [99], Step [10/19], Loss: 0.1730\n",
      "Epoch [99] Validation Loss: 0.3580\n",
      "Epoch [100], Step [10/19], Loss: 0.1621\n",
      "Epoch [100] Validation Loss: 0.5274\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# -------- Config ----------\n",
    "CLASS_NUM = 37              # number of classes (0-9, A-Z, plus blank)\n",
    "MAX_LABEL_LEN = 10          # max characters per sample (fake, for model design)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LR = 1e-3\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "# --------------------------\n",
    "\n",
    "# ===== Dataset & DataLoader =====\n",
    "def ctc_collate_fn(batch):\n",
    "    images, labels, lengths = zip(*batch)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    labels = torch.cat(labels)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "    return images, labels, lengths\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=ctc_collate_fn)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=ctc_collate_fn)\n",
    "\n",
    "# ===== Model, Loss, Optimizer =====\n",
    "model = build_lprnet(MAX_LABEL_LEN, CLASS_NUM).to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"Final_LPRNet_model.pth\", map_location=DEVICE))\n",
    "\n",
    "criterion = nn.CTCLoss(blank=train_ds.char_to_idx['-'], reduction='mean')\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "# ===== Training & Validation =====\n",
    "def train_one_epoch(epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (images, labels, target_lengths) in enumerate(train_loader):\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        target_lengths = target_lengths.to(DEVICE)\n",
    "\n",
    "        # Forward\n",
    "        logits = model(images)                     # [N, C, T]\n",
    "        logits = logits.permute(2, 0, 1)           # [T, N, C]\n",
    "        log_probs = logits.log_softmax(2)\n",
    "\n",
    "        # Input lengths = all T\n",
    "        input_lengths = torch.full(size=(images.size(0),), \n",
    "                                   fill_value=logits.size(0), \n",
    "                                   dtype=torch.long).to(DEVICE)\n",
    "\n",
    "        # Loss\n",
    "        loss = criterion(log_probs, labels, input_lengths, target_lengths)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (batch_idx+1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}], Step [{batch_idx+1}/{len(train_loader)}], \"\n",
    "                  f\"Loss: {running_loss/10:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "def validate(epoch):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, target_lengths in val_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            target_lengths = target_lengths.to(DEVICE)\n",
    "\n",
    "            logits = model(images)\n",
    "            logits = logits.permute(2, 0, 1)\n",
    "            log_probs = logits.log_softmax(2)\n",
    "\n",
    "            input_lengths = torch.full(size=(images.size(0),), \n",
    "                                       fill_value=logits.size(0), \n",
    "                                       dtype=torch.long).to(DEVICE)\n",
    "\n",
    "            loss = criterion(log_probs, labels, input_lengths, target_lengths)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Epoch [{epoch+1}] Validation Loss: {val_loss:.4f}\")\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "# ===== Main Loop =====\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_one_epoch(epoch)\n",
    "    val_loss = validate(epoch)\n",
    "\n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"lprnet_best2.pth\")\n",
    "        print(f\"✅ Saved best model at epoch {epoch+1} with val_loss={val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2688ad0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.406448428829511"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate_test():\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, target_lengths in test_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            target_lengths = target_lengths.to(DEVICE)\n",
    "\n",
    "            logits = model(images)\n",
    "            logits = logits.permute(2, 0, 1)\n",
    "            log_probs = logits.log_softmax(2)\n",
    "\n",
    "            input_lengths = torch.full(size=(images.size(0),), \n",
    "                                       fill_value=logits.size(0), \n",
    "                                       dtype=torch.long).to(DEVICE)\n",
    "\n",
    "            loss = criterion(log_probs, labels, input_lengths, target_lengths)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Test Loss: {val_loss:.4f}\")\n",
    "    return val_loss\n",
    "validate_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2ff615b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 18])\n"
     ]
    }
   ],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(\"lprnet_best.pth\", map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=ctc_collate_fn)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels, target_lengths in test_loader:\n",
    "        images = images.to(DEVICE)\n",
    "        logits = model(images)                     # [N, C, T]\n",
    "        preds = logits.argmax(1)                   # simple greedy decode (still needs CTC decoding)\n",
    "        print(preds.shape)  # torch.Size([32, 18])\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39ac89d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(logits, idx_to_char, blank_idx):\n",
    "    \"\"\"\n",
    "    logits: [T, N, C] tensor (log probs or raw logits)\n",
    "    idx_to_char: dictionary mapping int -> char\n",
    "    blank_idx: index of blank symbol\n",
    "    \"\"\"\n",
    "    preds = logits.argmax(2).permute(1, 0)   # [N, T]\n",
    "\n",
    "    results = []\n",
    "    for pred in preds:\n",
    "        string = \"\"\n",
    "        prev = None\n",
    "        for p in pred.cpu().numpy():\n",
    "            if p != prev and p != blank_idx:   # collapse + remove blank\n",
    "                string += idx_to_char[p]\n",
    "            prev = p\n",
    "        results.append(string)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4df15a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(model, image, dataset):\n",
    "    \"\"\"\n",
    "    model   : trained model\n",
    "    image   : tensor [3, 24, 94]\n",
    "    dataset : dataset object (for idx_to_char, blank index)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image = image.unsqueeze(0).to(DEVICE)        # add batch dim [1, 3, 24, 94]\n",
    "        logits = model(image)                        # [N, C, T]\n",
    "        logits = logits.permute(2, 0, 1)             # [T, N, C]\n",
    "\n",
    "        preds = greedy_decode(logits, dataset.idx_to_char, dataset.char_to_idx['-'])\n",
    "        return preds[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b06d5e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth: 75N1960G.png\n",
      "Prediction  : 75N1960G\n"
     ]
    }
   ],
   "source": [
    "# Take a sample from your test set\n",
    "\n",
    "i = 9\n",
    "img, _, _ = test_ds[i]\n",
    "print(\"Ground truth:\", test_ds.image_files[i])\n",
    "\n",
    "pred = predict_image(model, img, test_ds)\n",
    "print(\"Prediction  :\", pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8001c13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT: 1033IR | Pred: 1033R\n",
      "GT: 381ATK83 | Pred: 381ATK63\n",
      "GT: 381ATK83 | Pred: 381ATK63\n",
      "GT: 8427XX29 | Pred: B427XX29\n",
      "GT: AC508V | Pred: C508V\n",
      "GT: AL193VP | Pred: AL193HP\n",
      "GT: AX128FT | Pred: AX12BFT\n",
      "GT: B21GSB | Pred: B1GSB\n",
      "GT: B225RTK | Pred: B115RTK\n",
      "GT: B96SXV | Pred: B9SXV\n",
      "GT: B96SXV | Pred: B9SXV\n",
      "GT: BA999ZZ | Pred: BA999ZI\n",
      "GT: BZ310SW | Pred: BZ310SN\n",
      "GT: CM101LW | Pred: CM101L\n",
      "GT: CV194VA | Pred: CQ194VA\n",
      "GT: EJQ588 | Pred: E10588\n",
      "GT: ES484LJ | Pred: ES464LJ\n",
      "GT: FC882MC | Pred: FC82MC\n",
      "GT: FF788NK | Pred: FFT0BNK\n",
      "GT: FL986ZT | Pred: FL9B6ZT\n",
      "GT: FR697HF | Pred: FR697H\n",
      "GT: H864JGM | Pred: H864JHGM\n",
      "GT: I008646 | Pred: 1008646\n",
      "GT: IICE3083 | Pred: ICE30683\n",
      "GT: KZ746AR | Pred: KZ7AGAR\n",
      "GT: LBHT71 | Pred: LB\n",
      "GT: MWWK908 | Pred: MWWK900\n",
      "GT: PH20CNP | Pred: T6H61\n",
      "GT: PI36KOG | Pred: PI36K0\n",
      "GT: SG49711 | Pred: G49711\n",
      "GT: SLF9995 | Pred: SL995\n",
      "GT: TR95XMI | Pred: TR95XMH\n",
      "GT: TR95XMI | Pred: TR95XMH\n",
      "GT: VS236891 | Pred: S236891\n",
      "GT: VS236891 | Pred: S236891\n",
      "GT: WW921MX | Pred: WW921MK\n",
      "GT: YB75711 | Pred: YBZ5711\n",
      "Test Accuracy: 51.95%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_accuracy(model, dataset):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = len(dataset)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(total):\n",
    "            img, label_encoded, _ = dataset[i]\n",
    "\n",
    "            # Ground truth string\n",
    "            label_str = \"\".join(dataset.idx_to_char[idx.item()] for idx in label_encoded)\n",
    "\n",
    "            # Prediction\n",
    "            pred_str = predict_image(model, img, dataset)\n",
    "\n",
    "            if pred_str == label_str:\n",
    "                correct += 1\n",
    "            else:\n",
    "                print(f\"GT: {label_str} | Pred: {pred_str}\")\n",
    "                \n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# train_acc = evaluate_accuracy(model, train_ds)\n",
    "# print(f\"Train Accuracy: {train_acc:.2%}\")\n",
    "\n",
    "# val_acc = evaluate_accuracy(model, val_ds)\n",
    "# print(f\"Val Accuracy: {val_acc:.2%}\")\n",
    "\n",
    "test_acc = evaluate_accuracy(model, test_ds)\n",
    "print(f\"Test Accuracy: {test_acc:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea4a636",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lprnet-v1 (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
