{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffa235c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 3, 24, 94]               6\n",
      "              ReLU-2            [-1, 3, 24, 94]               0\n",
      "            Conv2d-3            [-1, 3, 24, 94]              12\n",
      "              ReLU-4            [-1, 3, 24, 94]               0\n",
      "            Conv2d-5            [-1, 3, 24, 94]              12\n",
      "              ReLU-6            [-1, 3, 24, 94]               0\n",
      "            Conv2d-7           [-1, 64, 22, 92]           1,792\n",
      "       BatchNorm2d-8           [-1, 64, 22, 92]             128\n",
      "              ReLU-9           [-1, 64, 22, 92]               0\n",
      "        MaxPool3d-10           [-1, 64, 20, 90]               0\n",
      "           Conv2d-11           [-1, 32, 20, 90]           2,080\n",
      "             ReLU-12           [-1, 32, 20, 90]               0\n",
      "           Conv2d-13           [-1, 32, 20, 90]           3,104\n",
      "             ReLU-14           [-1, 32, 20, 90]               0\n",
      "           Conv2d-15           [-1, 32, 20, 90]           3,104\n",
      "             ReLU-16           [-1, 32, 20, 90]               0\n",
      "           Conv2d-17          [-1, 128, 20, 90]           4,224\n",
      "small_basic_block-18          [-1, 128, 20, 90]               0\n",
      "      BatchNorm2d-19          [-1, 128, 20, 90]             256\n",
      "             ReLU-20          [-1, 128, 20, 90]               0\n",
      "        MaxPool3d-21           [-1, 64, 18, 44]               0\n",
      "           Conv2d-22           [-1, 64, 18, 44]           4,160\n",
      "             ReLU-23           [-1, 64, 18, 44]               0\n",
      "           Conv2d-24           [-1, 64, 18, 44]          12,352\n",
      "             ReLU-25           [-1, 64, 18, 44]               0\n",
      "           Conv2d-26           [-1, 64, 18, 44]          12,352\n",
      "             ReLU-27           [-1, 64, 18, 44]               0\n",
      "           Conv2d-28          [-1, 256, 18, 44]          16,640\n",
      "small_basic_block-29          [-1, 256, 18, 44]               0\n",
      "      BatchNorm2d-30          [-1, 256, 18, 44]             512\n",
      "             ReLU-31          [-1, 256, 18, 44]               0\n",
      "           Conv2d-32           [-1, 64, 18, 44]          16,448\n",
      "             ReLU-33           [-1, 64, 18, 44]               0\n",
      "           Conv2d-34           [-1, 64, 18, 44]          12,352\n",
      "             ReLU-35           [-1, 64, 18, 44]               0\n",
      "           Conv2d-36           [-1, 64, 18, 44]          12,352\n",
      "             ReLU-37           [-1, 64, 18, 44]               0\n",
      "           Conv2d-38          [-1, 256, 18, 44]          16,640\n",
      "small_basic_block-39          [-1, 256, 18, 44]               0\n",
      "      BatchNorm2d-40          [-1, 256, 18, 44]             512\n",
      "             ReLU-41          [-1, 256, 18, 44]               0\n",
      "        MaxPool3d-42           [-1, 64, 16, 21]               0\n",
      "          Dropout-43           [-1, 64, 16, 21]               0\n",
      "           Conv2d-44          [-1, 256, 16, 18]          65,792\n",
      "      BatchNorm2d-45          [-1, 256, 16, 18]             512\n",
      "             ReLU-46          [-1, 256, 16, 18]               0\n",
      "          Dropout-47          [-1, 256, 16, 18]               0\n",
      "           Conv2d-48            [-1, 37, 4, 18]         123,173\n",
      "      BatchNorm2d-49            [-1, 37, 4, 18]              74\n",
      "             ReLU-50            [-1, 37, 4, 18]               0\n",
      "           Conv2d-51            [-1, 37, 4, 18]          17,982\n",
      "================================================================\n",
      "Total params: 326,571\n",
      "Trainable params: 326,571\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 33.88\n",
      "Params size (MB): 1.25\n",
      "Estimated Total Size (MB): 35.14\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from model.LPRNet import build_lprnet\n",
    "from torchsummary import summary\n",
    "\n",
    "model = build_lprnet()\n",
    "summary(model, input_size=(1, 24, 94))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2709cf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import string\n",
    "\n",
    "class ImageFolderCTCDataset(Dataset):\n",
    "    def __init__(self, folder_path, image_shape=(1, 24, 94), augment=False):\n",
    "        self.folder_path = folder_path\n",
    "        self.image_files = os.listdir(folder_path)\n",
    "        _, height, width = image_shape\n",
    "\n",
    "        if augment:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((height, width)),\n",
    "                transforms.RandomAffine(\n",
    "                    degrees=5,\n",
    "                    translate=(0.05, 0.05),\n",
    "                    scale=(0.9, 1.1),\n",
    "                    shear=0\n",
    "                ),\n",
    "                transforms.Grayscale(num_output_channels=1),\n",
    "                transforms.ToTensor()   # grayscale tensor in [0,1]\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((height, width)),\n",
    "                transforms.Grayscale(num_output_channels=1),\n",
    "                transforms.ToTensor()   # grayscale tensor in [0,1]\n",
    "            ])\n",
    "\n",
    "        # dictionary build\n",
    "        self.chars = list(string.digits + string.ascii_uppercase)\n",
    "        self.chars.append('-')\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.folder_path, img_name)\n",
    "\n",
    "        image = Image.open(img_path).convert(\"L\")  # grayscale\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # Extract label\n",
    "        label_str = img_name.split('.')[0].split('_')[0]\n",
    "        label_encoded = [self.char_to_idx[ch] for ch in label_str]\n",
    "\n",
    "        return image, torch.tensor(label_encoded, dtype=torch.long), len(label_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70ae9664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 24, 94])\n",
      "tensor([ 0,  0, 21, 17,  2,  8,  7,  7])\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SHAPE = (3, 24, 94)\n",
    "train_ds = ImageFolderCTCDataset(r\"lprds\\train\", image_shape=IMAGE_SHAPE, augment=True)\n",
    "val_ds = ImageFolderCTCDataset(r\"lprds\\val\", image_shape=IMAGE_SHAPE)\n",
    "test_ds = ImageFolderCTCDataset(r\"lprds\\test\", image_shape=IMAGE_SHAPE)\n",
    "\n",
    "img, label_encoded, label_length = train_ds[0]\n",
    "\n",
    "print(img.shape)           # torch.Size([3, 24, 94])\n",
    "print(label_encoded)       # tensor([ 0,  0, 21, 17,  2,  8,  7,  7])\n",
    "print(label_length)        # 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df72ba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def ctc_collate_fn(batch):\n",
    "    images, labels, lengths = zip(*batch)  # unzip list of tuples\n",
    "\n",
    "    # Stack images [B, C, H, W]\n",
    "    images = torch.stack(images, dim=0)\n",
    "\n",
    "    # Concatenate all labels into one flat tensor\n",
    "    labels = torch.cat(labels)\n",
    "\n",
    "    # Convert lengths to tensor\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "\n",
    "    return images, labels, lengths\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=ctc_collate_fn)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=ctc_collate_fn)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=ctc_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35cbe796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 24, 94])\n",
      "torch.Size([223])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# check a batch\n",
    "images, labels, lengths = next(iter(train_loader))\n",
    "print(images.shape)   # [32, 3, 24, 94]\n",
    "print(labels.shape)   # flat 1D tensor, e.g. torch.Size([180])\n",
    "print(lengths.shape)  # [32], lengths of each label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6420c562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Step [10/19], Loss: 1.0673\n",
      "Epoch [1] Validation Loss: 5.0664\n",
      "✅ Saved best model at epoch 1 with val_loss=5.0664\n",
      "Epoch [2], Step [10/19], Loss: 0.8283\n",
      "Epoch [2] Validation Loss: 3.1402\n",
      "✅ Saved best model at epoch 2 with val_loss=3.1402\n",
      "Epoch [3], Step [10/19], Loss: 0.7835\n",
      "Epoch [3] Validation Loss: 1.0642\n",
      "✅ Saved best model at epoch 3 with val_loss=1.0642\n",
      "Epoch [4], Step [10/19], Loss: 0.6801\n",
      "Epoch [4] Validation Loss: 1.2981\n",
      "Epoch [5], Step [10/19], Loss: 0.6279\n",
      "Epoch [5] Validation Loss: 3.2551\n",
      "Epoch [6], Step [10/19], Loss: 0.5921\n",
      "Epoch [6] Validation Loss: 3.8942\n",
      "Epoch [7], Step [10/19], Loss: 0.5908\n",
      "Epoch [7] Validation Loss: 1.8545\n",
      "Epoch [8], Step [10/19], Loss: 0.5524\n",
      "Epoch [8] Validation Loss: 1.8384\n",
      "Epoch [9], Step [10/19], Loss: 0.6680\n",
      "Epoch [9] Validation Loss: 3.0841\n",
      "Epoch [10], Step [10/19], Loss: 0.5515\n",
      "Epoch [10] Validation Loss: 3.5757\n",
      "Epoch [11], Step [10/19], Loss: 0.5237\n",
      "Epoch [11] Validation Loss: 1.5007\n",
      "Epoch [12], Step [10/19], Loss: 0.4835\n",
      "Epoch [12] Validation Loss: 0.9276\n",
      "✅ Saved best model at epoch 12 with val_loss=0.9276\n",
      "Epoch [13], Step [10/19], Loss: 0.4789\n",
      "Epoch [13] Validation Loss: 0.8355\n",
      "✅ Saved best model at epoch 13 with val_loss=0.8355\n",
      "Epoch [14], Step [10/19], Loss: 0.5107\n",
      "Epoch [14] Validation Loss: 2.1238\n",
      "Epoch [15], Step [10/19], Loss: 0.5206\n",
      "Epoch [15] Validation Loss: 0.9832\n",
      "Epoch [16], Step [10/19], Loss: 0.5276\n",
      "Epoch [16] Validation Loss: 1.5160\n",
      "Epoch [17], Step [10/19], Loss: 0.5182\n",
      "Epoch [17] Validation Loss: 1.4075\n",
      "Epoch [18], Step [10/19], Loss: 0.4889\n",
      "Epoch [18] Validation Loss: 2.8032\n",
      "Epoch [19], Step [10/19], Loss: 0.4904\n",
      "Epoch [19] Validation Loss: 2.8087\n",
      "Epoch [20], Step [10/19], Loss: 0.4267\n",
      "Epoch [20] Validation Loss: 0.6173\n",
      "✅ Saved best model at epoch 20 with val_loss=0.6173\n",
      "Epoch [21], Step [10/19], Loss: 0.4906\n",
      "Epoch [21] Validation Loss: 0.7485\n",
      "Epoch [22], Step [10/19], Loss: 0.4315\n",
      "Epoch [22] Validation Loss: 2.6263\n",
      "Epoch [23], Step [10/19], Loss: 0.4187\n",
      "Epoch [23] Validation Loss: 1.8404\n",
      "Epoch [24], Step [10/19], Loss: 0.4094\n",
      "Epoch [24] Validation Loss: 0.8161\n",
      "Epoch [25], Step [10/19], Loss: 0.4180\n",
      "Epoch [25] Validation Loss: 1.8226\n",
      "Epoch [26], Step [10/19], Loss: 0.4361\n",
      "Epoch [26] Validation Loss: 2.2016\n",
      "Epoch [27], Step [10/19], Loss: 0.4012\n",
      "Epoch [27] Validation Loss: 0.6751\n",
      "Epoch [28], Step [10/19], Loss: 0.3842\n",
      "Epoch [28] Validation Loss: 0.7837\n",
      "Epoch [29], Step [10/19], Loss: 0.4416\n",
      "Epoch [29] Validation Loss: 1.2485\n",
      "Epoch [30], Step [10/19], Loss: 0.4809\n",
      "Epoch [30] Validation Loss: 0.8760\n",
      "Epoch [31], Step [10/19], Loss: 0.4137\n",
      "Epoch [31] Validation Loss: 0.4886\n",
      "✅ Saved best model at epoch 31 with val_loss=0.4886\n",
      "Epoch [32], Step [10/19], Loss: 0.4121\n",
      "Epoch [32] Validation Loss: 0.7399\n",
      "Epoch [33], Step [10/19], Loss: 0.3872\n",
      "Epoch [33] Validation Loss: 1.0718\n",
      "Epoch [34], Step [10/19], Loss: 0.3632\n",
      "Epoch [34] Validation Loss: 3.3792\n",
      "Epoch [35], Step [10/19], Loss: 0.4171\n",
      "Epoch [35] Validation Loss: 0.8737\n",
      "Epoch [36], Step [10/19], Loss: 0.4109\n",
      "Epoch [36] Validation Loss: 0.8160\n",
      "Epoch [37], Step [10/19], Loss: 0.4009\n",
      "Epoch [37] Validation Loss: 0.4931\n",
      "Epoch [38], Step [10/19], Loss: 0.4627\n",
      "Epoch [38] Validation Loss: 3.7354\n",
      "Epoch [39], Step [10/19], Loss: 0.4256\n",
      "Epoch [39] Validation Loss: 0.8541\n",
      "Epoch [40], Step [10/19], Loss: 0.3844\n",
      "Epoch [40] Validation Loss: 0.4959\n",
      "Epoch [41], Step [10/19], Loss: 0.4017\n",
      "Epoch [41] Validation Loss: 0.8851\n",
      "Epoch [42], Step [10/19], Loss: 0.3642\n",
      "Epoch [42] Validation Loss: 1.8665\n",
      "Epoch [43], Step [10/19], Loss: 0.3781\n",
      "Epoch [43] Validation Loss: 1.0375\n",
      "Epoch [44], Step [10/19], Loss: 0.3825\n",
      "Epoch [44] Validation Loss: 2.5475\n",
      "Epoch [45], Step [10/19], Loss: 0.3045\n",
      "Epoch [45] Validation Loss: 0.5699\n",
      "Epoch [46], Step [10/19], Loss: 0.3662\n",
      "Epoch [46] Validation Loss: 1.3028\n",
      "Epoch [47], Step [10/19], Loss: 0.3442\n",
      "Epoch [47] Validation Loss: 0.6100\n",
      "Epoch [48], Step [10/19], Loss: 0.3241\n",
      "Epoch [48] Validation Loss: 1.1671\n",
      "Epoch [49], Step [10/19], Loss: 0.3562\n",
      "Epoch [49] Validation Loss: 0.4931\n",
      "Epoch [50], Step [10/19], Loss: 0.3557\n",
      "Epoch [50] Validation Loss: 2.4040\n",
      "Epoch [51], Step [10/19], Loss: 0.4127\n",
      "Epoch [51] Validation Loss: 0.9321\n",
      "Epoch [52], Step [10/19], Loss: 0.3187\n",
      "Epoch [52] Validation Loss: 0.6080\n",
      "Epoch [53], Step [10/19], Loss: 0.2970\n",
      "Epoch [53] Validation Loss: 2.2643\n",
      "Epoch [54], Step [10/19], Loss: 0.2727\n",
      "Epoch [54] Validation Loss: 1.0735\n",
      "Epoch [55], Step [10/19], Loss: 0.3055\n",
      "Epoch [55] Validation Loss: 0.6733\n",
      "Epoch [56], Step [10/19], Loss: 0.3469\n",
      "Epoch [56] Validation Loss: 2.2008\n",
      "Epoch [57], Step [10/19], Loss: 0.2869\n",
      "Epoch [57] Validation Loss: 0.7470\n",
      "Epoch [58], Step [10/19], Loss: 0.2944\n",
      "Epoch [58] Validation Loss: 2.6024\n",
      "Epoch [59], Step [10/19], Loss: 0.2691\n",
      "Epoch [59] Validation Loss: 1.2777\n",
      "Epoch [60], Step [10/19], Loss: 0.2530\n",
      "Epoch [60] Validation Loss: 1.3015\n",
      "Epoch [61], Step [10/19], Loss: 0.2467\n",
      "Epoch [61] Validation Loss: 1.0038\n",
      "Epoch [62], Step [10/19], Loss: 0.2932\n",
      "Epoch [62] Validation Loss: 0.9052\n",
      "Epoch [63], Step [10/19], Loss: 0.4323\n",
      "Epoch [63] Validation Loss: 1.4127\n",
      "Epoch [64], Step [10/19], Loss: 0.3145\n",
      "Epoch [64] Validation Loss: 0.4154\n",
      "✅ Saved best model at epoch 64 with val_loss=0.4154\n",
      "Epoch [65], Step [10/19], Loss: 0.2565\n",
      "Epoch [65] Validation Loss: 1.5499\n",
      "Epoch [66], Step [10/19], Loss: 0.2649\n",
      "Epoch [66] Validation Loss: 0.4099\n",
      "✅ Saved best model at epoch 66 with val_loss=0.4099\n",
      "Epoch [67], Step [10/19], Loss: 0.2528\n",
      "Epoch [67] Validation Loss: 0.5297\n",
      "Epoch [68], Step [10/19], Loss: 0.3122\n",
      "Epoch [68] Validation Loss: 0.5548\n",
      "Epoch [69], Step [10/19], Loss: 0.3109\n",
      "Epoch [69] Validation Loss: 1.2972\n",
      "Epoch [70], Step [10/19], Loss: 0.2902\n",
      "Epoch [70] Validation Loss: 0.6684\n",
      "Epoch [71], Step [10/19], Loss: 0.2885\n",
      "Epoch [71] Validation Loss: 0.5621\n",
      "Epoch [72], Step [10/19], Loss: 0.2349\n",
      "Epoch [72] Validation Loss: 0.9842\n",
      "Epoch [73], Step [10/19], Loss: 0.2354\n",
      "Epoch [73] Validation Loss: 0.5267\n",
      "Epoch [74], Step [10/19], Loss: 0.2312\n",
      "Epoch [74] Validation Loss: 0.8082\n",
      "Epoch [75], Step [10/19], Loss: 0.2213\n",
      "Epoch [75] Validation Loss: 0.7546\n",
      "Epoch [76], Step [10/19], Loss: 0.2384\n",
      "Epoch [76] Validation Loss: 3.3281\n",
      "Epoch [77], Step [10/19], Loss: 0.3828\n",
      "Epoch [77] Validation Loss: 2.1012\n",
      "Epoch [78], Step [10/19], Loss: 0.3003\n",
      "Epoch [78] Validation Loss: 2.2919\n",
      "Epoch [79], Step [10/19], Loss: 0.2550\n",
      "Epoch [79] Validation Loss: 0.4449\n",
      "Epoch [80], Step [10/19], Loss: 0.2276\n",
      "Epoch [80] Validation Loss: 0.8003\n",
      "Epoch [81], Step [10/19], Loss: 0.2389\n",
      "Epoch [81] Validation Loss: 0.5289\n",
      "Epoch [82], Step [10/19], Loss: 0.2482\n",
      "Epoch [82] Validation Loss: 0.4934\n",
      "Epoch [83], Step [10/19], Loss: 0.3598\n",
      "Epoch [83] Validation Loss: 1.1022\n",
      "Epoch [84], Step [10/19], Loss: 0.3841\n",
      "Epoch [84] Validation Loss: 0.7040\n",
      "Epoch [85], Step [10/19], Loss: 0.3600\n",
      "Epoch [85] Validation Loss: 0.6999\n",
      "Epoch [86], Step [10/19], Loss: 0.2553\n",
      "Epoch [86] Validation Loss: 1.0299\n",
      "Epoch [87], Step [10/19], Loss: 0.2361\n",
      "Epoch [87] Validation Loss: 1.1342\n",
      "Epoch [88], Step [10/19], Loss: 0.2560\n",
      "Epoch [88] Validation Loss: 0.4606\n",
      "Epoch [89], Step [10/19], Loss: 0.2204\n",
      "Epoch [89] Validation Loss: 1.4617\n",
      "Epoch [90], Step [10/19], Loss: 0.2547\n",
      "Epoch [90] Validation Loss: 0.5679\n",
      "Epoch [91], Step [10/19], Loss: 0.2517\n",
      "Epoch [91] Validation Loss: 0.6968\n",
      "Epoch [92], Step [10/19], Loss: 0.1952\n",
      "Epoch [92] Validation Loss: 1.4023\n",
      "Epoch [93], Step [10/19], Loss: 0.2926\n",
      "Epoch [93] Validation Loss: 1.8157\n",
      "Epoch [94], Step [10/19], Loss: 0.2693\n",
      "Epoch [94] Validation Loss: 2.9646\n",
      "Epoch [95], Step [10/19], Loss: 0.2414\n",
      "Epoch [95] Validation Loss: 1.3812\n",
      "Epoch [96], Step [10/19], Loss: 0.1944\n",
      "Epoch [96] Validation Loss: 0.5749\n",
      "Epoch [97], Step [10/19], Loss: 0.2403\n",
      "Epoch [97] Validation Loss: 0.5403\n",
      "Epoch [98], Step [10/19], Loss: 0.2158\n",
      "Epoch [98] Validation Loss: 0.9113\n",
      "Epoch [99], Step [10/19], Loss: 0.2303\n",
      "Epoch [99] Validation Loss: 0.6219\n",
      "Epoch [100], Step [10/19], Loss: 0.2532\n",
      "Epoch [100] Validation Loss: 0.9024\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# -------- Config ----------\n",
    "CLASS_NUM = 37              # number of classes (0-9, A-Z, plus blank)\n",
    "MAX_LABEL_LEN = 10          # max characters per sample (fake, for model design)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LR = 1e-3\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "# --------------------------\n",
    "\n",
    "# ===== Dataset & DataLoader =====\n",
    "def ctc_collate_fn(batch):\n",
    "    images, labels, lengths = zip(*batch)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    labels = torch.cat(labels)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "    return images, labels, lengths\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=ctc_collate_fn)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=ctc_collate_fn)\n",
    "\n",
    "# ===== Model, Loss, Optimizer =====\n",
    "model = build_lprnet(MAX_LABEL_LEN, CLASS_NUM).to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"lprnet_best.pth\", map_location=DEVICE))\n",
    "\n",
    "criterion = nn.CTCLoss(blank=train_ds.char_to_idx['-'], reduction='mean')\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "# ===== Training & Validation =====\n",
    "def train_one_epoch(epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (images, labels, target_lengths) in enumerate(train_loader):\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        target_lengths = target_lengths.to(DEVICE)\n",
    "\n",
    "        # Forward\n",
    "        logits = model(images)                     # [N, C, T]\n",
    "        logits = logits.permute(2, 0, 1)           # [T, N, C]\n",
    "        log_probs = logits.log_softmax(2)\n",
    "\n",
    "        # Input lengths = all T\n",
    "        input_lengths = torch.full(size=(images.size(0),), \n",
    "                                   fill_value=logits.size(0), \n",
    "                                   dtype=torch.long).to(DEVICE)\n",
    "\n",
    "        # Loss\n",
    "        loss = criterion(log_probs, labels, input_lengths, target_lengths)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (batch_idx+1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}], Step [{batch_idx+1}/{len(train_loader)}], \"\n",
    "                  f\"Loss: {running_loss/10:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "def validate(epoch):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, target_lengths in val_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            target_lengths = target_lengths.to(DEVICE)\n",
    "\n",
    "            logits = model(images)\n",
    "            logits = logits.permute(2, 0, 1)\n",
    "            log_probs = logits.log_softmax(2)\n",
    "\n",
    "            input_lengths = torch.full(size=(images.size(0),), \n",
    "                                       fill_value=logits.size(0), \n",
    "                                       dtype=torch.long).to(DEVICE)\n",
    "\n",
    "            loss = criterion(log_probs, labels, input_lengths, target_lengths)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Epoch [{epoch+1}] Validation Loss: {val_loss:.4f}\")\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "# ===== Main Loop =====\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_one_epoch(epoch)\n",
    "    val_loss = validate(epoch)\n",
    "\n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"lprnet_best.pth\")\n",
    "        print(f\"✅ Saved best model at epoch {epoch+1} with val_loss={val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2688ad0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6055\n"
     ]
    }
   ],
   "source": [
    "def validate_test():\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, target_lengths in test_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            target_lengths = target_lengths.to(DEVICE)\n",
    "\n",
    "            logits = model(images)\n",
    "            logits = logits.permute(2, 0, 1)\n",
    "            log_probs = logits.log_softmax(2)\n",
    "\n",
    "            input_lengths = torch.full(size=(images.size(0),), \n",
    "                                       fill_value=logits.size(0), \n",
    "                                       dtype=torch.long).to(DEVICE)\n",
    "\n",
    "            loss = criterion(log_probs, labels, input_lengths, target_lengths)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    return val_loss\n",
    "\n",
    "print(f\"Test Loss: {validate_test():.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ff615b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 18])\n"
     ]
    }
   ],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(\"lprnet_best.pth\", map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=ctc_collate_fn)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels, target_lengths in test_loader:\n",
    "        images = images.to(DEVICE)\n",
    "        logits = model(images)                     # [N, C, T]\n",
    "        preds = logits.argmax(1)                   # simple greedy decode (still needs CTC decoding)\n",
    "        print(preds.shape)  # torch.Size([32, 18])\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39ac89d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(logits, idx_to_char, blank_idx):\n",
    "    \"\"\"\n",
    "    logits: [T, N, C] tensor (log probs or raw logits)\n",
    "    idx_to_char: dictionary mapping int -> char\n",
    "    blank_idx: index of blank symbol\n",
    "    \"\"\"\n",
    "    preds = logits.argmax(2).permute(1, 0)   # [N, T]\n",
    "\n",
    "    results = []\n",
    "    for pred in preds:\n",
    "        string = \"\"\n",
    "        prev = None\n",
    "        for p in pred.cpu().numpy():\n",
    "            if p != prev and p != blank_idx:   # collapse + remove blank\n",
    "                string += idx_to_char[p]\n",
    "            prev = p\n",
    "        results.append(string)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4df15a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(model, image, dataset):\n",
    "    \"\"\"\n",
    "    model   : trained model\n",
    "    image   : tensor [3, 24, 94]\n",
    "    dataset : dataset object (for idx_to_char, blank index)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image = image.unsqueeze(0).to(DEVICE)        # add batch dim [1, 3, 24, 94]\n",
    "        logits = model(image)                        # [N, C, T]\n",
    "        logits = logits.permute(2, 0, 1)             # [T, N, C]\n",
    "\n",
    "        preds = greedy_decode(logits, dataset.idx_to_char, dataset.char_to_idx['-'])\n",
    "        return preds[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b06d5e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth: 8427XX29.jpg\n",
      "Prediction  : 8427XX29\n"
     ]
    }
   ],
   "source": [
    "# Take a sample from your test set\n",
    "\n",
    "i = 9\n",
    "img, _, _ = test_ds[i]\n",
    "print(\"Ground truth:\", test_ds.image_files[i])\n",
    "\n",
    "pred = predict_image(model, img, test_ds)\n",
    "print(\"Prediction  :\", pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8001c13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT: 1033IR | Pred: 1033HR\n",
      "GT: 2348XR25 | Pred: 349Y92\n",
      "GT: 55SG53 | Pred: 56SG53\n",
      "GT: 5B40001 | Pred: 5840001\n",
      "GT: AC508V | Pred: C508V\n",
      "GT: AL193VP | Pred: AL193WP\n",
      "GT: B225RTK | Pred: B275RTK\n",
      "GT: B961TAG | Pred: B961TA\n",
      "GT: CM101LW | Pred: 4133119\n",
      "GT: CV194VA | Pred: CY194VA\n",
      "GT: EH577PH | Pred: EMH577PH\n",
      "GT: EJQ588 | Pred: E10588\n",
      "GT: EQ725QJ | Pred: CQ7250J\n",
      "GT: FC882MC | Pred: FC882WC\n",
      "GT: GB1ECB | Pred: GB1CB\n",
      "GT: H864JGM | Pred: H864JHGM\n",
      "GT: I008646 | Pred: T008646\n",
      "GT: IICE3083 | Pred: CE3683\n",
      "GT: KZ746AR | Pred: KZ7A6AR\n",
      "GT: LBHT71 | Pred: 1931331\n",
      "GT: PH20CNP | Pred: P20CN\n",
      "GT: PI36K0G | Pred: PI36KOG\n",
      "GT: SG49711 | Pred: S49711\n",
      "GT: SLF9995 | Pred: S139993\n",
      "GT: TR95XMI | Pred: TR95XM\n",
      "GT: VI496AN | Pred: V496AN\n",
      "GT: VS236891 | Pred: S236891\n",
      "Test Accuracy: 63.01%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_accuracy(model, dataset):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = len(dataset)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(total):\n",
    "            img, label_encoded, _ = dataset[i]\n",
    "\n",
    "            # Ground truth string\n",
    "            label_str = \"\".join(dataset.idx_to_char[idx.item()] for idx in label_encoded)\n",
    "\n",
    "            # Prediction\n",
    "            pred_str = predict_image(model, img, dataset)\n",
    "\n",
    "            if pred_str == label_str:\n",
    "                correct += 1\n",
    "            else:\n",
    "                print(f\"GT: {label_str} | Pred: {pred_str}\")\n",
    "                \n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# train_acc = evaluate_accuracy(model, train_ds)\n",
    "# print(f\"Train Accuracy: {train_acc:.2%}\")\n",
    "\n",
    "# val_acc = evaluate_accuracy(model, val_ds)\n",
    "# print(f\"Val Accuracy: {val_acc:.2%}\")\n",
    "\n",
    "test_acc = evaluate_accuracy(model, test_ds)\n",
    "print(f\"Test Accuracy: {test_acc:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea4a636",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lprnet-v1 (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
