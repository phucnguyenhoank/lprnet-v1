{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffa235c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 22, 92]           1,792\n",
      "       BatchNorm2d-2           [-1, 64, 22, 92]             128\n",
      "              ReLU-3           [-1, 64, 22, 92]               0\n",
      "         MaxPool3d-4           [-1, 64, 20, 90]               0\n",
      "            Conv2d-5           [-1, 32, 20, 90]           2,080\n",
      "              ReLU-6           [-1, 32, 20, 90]               0\n",
      "            Conv2d-7           [-1, 32, 20, 90]           3,104\n",
      "              ReLU-8           [-1, 32, 20, 90]               0\n",
      "            Conv2d-9           [-1, 32, 20, 90]           3,104\n",
      "             ReLU-10           [-1, 32, 20, 90]               0\n",
      "           Conv2d-11          [-1, 128, 20, 90]           4,224\n",
      "small_basic_block-12          [-1, 128, 20, 90]               0\n",
      "      BatchNorm2d-13          [-1, 128, 20, 90]             256\n",
      "             ReLU-14          [-1, 128, 20, 90]               0\n",
      "        MaxPool3d-15           [-1, 64, 18, 44]               0\n",
      "           Conv2d-16           [-1, 64, 18, 44]           4,160\n",
      "             ReLU-17           [-1, 64, 18, 44]               0\n",
      "           Conv2d-18           [-1, 64, 18, 44]          12,352\n",
      "             ReLU-19           [-1, 64, 18, 44]               0\n",
      "           Conv2d-20           [-1, 64, 18, 44]          12,352\n",
      "             ReLU-21           [-1, 64, 18, 44]               0\n",
      "           Conv2d-22          [-1, 256, 18, 44]          16,640\n",
      "small_basic_block-23          [-1, 256, 18, 44]               0\n",
      "      BatchNorm2d-24          [-1, 256, 18, 44]             512\n",
      "             ReLU-25          [-1, 256, 18, 44]               0\n",
      "           Conv2d-26           [-1, 64, 18, 44]          16,448\n",
      "             ReLU-27           [-1, 64, 18, 44]               0\n",
      "           Conv2d-28           [-1, 64, 18, 44]          12,352\n",
      "             ReLU-29           [-1, 64, 18, 44]               0\n",
      "           Conv2d-30           [-1, 64, 18, 44]          12,352\n",
      "             ReLU-31           [-1, 64, 18, 44]               0\n",
      "           Conv2d-32          [-1, 256, 18, 44]          16,640\n",
      "small_basic_block-33          [-1, 256, 18, 44]               0\n",
      "      BatchNorm2d-34          [-1, 256, 18, 44]             512\n",
      "             ReLU-35          [-1, 256, 18, 44]               0\n",
      "        MaxPool3d-36           [-1, 64, 16, 21]               0\n",
      "          Dropout-37           [-1, 64, 16, 21]               0\n",
      "           Conv2d-38          [-1, 256, 16, 18]          65,792\n",
      "      BatchNorm2d-39          [-1, 256, 16, 18]             512\n",
      "             ReLU-40          [-1, 256, 16, 18]               0\n",
      "          Dropout-41          [-1, 256, 16, 18]               0\n",
      "           Conv2d-42            [-1, 37, 4, 18]         123,173\n",
      "      BatchNorm2d-43            [-1, 37, 4, 18]              74\n",
      "             ReLU-44            [-1, 37, 4, 18]               0\n",
      "           Conv2d-45            [-1, 37, 4, 18]          17,982\n",
      "================================================================\n",
      "Total params: 326,541\n",
      "Trainable params: 326,541\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.03\n",
      "Forward/backward pass size (MB): 33.57\n",
      "Params size (MB): 1.25\n",
      "Estimated Total Size (MB): 34.84\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from model.LPRNet import build_lprnet\n",
    "from torchsummary import summary\n",
    "\n",
    "model = build_lprnet()\n",
    "summary(model, input_size=(3, 24, 94))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2709cf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import string\n",
    "\n",
    "class ImageFolderCTCDataset(Dataset):\n",
    "    def __init__(self, folder_path, image_shape=(3, 24, 94), augment=False):\n",
    "        self.folder_path = folder_path\n",
    "        self.image_files = os.listdir(folder_path)\n",
    "        _, height, width = image_shape\n",
    "\n",
    "        if augment:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((height, width)),\n",
    "                transforms.RandomAffine(\n",
    "                    degrees=5,              # small rotation (±5°)\n",
    "                    translate=(0.05, 0.05), # shift up to 5% horizontally/vertically\n",
    "                    scale=(0.9, 1.1),       # zoom in/out 10%\n",
    "                    shear=0                 # keep shear = 0 (optional)\n",
    "                ),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((height, width)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "            ])\n",
    "\n",
    "        # dictionary build\n",
    "        self.chars = list(string.digits + string.ascii_uppercase)\n",
    "        self.chars.append('-')\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.folder_path, img_name)\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # Extract label\n",
    "        label_str = img_name.split('.')[0].split('_')[0]\n",
    "        label_encoded = [self.char_to_idx[ch] for ch in label_str]\n",
    "\n",
    "        return image, torch.tensor(label_encoded, dtype=torch.long), len(label_encoded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70ae9664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 24, 94])\n",
      "tensor([ 0,  0, 21, 17,  2,  8,  7,  7])\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SHAPE = (3, 24, 94)\n",
    "train_ds = ImageFolderCTCDataset(r\"lprds\\train\", image_shape=IMAGE_SHAPE, augment=True)\n",
    "val_ds = ImageFolderCTCDataset(r\"lprds\\val\", image_shape=IMAGE_SHAPE)\n",
    "test_ds = ImageFolderCTCDataset(r\"lprds\\test\", image_shape=IMAGE_SHAPE)\n",
    "\n",
    "img, label_encoded, label_length = train_ds[0]\n",
    "\n",
    "print(img.shape)           # torch.Size([3, 24, 94])\n",
    "print(label_encoded)       # tensor([ 0,  0, 21, 17,  2,  8,  7,  7])\n",
    "print(label_length)        # 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df72ba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def ctc_collate_fn(batch):\n",
    "    images, labels, lengths = zip(*batch)  # unzip list of tuples\n",
    "\n",
    "    # Stack images [B, C, H, W]\n",
    "    images = torch.stack(images, dim=0)\n",
    "\n",
    "    # Concatenate all labels into one flat tensor\n",
    "    labels = torch.cat(labels)\n",
    "\n",
    "    # Convert lengths to tensor\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "\n",
    "    return images, labels, lengths\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=ctc_collate_fn)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=ctc_collate_fn)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=ctc_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35cbe796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 24, 94])\n",
      "torch.Size([223])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# check a batch\n",
    "images, labels, lengths = next(iter(train_loader))\n",
    "print(images.shape)   # [32, 3, 24, 94]\n",
    "print(labels.shape)   # flat 1D tensor, e.g. torch.Size([180])\n",
    "print(lengths.shape)  # [32], lengths of each label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6420c562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Step [10/19], Loss: 0.1926\n",
      "Epoch [1] Validation Loss: 0.4266\n",
      "✅ Saved best model at epoch 1 with val_loss=0.4266\n",
      "Epoch [2], Step [10/19], Loss: 0.1785\n",
      "Epoch [2] Validation Loss: 0.3499\n",
      "✅ Saved best model at epoch 2 with val_loss=0.3499\n",
      "Epoch [3], Step [10/19], Loss: 0.1999\n",
      "Epoch [3] Validation Loss: 0.3297\n",
      "✅ Saved best model at epoch 3 with val_loss=0.3297\n",
      "Epoch [4], Step [10/19], Loss: 0.1750\n",
      "Epoch [4] Validation Loss: 0.4172\n",
      "Epoch [5], Step [10/19], Loss: 0.1634\n",
      "Epoch [5] Validation Loss: 0.3556\n",
      "Epoch [6], Step [10/19], Loss: 0.1763\n",
      "Epoch [6] Validation Loss: 0.3729\n",
      "Epoch [7], Step [10/19], Loss: 0.1322\n",
      "Epoch [7] Validation Loss: 0.3417\n",
      "Epoch [8], Step [10/19], Loss: 0.1478\n",
      "Epoch [8] Validation Loss: 0.3513\n",
      "Epoch [9], Step [10/19], Loss: 0.1826\n",
      "Epoch [9] Validation Loss: 0.4258\n",
      "Epoch [10], Step [10/19], Loss: 0.2068\n",
      "Epoch [10] Validation Loss: 0.4218\n",
      "Epoch [11], Step [10/19], Loss: 0.1889\n",
      "Epoch [11] Validation Loss: 0.4206\n",
      "Epoch [12], Step [10/19], Loss: 0.1568\n",
      "Epoch [12] Validation Loss: 0.3689\n",
      "Epoch [13], Step [10/19], Loss: 0.1454\n",
      "Epoch [13] Validation Loss: 0.3123\n",
      "✅ Saved best model at epoch 13 with val_loss=0.3123\n",
      "Epoch [14], Step [10/19], Loss: 0.1586\n",
      "Epoch [14] Validation Loss: 0.3000\n",
      "✅ Saved best model at epoch 14 with val_loss=0.3000\n",
      "Epoch [15], Step [10/19], Loss: 0.1600\n",
      "Epoch [15] Validation Loss: 0.3889\n",
      "Epoch [16], Step [10/19], Loss: 0.1844\n",
      "Epoch [16] Validation Loss: 0.3270\n",
      "Epoch [17], Step [10/19], Loss: 0.1823\n",
      "Epoch [17] Validation Loss: 0.3996\n",
      "Epoch [18], Step [10/19], Loss: 0.1479\n",
      "Epoch [18] Validation Loss: 0.3549\n",
      "Epoch [19], Step [10/19], Loss: 0.1696\n",
      "Epoch [19] Validation Loss: 0.3378\n",
      "Epoch [20], Step [10/19], Loss: 0.1596\n",
      "Epoch [20] Validation Loss: 0.2947\n",
      "✅ Saved best model at epoch 20 with val_loss=0.2947\n",
      "Epoch [21], Step [10/19], Loss: 0.1657\n",
      "Epoch [21] Validation Loss: 0.3290\n",
      "Epoch [22], Step [10/19], Loss: 0.1392\n",
      "Epoch [22] Validation Loss: 0.3789\n",
      "Epoch [23], Step [10/19], Loss: 0.1926\n",
      "Epoch [23] Validation Loss: 0.3816\n",
      "Epoch [24], Step [10/19], Loss: 0.2160\n",
      "Epoch [24] Validation Loss: 0.3724\n",
      "Epoch [25], Step [10/19], Loss: 0.1668\n",
      "Epoch [25] Validation Loss: 0.2866\n",
      "✅ Saved best model at epoch 25 with val_loss=0.2866\n",
      "Epoch [26], Step [10/19], Loss: 0.1456\n",
      "Epoch [26] Validation Loss: 0.3207\n",
      "Epoch [27], Step [10/19], Loss: 0.1560\n",
      "Epoch [27] Validation Loss: 0.3105\n",
      "Epoch [28], Step [10/19], Loss: 0.1344\n",
      "Epoch [28] Validation Loss: 0.3052\n",
      "Epoch [29], Step [10/19], Loss: 0.1155\n",
      "Epoch [29] Validation Loss: 0.3031\n",
      "Epoch [30], Step [10/19], Loss: 0.2058\n",
      "Epoch [30] Validation Loss: 0.3843\n",
      "Epoch [31], Step [10/19], Loss: 0.1541\n",
      "Epoch [31] Validation Loss: 0.3026\n",
      "Epoch [32], Step [10/19], Loss: 0.1808\n",
      "Epoch [32] Validation Loss: 0.2911\n",
      "Epoch [33], Step [10/19], Loss: 0.1403\n",
      "Epoch [33] Validation Loss: 0.2536\n",
      "✅ Saved best model at epoch 33 with val_loss=0.2536\n",
      "Epoch [34], Step [10/19], Loss: 0.1570\n",
      "Epoch [34] Validation Loss: 0.3412\n",
      "Epoch [35], Step [10/19], Loss: 0.1551\n",
      "Epoch [35] Validation Loss: 0.3851\n",
      "Epoch [36], Step [10/19], Loss: 0.1137\n",
      "Epoch [36] Validation Loss: 0.3197\n",
      "Epoch [37], Step [10/19], Loss: 0.1649\n",
      "Epoch [37] Validation Loss: 0.3221\n",
      "Epoch [38], Step [10/19], Loss: 0.1651\n",
      "Epoch [38] Validation Loss: 0.2898\n",
      "Epoch [39], Step [10/19], Loss: 0.1488\n",
      "Epoch [39] Validation Loss: 0.3106\n",
      "Epoch [40], Step [10/19], Loss: 0.1257\n",
      "Epoch [40] Validation Loss: 0.3031\n",
      "Epoch [41], Step [10/19], Loss: 0.1509\n",
      "Epoch [41] Validation Loss: 0.2879\n",
      "Epoch [42], Step [10/19], Loss: 0.1299\n",
      "Epoch [42] Validation Loss: 0.3075\n",
      "Epoch [43], Step [10/19], Loss: 0.1249\n",
      "Epoch [43] Validation Loss: 0.3108\n",
      "Epoch [44], Step [10/19], Loss: 0.1337\n",
      "Epoch [44] Validation Loss: 0.3600\n",
      "Epoch [45], Step [10/19], Loss: 0.1644\n",
      "Epoch [45] Validation Loss: 0.3027\n",
      "Epoch [46], Step [10/19], Loss: 0.1898\n",
      "Epoch [46] Validation Loss: 0.3776\n",
      "Epoch [47], Step [10/19], Loss: 0.1816\n",
      "Epoch [47] Validation Loss: 0.3912\n",
      "Epoch [48], Step [10/19], Loss: 0.1614\n",
      "Epoch [48] Validation Loss: 0.4404\n",
      "Epoch [49], Step [10/19], Loss: 0.1565\n",
      "Epoch [49] Validation Loss: 0.3432\n",
      "Epoch [50], Step [10/19], Loss: 0.2302\n",
      "Epoch [50] Validation Loss: 0.3881\n",
      "Epoch [51], Step [10/19], Loss: 0.2232\n",
      "Epoch [51] Validation Loss: 0.3595\n",
      "Epoch [52], Step [10/19], Loss: 0.1394\n",
      "Epoch [52] Validation Loss: 0.3426\n",
      "Epoch [53], Step [10/19], Loss: 0.1195\n",
      "Epoch [53] Validation Loss: 0.3606\n",
      "Epoch [54], Step [10/19], Loss: 0.1452\n",
      "Epoch [54] Validation Loss: 0.3258\n",
      "Epoch [55], Step [10/19], Loss: 0.1280\n",
      "Epoch [55] Validation Loss: 0.3284\n",
      "Epoch [56], Step [10/19], Loss: 0.1361\n",
      "Epoch [56] Validation Loss: 0.2867\n",
      "Epoch [57], Step [10/19], Loss: 0.1450\n",
      "Epoch [57] Validation Loss: 0.3201\n",
      "Epoch [58], Step [10/19], Loss: 0.1255\n",
      "Epoch [58] Validation Loss: 0.3192\n",
      "Epoch [59], Step [10/19], Loss: 0.1176\n",
      "Epoch [59] Validation Loss: 0.3208\n",
      "Epoch [60], Step [10/19], Loss: 0.1311\n",
      "Epoch [60] Validation Loss: 0.3502\n",
      "Epoch [61], Step [10/19], Loss: 0.1581\n",
      "Epoch [61] Validation Loss: 0.4458\n",
      "Epoch [62], Step [10/19], Loss: 0.2230\n",
      "Epoch [62] Validation Loss: 0.3657\n",
      "Epoch [63], Step [10/19], Loss: 0.1474\n",
      "Epoch [63] Validation Loss: 0.3080\n",
      "Epoch [64], Step [10/19], Loss: 0.1379\n",
      "Epoch [64] Validation Loss: 0.3601\n",
      "Epoch [65], Step [10/19], Loss: 0.1824\n",
      "Epoch [65] Validation Loss: 0.3559\n",
      "Epoch [66], Step [10/19], Loss: 0.1685\n",
      "Epoch [66] Validation Loss: 0.3312\n",
      "Epoch [67], Step [10/19], Loss: 0.1322\n",
      "Epoch [67] Validation Loss: 0.3692\n",
      "Epoch [68], Step [10/19], Loss: 0.1418\n",
      "Epoch [68] Validation Loss: 0.4267\n",
      "Epoch [69], Step [10/19], Loss: 0.1486\n",
      "Epoch [69] Validation Loss: 0.3525\n",
      "Epoch [70], Step [10/19], Loss: 0.1082\n",
      "Epoch [70] Validation Loss: 0.3014\n",
      "Epoch [71], Step [10/19], Loss: 0.1418\n",
      "Epoch [71] Validation Loss: 0.3460\n",
      "Epoch [72], Step [10/19], Loss: 0.1082\n",
      "Epoch [72] Validation Loss: 0.3171\n",
      "Epoch [73], Step [10/19], Loss: 0.0879\n",
      "Epoch [73] Validation Loss: 0.2843\n",
      "Epoch [74], Step [10/19], Loss: 0.1043\n",
      "Epoch [74] Validation Loss: 0.3719\n",
      "Epoch [75], Step [10/19], Loss: 0.1535\n",
      "Epoch [75] Validation Loss: 0.3307\n",
      "Epoch [76], Step [10/19], Loss: 0.1412\n",
      "Epoch [76] Validation Loss: 0.3076\n",
      "Epoch [77], Step [10/19], Loss: 0.1189\n",
      "Epoch [77] Validation Loss: 0.3137\n",
      "Epoch [78], Step [10/19], Loss: 0.1443\n",
      "Epoch [78] Validation Loss: 0.3439\n",
      "Epoch [79], Step [10/19], Loss: 0.1301\n",
      "Epoch [79] Validation Loss: 0.3463\n",
      "Epoch [80], Step [10/19], Loss: 0.1373\n",
      "Epoch [80] Validation Loss: 0.3089\n",
      "Epoch [81], Step [10/19], Loss: 0.1222\n",
      "Epoch [81] Validation Loss: 0.3837\n",
      "Epoch [82], Step [10/19], Loss: 0.1421\n",
      "Epoch [82] Validation Loss: 0.3211\n",
      "Epoch [83], Step [10/19], Loss: 0.1040\n",
      "Epoch [83] Validation Loss: 0.3155\n",
      "Epoch [84], Step [10/19], Loss: 0.1192\n",
      "Epoch [84] Validation Loss: 0.3067\n",
      "Epoch [85], Step [10/19], Loss: 0.1128\n",
      "Epoch [85] Validation Loss: 0.3807\n",
      "Epoch [86], Step [10/19], Loss: 0.1236\n",
      "Epoch [86] Validation Loss: 0.3313\n",
      "Epoch [87], Step [10/19], Loss: 0.1035\n",
      "Epoch [87] Validation Loss: 0.3639\n",
      "Epoch [88], Step [10/19], Loss: 0.1326\n",
      "Epoch [88] Validation Loss: 0.3693\n",
      "Epoch [89], Step [10/19], Loss: 0.1423\n",
      "Epoch [89] Validation Loss: 0.4024\n",
      "Epoch [90], Step [10/19], Loss: 0.1247\n",
      "Epoch [90] Validation Loss: 0.3706\n",
      "Epoch [91], Step [10/19], Loss: 0.1252\n",
      "Epoch [91] Validation Loss: 0.3392\n",
      "Epoch [92], Step [10/19], Loss: 0.1199\n",
      "Epoch [92] Validation Loss: 0.3225\n",
      "Epoch [93], Step [10/19], Loss: 0.1215\n",
      "Epoch [93] Validation Loss: 0.3417\n",
      "Epoch [94], Step [10/19], Loss: 0.0960\n",
      "Epoch [94] Validation Loss: 0.3324\n",
      "Epoch [95], Step [10/19], Loss: 0.1245\n",
      "Epoch [95] Validation Loss: 0.3737\n",
      "Epoch [96], Step [10/19], Loss: 0.0866\n",
      "Epoch [96] Validation Loss: 0.3948\n",
      "Epoch [97], Step [10/19], Loss: 0.1189\n",
      "Epoch [97] Validation Loss: 0.4359\n",
      "Epoch [98], Step [10/19], Loss: 0.1528\n",
      "Epoch [98] Validation Loss: 0.3721\n",
      "Epoch [99], Step [10/19], Loss: 0.1656\n",
      "Epoch [99] Validation Loss: 0.4788\n",
      "Epoch [100], Step [10/19], Loss: 0.1419\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 100\u001b[39m\n\u001b[32m     97\u001b[39m best_val_loss = \u001b[38;5;28mfloat\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33minf\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m     val_loss = validate(epoch)\n\u001b[32m    103\u001b[39m     \u001b[38;5;66;03m# Save best model\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(epoch)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[32m     58\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m optimizer.step()\n\u001b[32m     62\u001b[39m running_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\code\\lprnet-v1\\.venv\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\code\\lprnet-v1\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\code\\lprnet-v1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# -------- Config ----------\n",
    "CLASS_NUM = 37              # number of classes (0-9, A-Z, plus blank)\n",
    "MAX_LABEL_LEN = 10          # max characters per sample (fake, for model design)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LR = 1e-3\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 32\n",
    "# --------------------------\n",
    "\n",
    "# ===== Dataset & DataLoader =====\n",
    "def ctc_collate_fn(batch):\n",
    "    images, labels, lengths = zip(*batch)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    labels = torch.cat(labels)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "    return images, labels, lengths\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=ctc_collate_fn)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=ctc_collate_fn)\n",
    "\n",
    "# ===== Model, Loss, Optimizer =====\n",
    "model = build_lprnet(MAX_LABEL_LEN, CLASS_NUM).to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"lprnet_best.pth\", map_location=DEVICE))\n",
    "\n",
    "criterion = nn.CTCLoss(blank=train_ds.char_to_idx['-'], reduction='mean')\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "# ===== Training & Validation =====\n",
    "def train_one_epoch(epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (images, labels, target_lengths) in enumerate(train_loader):\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        target_lengths = target_lengths.to(DEVICE)\n",
    "\n",
    "        # Forward\n",
    "        logits = model(images)                     # [N, C, T]\n",
    "        logits = logits.permute(2, 0, 1)           # [T, N, C]\n",
    "        log_probs = logits.log_softmax(2)\n",
    "\n",
    "        # Input lengths = all T\n",
    "        input_lengths = torch.full(size=(images.size(0),), \n",
    "                                   fill_value=logits.size(0), \n",
    "                                   dtype=torch.long).to(DEVICE)\n",
    "\n",
    "        # Loss\n",
    "        loss = criterion(log_probs, labels, input_lengths, target_lengths)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (batch_idx+1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}], Step [{batch_idx+1}/{len(train_loader)}], \"\n",
    "                  f\"Loss: {running_loss/10:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "def validate(epoch):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, target_lengths in val_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            target_lengths = target_lengths.to(DEVICE)\n",
    "\n",
    "            logits = model(images)\n",
    "            logits = logits.permute(2, 0, 1)\n",
    "            log_probs = logits.log_softmax(2)\n",
    "\n",
    "            input_lengths = torch.full(size=(images.size(0),), \n",
    "                                       fill_value=logits.size(0), \n",
    "                                       dtype=torch.long).to(DEVICE)\n",
    "\n",
    "            loss = criterion(log_probs, labels, input_lengths, target_lengths)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Epoch [{epoch+1}] Validation Loss: {val_loss:.4f}\")\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "# ===== Main Loop =====\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_one_epoch(epoch)\n",
    "    val_loss = validate(epoch)\n",
    "\n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"lprnet_best.pth\")\n",
    "        print(f\"✅ Saved best model at epoch {epoch+1} with val_loss={val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2688ad0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1876\n"
     ]
    }
   ],
   "source": [
    "def validate_test():\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, target_lengths in test_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            target_lengths = target_lengths.to(DEVICE)\n",
    "\n",
    "            logits = model(images)\n",
    "            logits = logits.permute(2, 0, 1)\n",
    "            log_probs = logits.log_softmax(2)\n",
    "\n",
    "            input_lengths = torch.full(size=(images.size(0),), \n",
    "                                       fill_value=logits.size(0), \n",
    "                                       dtype=torch.long).to(DEVICE)\n",
    "\n",
    "            loss = criterion(log_probs, labels, input_lengths, target_lengths)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    return val_loss\n",
    "\n",
    "print(f\"Test Loss: {validate_test():.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ff615b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 18])\n"
     ]
    }
   ],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(\"lprnet_best.pth\", map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=ctc_collate_fn)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels, target_lengths in test_loader:\n",
    "        images = images.to(DEVICE)\n",
    "        logits = model(images)                     # [N, C, T]\n",
    "        preds = logits.argmax(1)                   # simple greedy decode (still needs CTC decoding)\n",
    "        print(preds.shape)  # torch.Size([32, 18])\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39ac89d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(logits, idx_to_char, blank_idx):\n",
    "    \"\"\"\n",
    "    logits: [T, N, C] tensor (log probs or raw logits)\n",
    "    idx_to_char: dictionary mapping int -> char\n",
    "    blank_idx: index of blank symbol\n",
    "    \"\"\"\n",
    "    preds = logits.argmax(2).permute(1, 0)   # [N, T]\n",
    "\n",
    "    results = []\n",
    "    for pred in preds:\n",
    "        string = \"\"\n",
    "        prev = None\n",
    "        for p in pred.cpu().numpy():\n",
    "            if p != prev and p != blank_idx:   # collapse + remove blank\n",
    "                string += idx_to_char[p]\n",
    "            prev = p\n",
    "        results.append(string)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4df15a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(model, image, dataset):\n",
    "    \"\"\"\n",
    "    model   : trained model\n",
    "    image   : tensor [3, 24, 94]\n",
    "    dataset : dataset object (for idx_to_char, blank index)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image = image.unsqueeze(0).to(DEVICE)        # add batch dim [1, 3, 24, 94]\n",
    "        logits = model(image)                        # [N, C, T]\n",
    "        logits = logits.permute(2, 0, 1)             # [T, N, C]\n",
    "\n",
    "        preds = greedy_decode(logits, dataset.idx_to_char, dataset.char_to_idx['-'])\n",
    "        return preds[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b06d5e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth: 8427XX29.jpg\n",
      "Prediction  : 8427XX29\n"
     ]
    }
   ],
   "source": [
    "# Take a sample from your test set\n",
    "\n",
    "i = 9\n",
    "img, _, _ = test_ds[i]\n",
    "print(\"Ground truth:\", test_ds.image_files[i])\n",
    "\n",
    "pred = predict_image(model, img, test_ds)\n",
    "print(\"Prediction  :\", pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8001c13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT: 1033IR | Pred: 1033TR\n",
      "GT: 381ATK83 | Pred: 381ATK683\n",
      "GT: 5B40001 | Pred: 5840001\n",
      "GT: AL193VP | Pred: 193VP\n",
      "GT: B21GSB | Pred: B2GSB\n",
      "GT: B225RTK | Pred: B25RTK\n",
      "GT: B961TAG | Pred: B961TA\n",
      "GT: BA999ZZ | Pred: BA9992Z\n",
      "GT: CM101LW | Pred: CM1D1LV\n",
      "GT: EJQ588 | Pred: EJ0588\n",
      "GT: EM727PD | Pred: EM77PD\n",
      "GT: EQ725QJ | Pred: EQ7250J\n",
      "GT: I008646 | Pred: T008646\n",
      "GT: IICE3083 | Pred: CE3683\n",
      "GT: KZ746AR | Pred: KZ7A6AR\n",
      "GT: LBHT71 | Pred: L8H71\n",
      "GT: LYF277 | Pred: EYF277\n",
      "GT: PH20CNP | Pred: P20CNP\n",
      "GT: PI36K0G | Pred: PI36K0\n",
      "GT: SG49711 | Pred: S49711\n",
      "GT: VI496AN | Pred: VYI496AN\n",
      "GT: WW697BH | Pred: VW697BH\n",
      "Test Accuracy: 69.86%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_accuracy(model, dataset):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = len(dataset)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(total):\n",
    "            img, label_encoded, _ = dataset[i]\n",
    "\n",
    "            # Ground truth string\n",
    "            label_str = \"\".join(dataset.idx_to_char[idx.item()] for idx in label_encoded)\n",
    "\n",
    "            # Prediction\n",
    "            pred_str = predict_image(model, img, dataset)\n",
    "\n",
    "            if pred_str == label_str:\n",
    "                correct += 1\n",
    "            else:\n",
    "                print(f\"GT: {label_str} | Pred: {pred_str}\")\n",
    "                \n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# train_acc = evaluate_accuracy(model, train_ds)\n",
    "# print(f\"Train Accuracy: {train_acc:.2%}\")\n",
    "\n",
    "# val_acc = evaluate_accuracy(model, val_ds)\n",
    "# print(f\"Val Accuracy: {val_acc:.2%}\")\n",
    "\n",
    "test_acc = evaluate_accuracy(model, test_ds)\n",
    "print(f\"Test Accuracy: {test_acc:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea4a636",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lprnet-v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
