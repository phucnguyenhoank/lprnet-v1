{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82L5PopZUhM2",
        "outputId": "613e40c4-f878-4f8c-e3f7-e20fb2dadc3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 8, 18, 88]           1,184\n",
            "         MaxPool2d-2             [-1, 8, 9, 44]               0\n",
            "              ReLU-3             [-1, 8, 9, 44]               0\n",
            "            Conv2d-4            [-1, 10, 5, 40]           2,010\n",
            "         MaxPool2d-5            [-1, 10, 2, 20]               0\n",
            "              ReLU-6            [-1, 10, 2, 20]               0\n",
            "            Linear-7                   [-1, 32]          12,832\n",
            "              ReLU-8                   [-1, 32]               0\n",
            "            Linear-9                    [-1, 6]             198\n",
            "           Conv2d-10           [-1, 64, 22, 92]           1,792\n",
            "      BatchNorm2d-11           [-1, 64, 22, 92]             128\n",
            "             ReLU-12           [-1, 64, 22, 92]               0\n",
            "        MaxPool3d-13           [-1, 64, 20, 90]               0\n",
            "           Conv2d-14           [-1, 32, 20, 90]           2,080\n",
            "             ReLU-15           [-1, 32, 20, 90]               0\n",
            "           Conv2d-16           [-1, 32, 20, 90]           3,104\n",
            "             ReLU-17           [-1, 32, 20, 90]               0\n",
            "           Conv2d-18           [-1, 32, 20, 90]           3,104\n",
            "             ReLU-19           [-1, 32, 20, 90]               0\n",
            "           Conv2d-20          [-1, 128, 20, 90]           4,224\n",
            "small_basic_block-21          [-1, 128, 20, 90]               0\n",
            "      BatchNorm2d-22          [-1, 128, 20, 90]             256\n",
            "             ReLU-23          [-1, 128, 20, 90]               0\n",
            "        MaxPool3d-24           [-1, 64, 18, 44]               0\n",
            "           Conv2d-25           [-1, 64, 18, 44]           4,160\n",
            "             ReLU-26           [-1, 64, 18, 44]               0\n",
            "           Conv2d-27           [-1, 64, 18, 44]          12,352\n",
            "             ReLU-28           [-1, 64, 18, 44]               0\n",
            "           Conv2d-29           [-1, 64, 18, 44]          12,352\n",
            "             ReLU-30           [-1, 64, 18, 44]               0\n",
            "           Conv2d-31          [-1, 256, 18, 44]          16,640\n",
            "small_basic_block-32          [-1, 256, 18, 44]               0\n",
            "      BatchNorm2d-33          [-1, 256, 18, 44]             512\n",
            "             ReLU-34          [-1, 256, 18, 44]               0\n",
            "           Conv2d-35           [-1, 64, 18, 44]          16,448\n",
            "             ReLU-36           [-1, 64, 18, 44]               0\n",
            "           Conv2d-37           [-1, 64, 18, 44]          12,352\n",
            "             ReLU-38           [-1, 64, 18, 44]               0\n",
            "           Conv2d-39           [-1, 64, 18, 44]          12,352\n",
            "             ReLU-40           [-1, 64, 18, 44]               0\n",
            "           Conv2d-41          [-1, 256, 18, 44]          16,640\n",
            "small_basic_block-42          [-1, 256, 18, 44]               0\n",
            "      BatchNorm2d-43          [-1, 256, 18, 44]             512\n",
            "             ReLU-44          [-1, 256, 18, 44]               0\n",
            "        MaxPool3d-45           [-1, 64, 16, 21]               0\n",
            "          Dropout-46           [-1, 64, 16, 21]               0\n",
            "           Conv2d-47          [-1, 256, 16, 18]          65,792\n",
            "      BatchNorm2d-48          [-1, 256, 16, 18]             512\n",
            "             ReLU-49          [-1, 256, 16, 18]               0\n",
            "          Dropout-50          [-1, 256, 16, 18]               0\n",
            "           Conv2d-51            [-1, 37, 4, 18]         123,173\n",
            "      BatchNorm2d-52            [-1, 37, 4, 18]              74\n",
            "             ReLU-53            [-1, 37, 4, 18]               0\n",
            "           Conv2d-54            [-1, 37, 4, 18]          17,982\n",
            "================================================================\n",
            "Total params: 342,765\n",
            "Trainable params: 342,765\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.03\n",
            "Forward/backward pass size (MB): 33.74\n",
            "Params size (MB): 1.31\n",
            "Estimated Total Size (MB): 35.07\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "\n",
        "class small_basic_block(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out):\n",
        "        super(small_basic_block, self).__init__()\n",
        "        q_ch_out = ch_out // 4\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(ch_in, q_ch_out, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(q_ch_out, q_ch_out, kernel_size=(3, 1), padding=(1, 0)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(q_ch_out, q_ch_out, kernel_size=(1, 3), padding=(0, 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(q_ch_out, ch_out, kernel_size=1),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class LPRNet(nn.Module):\n",
        "    def __init__(self, lpr_max_len, class_num, dropout_rate=0.5):\n",
        "        super(LPRNet, self).__init__()\n",
        "        self.lpr_max_len = lpr_max_len\n",
        "        self.class_num = class_num\n",
        "\n",
        "        self.backbone = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),  # 2\n",
        "            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 1, 1)),\n",
        "            small_basic_block(ch_in=64, ch_out=128),  # [-1, 128, 20, 90]\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            nn.ReLU(),  # 6\n",
        "            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(2, 1, 2)),\n",
        "            small_basic_block(ch_in=64, ch_out=256),\n",
        "            nn.BatchNorm2d(num_features=256),\n",
        "            nn.ReLU(),\n",
        "            small_basic_block(ch_in=256, ch_out=256),\n",
        "            nn.BatchNorm2d(num_features=256),\n",
        "            nn.ReLU(),  # 13\n",
        "            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(4, 1, 2)),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Conv2d(in_channels=64, out_channels=256, kernel_size=(1, 4), stride=1),\n",
        "            nn.BatchNorm2d(num_features=256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Conv2d(in_channels=256, out_channels=class_num, kernel_size=(13, 1), stride=1),\n",
        "            nn.BatchNorm2d(num_features=class_num),\n",
        "            nn.ReLU(),  # 22\n",
        "        )\n",
        "\n",
        "        self.container = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=448 + self.class_num,\n",
        "                      out_channels=self.class_num,\n",
        "                      kernel_size=(1, 1), stride=(1, 1)),\n",
        "        )\n",
        "\n",
        "        self.localization = nn.Sequential(\n",
        "            nn.Conv2d(3, 8, kernel_size=7),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(8, 10, kernel_size=5),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        # Regressor for the 3 * 2 affine matrix\n",
        "        self.fc_loc = nn.Sequential(\n",
        "            nn.Linear(10 * 2 * 20, 32),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(32, 3 * 2)\n",
        "        )\n",
        "\n",
        "        # Initialize the weights/bias with identity transformation\n",
        "        self.fc_loc[2].weight.data.zero_()\n",
        "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
        "\n",
        "    # Spatial transformer network forward function\n",
        "    def stn(self, x):\n",
        "        # x = torch.Size([2, 1, 24, 94])\n",
        "        xs = self.localization(x)\n",
        "        xs = xs.view(xs.size(0), -1) # torch.Size([2, 10, 2, 20])\n",
        "        theta = self.fc_loc(xs)  # torch.Size([2, 6])\n",
        "        theta = theta.view(theta.size(0), 2, 3)\n",
        "\n",
        "        grid = F.affine_grid(theta, x.size(), align_corners=True)\n",
        "        x = F.grid_sample(x, grid, align_corners=True)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        # transform the input\n",
        "        x = self.stn(x)\n",
        "\n",
        "        keep_features = list()\n",
        "        for i, layer in enumerate(self.backbone.children()):\n",
        "            x = layer(x)\n",
        "            if i in [2, 6, 13, 22]:\n",
        "                keep_features.append(x)\n",
        "\n",
        "        global_context = list()\n",
        "        for i, f in enumerate(keep_features):\n",
        "            if i in [0, 1]:\n",
        "                f = nn.AvgPool2d(kernel_size=5, stride=5)(f)\n",
        "            if i in [2]:\n",
        "                f = nn.AvgPool2d(kernel_size=(4, 10), stride=(4, 2))(f)\n",
        "            f_pow = torch.pow(f, 2)\n",
        "            f_mean = torch.mean(f_pow)\n",
        "            f = torch.div(f, f_mean)\n",
        "            global_context.append(f)\n",
        "\n",
        "        x = torch.cat(global_context, 1)\n",
        "        x = self.container(x)\n",
        "        logits = torch.mean(x, dim=2)\n",
        "        return logits\n",
        "\n",
        "    def show_num_layer(self):\n",
        "        for i, layer in enumerate(self.backbone.children()):\n",
        "            print(f\"{i}: {layer}\")\n",
        "\n",
        "\n",
        "def build_lprnet(lpr_max_len=10, class_num=37, dropout_rate=0.5):\n",
        "    return LPRNet(lpr_max_len, class_num, dropout_rate)\n",
        "\n",
        "# Define DEVICE before building the model\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = build_lprnet().to(DEVICE) # Move model to DEVICE\n",
        "summary(model, input_size=(3, 24, 94), device=DEVICE.type) # Specify device for summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import string\n",
        "\n",
        "class ImageFolderCTCDataset(Dataset):\n",
        "    def __init__(self, folder_path, image_shape=(3, 24, 94), augment=False):\n",
        "        self.folder_path = folder_path\n",
        "        self.image_files = os.listdir(folder_path)\n",
        "        _, height, width = image_shape\n",
        "\n",
        "        if augment:\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize((height, width)),\n",
        "                transforms.RandomAffine(\n",
        "                    degrees=5,\n",
        "                    translate=(0.05, 0.05),\n",
        "                    scale=(0.9, 1.1),\n",
        "                    shear=0\n",
        "                ),\n",
        "                transforms.ToTensor(),  # RGB tensor in [0,1], shape (3,H,W)\n",
        "            ])\n",
        "        else:\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize((height, width)),\n",
        "                transforms.ToTensor(),  # RGB tensor in [0,1], shape (3,H,W)\n",
        "            ])\n",
        "\n",
        "        # dictionary build\n",
        "        self.chars = list(string.digits + string.ascii_uppercase)\n",
        "        self.chars.append('-')\n",
        "        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}\n",
        "        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = os.path.join(self.folder_path, img_name)\n",
        "\n",
        "        # Load as RGB\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        image = self.transform(image)\n",
        "\n",
        "        # Extract label\n",
        "        label_str = img_name.split('.')[0].split('_')[0]\n",
        "        label_encoded = [self.char_to_idx[ch] for ch in label_str]\n",
        "\n",
        "        return image, torch.tensor(label_encoded, dtype=torch.long), len(label_encoded)\n"
      ],
      "metadata": {
        "id": "B0COCj_9U9Lk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SHAPE = (3, 24, 94)\n",
        "data_set_folder = r\"/content/drive/MyDrive/Colab Notebooks/lprnet_split_data\"\n",
        "train_ds = ImageFolderCTCDataset(data_set_folder + \"/train\", image_shape=IMAGE_SHAPE, augment=True)\n",
        "val_ds = ImageFolderCTCDataset(data_set_folder + \"/val\", image_shape=IMAGE_SHAPE)\n",
        "test_ds = ImageFolderCTCDataset(data_set_folder + \"/test\", image_shape=IMAGE_SHAPE)\n",
        "\n",
        "img, label_encoded, label_length = train_ds[0]\n",
        "\n",
        "print(img.shape)           # torch.Size([3, 24, 94])\n",
        "print(label_encoded)       # tensor([ 0,  0, 21, 17,  2,  8,  7,  7])\n",
        "print(label_length)        # 8\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBOIUm2QXlkA",
        "outputId": "3863a1de-9e69-43b3-c678-ef991a45c0e1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 24, 94])\n",
            "tensor([10, 12,  1,  7,  5,  8])\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# -------- Config ----------\n",
        "CLASS_NUM = 37              # number of classes (0-9, A-Z, plus blank)\n",
        "MAX_LABEL_LEN = 10          # max characters per sample (fake, for model design)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "LR = 1e-3\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 32\n",
        "# --------------------------\n",
        "\n",
        "# ===== Dataset & DataLoader =====\n",
        "def ctc_collate_fn(batch):\n",
        "    images, labels, lengths = zip(*batch)\n",
        "    images = torch.stack(images, dim=0)\n",
        "    labels = torch.cat(labels)\n",
        "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
        "    return images, labels, lengths\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=ctc_collate_fn)\n",
        "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=ctc_collate_fn)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=ctc_collate_fn)\n",
        "\n",
        "# check a batch\n",
        "images, labels, lengths = next(iter(train_loader))\n",
        "print(images.shape)   # [32, 3, 24, 94]\n",
        "print(labels.shape)   # flat 1D tensor, e.g. torch.Size([180])\n",
        "print(lengths.shape)  # [32], lengths of each label\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxvfA5uXY_uu",
        "outputId": "1351bf75-f75b-43a7-b9b3-f6fb9d5075cf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3, 24, 94])\n",
            "torch.Size([192])\n",
            "torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ===== Model, Loss, Optimizer =====\n",
        "model = build_lprnet(MAX_LABEL_LEN, CLASS_NUM).to(DEVICE)\n",
        "model.load_state_dict(torch.load(\"chinese_lprnet_best.pth\", map_location=DEVICE))\n",
        "\n",
        "criterion = nn.CTCLoss(blank=train_ds.char_to_idx['-'], reduction='mean')\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "\n",
        "# ===== Training & Validation =====\n",
        "def train_one_epoch(epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (images, labels, target_lengths) in enumerate(train_loader):\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        target_lengths = target_lengths.to(DEVICE)\n",
        "\n",
        "        # Forward\n",
        "        logits = model(images)                     # [N, C, T]\n",
        "        logits = logits.permute(2, 0, 1)           # [T, N, C]\n",
        "        log_probs = logits.log_softmax(2)\n",
        "\n",
        "        # Input lengths = all T\n",
        "        input_lengths = torch.full(size=(images.size(0),),\n",
        "                                   fill_value=logits.size(0),\n",
        "                                   dtype=torch.long).to(DEVICE)\n",
        "\n",
        "        # Loss\n",
        "        loss = criterion(log_probs, labels, input_lengths, target_lengths)\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if (batch_idx+1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}], Step [{batch_idx+1}/{len(train_loader)}], \"\n",
        "                  f\"Loss: {running_loss/10:.4f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "\n",
        "def validate(epoch):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels, target_lengths in val_loader:\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "            target_lengths = target_lengths.to(DEVICE)\n",
        "\n",
        "            logits = model(images)\n",
        "            logits = logits.permute(2, 0, 1)\n",
        "            log_probs = logits.log_softmax(2)\n",
        "\n",
        "            input_lengths = torch.full(size=(images.size(0),),\n",
        "                                       fill_value=logits.size(0),\n",
        "                                       dtype=torch.long).to(DEVICE)\n",
        "\n",
        "            loss = criterion(log_probs, labels, input_lengths, target_lengths)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    print(f\"Epoch [{epoch+1}] Validation Loss: {val_loss:.4f}\")\n",
        "    return val_loss"
      ],
      "metadata": {
        "id": "bE4K2zTvZ-fX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Main Loop =====\n",
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train_one_epoch(epoch)\n",
        "    val_loss = validate(epoch)\n",
        "\n",
        "    # Save best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), f\"chinese_lprnet_best_{str(best_val_loss).replace('.', '_')}.pth\")\n",
        "        print(f\"✅ Saved best model at epoch {epoch+1} with val_loss={val_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhFdLSEKaQvP",
        "outputId": "a241439b-14a9-4cdf-d8f4-fc78792c3441"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1], Step [10/25], Loss: 4.5267\n",
            "Epoch [1], Step [20/25], Loss: 3.4087\n",
            "Epoch [1] Validation Loss: 23.6184\n",
            "✅ Saved best model at epoch 1 with val_loss=23.6184\n",
            "Epoch [2], Step [10/25], Loss: 3.0917\n",
            "Epoch [2], Step [20/25], Loss: 3.0818\n",
            "Epoch [2] Validation Loss: 7.2242\n",
            "✅ Saved best model at epoch 2 with val_loss=7.2242\n",
            "Epoch [3], Step [10/25], Loss: 2.9596\n",
            "Epoch [3], Step [20/25], Loss: 2.9706\n",
            "Epoch [3] Validation Loss: 3.1290\n",
            "✅ Saved best model at epoch 3 with val_loss=3.1290\n",
            "Epoch [4], Step [10/25], Loss: 2.8922\n",
            "Epoch [4], Step [20/25], Loss: 2.8405\n",
            "Epoch [4] Validation Loss: 2.9501\n",
            "✅ Saved best model at epoch 4 with val_loss=2.9501\n",
            "Epoch [5], Step [10/25], Loss: 2.7829\n",
            "Epoch [5], Step [20/25], Loss: 2.7819\n",
            "Epoch [5] Validation Loss: 2.9520\n",
            "Epoch [6], Step [10/25], Loss: 2.6637\n",
            "Epoch [6], Step [20/25], Loss: 2.5567\n",
            "Epoch [6] Validation Loss: 2.7913\n",
            "✅ Saved best model at epoch 6 with val_loss=2.7913\n",
            "Epoch [7], Step [10/25], Loss: 2.3956\n",
            "Epoch [7], Step [20/25], Loss: 2.2667\n",
            "Epoch [7] Validation Loss: 2.7152\n",
            "✅ Saved best model at epoch 7 with val_loss=2.7152\n",
            "Epoch [8], Step [10/25], Loss: 2.0295\n",
            "Epoch [8], Step [20/25], Loss: 1.8713\n",
            "Epoch [8] Validation Loss: 1.8869\n",
            "✅ Saved best model at epoch 8 with val_loss=1.8869\n",
            "Epoch [9], Step [10/25], Loss: 1.6120\n",
            "Epoch [9], Step [20/25], Loss: 1.4026\n",
            "Epoch [9] Validation Loss: 1.7345\n",
            "✅ Saved best model at epoch 9 with val_loss=1.7345\n",
            "Epoch [10], Step [10/25], Loss: 1.1859\n",
            "Epoch [10], Step [20/25], Loss: 1.0911\n",
            "Epoch [10] Validation Loss: 1.1189\n",
            "✅ Saved best model at epoch 10 with val_loss=1.1189\n",
            "Epoch [11], Step [10/25], Loss: 0.9709\n",
            "Epoch [11], Step [20/25], Loss: 0.9272\n",
            "Epoch [11] Validation Loss: 0.8460\n",
            "✅ Saved best model at epoch 11 with val_loss=0.8460\n",
            "Epoch [12], Step [10/25], Loss: 0.7619\n",
            "Epoch [12], Step [20/25], Loss: 0.7352\n",
            "Epoch [12] Validation Loss: 0.7455\n",
            "✅ Saved best model at epoch 12 with val_loss=0.7455\n",
            "Epoch [13], Step [10/25], Loss: 0.6989\n",
            "Epoch [13], Step [20/25], Loss: 0.6439\n",
            "Epoch [13] Validation Loss: 0.8222\n",
            "Epoch [14], Step [10/25], Loss: 0.5898\n",
            "Epoch [14], Step [20/25], Loss: 0.5920\n",
            "Epoch [14] Validation Loss: 0.6490\n",
            "✅ Saved best model at epoch 14 with val_loss=0.6490\n",
            "Epoch [15], Step [10/25], Loss: 0.5480\n",
            "Epoch [15], Step [20/25], Loss: 0.5206\n",
            "Epoch [15] Validation Loss: 0.6542\n",
            "Epoch [16], Step [10/25], Loss: 0.4715\n",
            "Epoch [16], Step [20/25], Loss: 0.4482\n",
            "Epoch [16] Validation Loss: 0.6243\n",
            "✅ Saved best model at epoch 16 with val_loss=0.6243\n",
            "Epoch [17], Step [10/25], Loss: 0.4445\n",
            "Epoch [17], Step [20/25], Loss: 0.4727\n",
            "Epoch [17] Validation Loss: 0.4057\n",
            "✅ Saved best model at epoch 17 with val_loss=0.4057\n",
            "Epoch [18], Step [10/25], Loss: 0.3475\n",
            "Epoch [18], Step [20/25], Loss: 0.3621\n",
            "Epoch [18] Validation Loss: 0.4538\n",
            "Epoch [19], Step [10/25], Loss: 0.3197\n",
            "Epoch [19], Step [20/25], Loss: 0.3965\n",
            "Epoch [19] Validation Loss: 0.3725\n",
            "✅ Saved best model at epoch 19 with val_loss=0.3725\n",
            "Epoch [20], Step [10/25], Loss: 0.3293\n",
            "Epoch [20], Step [20/25], Loss: 0.3187\n",
            "Epoch [20] Validation Loss: 0.3327\n",
            "✅ Saved best model at epoch 20 with val_loss=0.3327\n",
            "Epoch [21], Step [10/25], Loss: 0.3278\n",
            "Epoch [21], Step [20/25], Loss: 0.2709\n",
            "Epoch [21] Validation Loss: 0.3350\n",
            "Epoch [22], Step [10/25], Loss: 0.2738\n",
            "Epoch [22], Step [20/25], Loss: 0.2810\n",
            "Epoch [22] Validation Loss: 0.2255\n",
            "✅ Saved best model at epoch 22 with val_loss=0.2255\n",
            "Epoch [23], Step [10/25], Loss: 0.2722\n",
            "Epoch [23], Step [20/25], Loss: 0.2728\n",
            "Epoch [23] Validation Loss: 0.3302\n",
            "Epoch [24], Step [10/25], Loss: 0.2723\n",
            "Epoch [24], Step [20/25], Loss: 0.2653\n",
            "Epoch [24] Validation Loss: 0.2138\n",
            "✅ Saved best model at epoch 24 with val_loss=0.2138\n",
            "Epoch [25], Step [10/25], Loss: 0.2363\n",
            "Epoch [25], Step [20/25], Loss: 0.2630\n",
            "Epoch [25] Validation Loss: 0.3710\n",
            "Epoch [26], Step [10/25], Loss: 0.2186\n",
            "Epoch [26], Step [20/25], Loss: 0.2299\n",
            "Epoch [26] Validation Loss: 0.2097\n",
            "✅ Saved best model at epoch 26 with val_loss=0.2097\n",
            "Epoch [27], Step [10/25], Loss: 0.2320\n",
            "Epoch [27], Step [20/25], Loss: 0.2063\n",
            "Epoch [27] Validation Loss: 0.1966\n",
            "✅ Saved best model at epoch 27 with val_loss=0.1966\n",
            "Epoch [28], Step [10/25], Loss: 0.2465\n",
            "Epoch [28], Step [20/25], Loss: 0.1998\n",
            "Epoch [28] Validation Loss: 0.2576\n",
            "Epoch [29], Step [10/25], Loss: 0.1398\n",
            "Epoch [29], Step [20/25], Loss: 0.1800\n",
            "Epoch [29] Validation Loss: 0.2582\n",
            "Epoch [30], Step [10/25], Loss: 0.2151\n",
            "Epoch [30], Step [20/25], Loss: 0.1868\n",
            "Epoch [30] Validation Loss: 0.2942\n",
            "Epoch [31], Step [10/25], Loss: 0.1552\n",
            "Epoch [31], Step [20/25], Loss: 0.2027\n",
            "Epoch [31] Validation Loss: 0.3232\n",
            "Epoch [32], Step [10/25], Loss: 0.2278\n",
            "Epoch [32], Step [20/25], Loss: 0.2285\n",
            "Epoch [32] Validation Loss: 0.2669\n",
            "Epoch [33], Step [10/25], Loss: 0.1722\n",
            "Epoch [33], Step [20/25], Loss: 0.2047\n",
            "Epoch [33] Validation Loss: 0.1700\n",
            "✅ Saved best model at epoch 33 with val_loss=0.1700\n",
            "Epoch [34], Step [10/25], Loss: 0.1708\n",
            "Epoch [34], Step [20/25], Loss: 0.1586\n",
            "Epoch [34] Validation Loss: 0.2750\n",
            "Epoch [35], Step [10/25], Loss: 0.2032\n",
            "Epoch [35], Step [20/25], Loss: 0.1597\n",
            "Epoch [35] Validation Loss: 0.1167\n",
            "✅ Saved best model at epoch 35 with val_loss=0.1167\n",
            "Epoch [36], Step [10/25], Loss: 0.1529\n",
            "Epoch [36], Step [20/25], Loss: 0.1543\n",
            "Epoch [36] Validation Loss: 0.2926\n",
            "Epoch [37], Step [10/25], Loss: 0.1807\n",
            "Epoch [37], Step [20/25], Loss: 0.1402\n",
            "Epoch [37] Validation Loss: 0.1453\n",
            "Epoch [38], Step [10/25], Loss: 0.1136\n",
            "Epoch [38], Step [20/25], Loss: 0.1123\n",
            "Epoch [38] Validation Loss: 0.1803\n",
            "Epoch [39], Step [10/25], Loss: 0.1493\n",
            "Epoch [39], Step [20/25], Loss: 0.1464\n",
            "Epoch [39] Validation Loss: 0.2060\n",
            "Epoch [40], Step [10/25], Loss: 0.1438\n",
            "Epoch [40], Step [20/25], Loss: 0.1491\n",
            "Epoch [40] Validation Loss: 0.1773\n",
            "Epoch [41], Step [10/25], Loss: 0.1494\n",
            "Epoch [41], Step [20/25], Loss: 0.1297\n",
            "Epoch [41] Validation Loss: 0.0982\n",
            "✅ Saved best model at epoch 41 with val_loss=0.0982\n",
            "Epoch [42], Step [10/25], Loss: 0.1624\n",
            "Epoch [42], Step [20/25], Loss: 0.1484\n",
            "Epoch [42] Validation Loss: 0.0846\n",
            "✅ Saved best model at epoch 42 with val_loss=0.0846\n",
            "Epoch [43], Step [10/25], Loss: 0.1086\n",
            "Epoch [43], Step [20/25], Loss: 0.1393\n",
            "Epoch [43] Validation Loss: 0.1327\n",
            "Epoch [44], Step [10/25], Loss: 0.1205\n",
            "Epoch [44], Step [20/25], Loss: 0.1120\n",
            "Epoch [44] Validation Loss: 0.1137\n",
            "Epoch [45], Step [10/25], Loss: 0.1373\n",
            "Epoch [45], Step [20/25], Loss: 0.0892\n",
            "Epoch [45] Validation Loss: 0.0967\n",
            "Epoch [46], Step [10/25], Loss: 0.1010\n",
            "Epoch [46], Step [20/25], Loss: 0.1073\n",
            "Epoch [46] Validation Loss: 0.1629\n",
            "Epoch [47], Step [10/25], Loss: 0.0857\n",
            "Epoch [47], Step [20/25], Loss: 0.0886\n",
            "Epoch [47] Validation Loss: 0.0597\n",
            "✅ Saved best model at epoch 47 with val_loss=0.0597\n",
            "Epoch [48], Step [10/25], Loss: 0.1111\n",
            "Epoch [48], Step [20/25], Loss: 0.1348\n",
            "Epoch [48] Validation Loss: 0.0937\n",
            "Epoch [49], Step [10/25], Loss: 0.1262\n",
            "Epoch [49], Step [20/25], Loss: 0.1064\n",
            "Epoch [49] Validation Loss: 0.1431\n",
            "Epoch [50], Step [10/25], Loss: 0.0974\n",
            "Epoch [50], Step [20/25], Loss: 0.1190\n",
            "Epoch [50] Validation Loss: 0.0475\n",
            "✅ Saved best model at epoch 50 with val_loss=0.0475\n",
            "Epoch [51], Step [10/25], Loss: 0.1089\n",
            "Epoch [51], Step [20/25], Loss: 0.1031\n",
            "Epoch [51] Validation Loss: 0.2190\n",
            "Epoch [52], Step [10/25], Loss: 0.1264\n",
            "Epoch [52], Step [20/25], Loss: 0.0922\n",
            "Epoch [52] Validation Loss: 0.1354\n",
            "Epoch [53], Step [10/25], Loss: 0.0989\n",
            "Epoch [53], Step [20/25], Loss: 0.0944\n",
            "Epoch [53] Validation Loss: 0.2257\n",
            "Epoch [54], Step [10/25], Loss: 0.1241\n",
            "Epoch [54], Step [20/25], Loss: 0.1002\n",
            "Epoch [54] Validation Loss: 0.1009\n",
            "Epoch [55], Step [10/25], Loss: 0.0905\n",
            "Epoch [55], Step [20/25], Loss: 0.1120\n",
            "Epoch [55] Validation Loss: 0.0729\n",
            "Epoch [56], Step [10/25], Loss: 0.0879\n",
            "Epoch [56], Step [20/25], Loss: 0.1077\n",
            "Epoch [56] Validation Loss: 0.1731\n",
            "Epoch [57], Step [10/25], Loss: 0.1099\n",
            "Epoch [57], Step [20/25], Loss: 0.0925\n",
            "Epoch [57] Validation Loss: 0.1422\n",
            "Epoch [58], Step [10/25], Loss: 0.0984\n",
            "Epoch [58], Step [20/25], Loss: 0.0778\n",
            "Epoch [58] Validation Loss: 0.2370\n",
            "Epoch [59], Step [10/25], Loss: 0.1132\n",
            "Epoch [59], Step [20/25], Loss: 0.1237\n",
            "Epoch [59] Validation Loss: 0.1555\n",
            "Epoch [60], Step [10/25], Loss: 0.1055\n",
            "Epoch [60], Step [20/25], Loss: 0.0945\n",
            "Epoch [60] Validation Loss: 0.0725\n",
            "Epoch [61], Step [10/25], Loss: 0.1000\n",
            "Epoch [61], Step [20/25], Loss: 0.0893\n",
            "Epoch [61] Validation Loss: 0.0830\n",
            "Epoch [62], Step [10/25], Loss: 0.0778\n",
            "Epoch [62], Step [20/25], Loss: 0.0779\n",
            "Epoch [62] Validation Loss: 0.0785\n",
            "Epoch [63], Step [10/25], Loss: 0.0918\n",
            "Epoch [63], Step [20/25], Loss: 0.0991\n",
            "Epoch [63] Validation Loss: 0.1483\n",
            "Epoch [64], Step [10/25], Loss: 0.0805\n",
            "Epoch [64], Step [20/25], Loss: 0.0952\n",
            "Epoch [64] Validation Loss: 0.1853\n",
            "Epoch [65], Step [10/25], Loss: 0.0734\n",
            "Epoch [65], Step [20/25], Loss: 0.0944\n",
            "Epoch [65] Validation Loss: 0.0833\n",
            "Epoch [66], Step [10/25], Loss: 0.0775\n",
            "Epoch [66], Step [20/25], Loss: 0.0847\n",
            "Epoch [66] Validation Loss: 0.1408\n",
            "Epoch [67], Step [10/25], Loss: 0.0907\n",
            "Epoch [67], Step [20/25], Loss: 0.0886\n",
            "Epoch [67] Validation Loss: 0.1215\n",
            "Epoch [68], Step [10/25], Loss: 0.0848\n",
            "Epoch [68], Step [20/25], Loss: 0.0760\n",
            "Epoch [68] Validation Loss: 0.0906\n",
            "Epoch [69], Step [10/25], Loss: 0.0659\n",
            "Epoch [69], Step [20/25], Loss: 0.0683\n",
            "Epoch [69] Validation Loss: 0.0792\n",
            "Epoch [70], Step [10/25], Loss: 0.0642\n",
            "Epoch [70], Step [20/25], Loss: 0.0737\n",
            "Epoch [70] Validation Loss: 0.0835\n",
            "Epoch [71], Step [10/25], Loss: 0.0901\n",
            "Epoch [71], Step [20/25], Loss: 0.0855\n",
            "Epoch [71] Validation Loss: 0.0562\n",
            "Epoch [72], Step [10/25], Loss: 0.0751\n",
            "Epoch [72], Step [20/25], Loss: 0.0696\n",
            "Epoch [72] Validation Loss: 0.0634\n",
            "Epoch [73], Step [10/25], Loss: 0.0583\n",
            "Epoch [73], Step [20/25], Loss: 0.0718\n",
            "Epoch [73] Validation Loss: 0.0487\n",
            "Epoch [74], Step [10/25], Loss: 0.0729\n",
            "Epoch [74], Step [20/25], Loss: 0.0628\n",
            "Epoch [74] Validation Loss: 0.0767\n",
            "Epoch [75], Step [10/25], Loss: 0.0586\n",
            "Epoch [75], Step [20/25], Loss: 0.0817\n",
            "Epoch [75] Validation Loss: 0.0773\n",
            "Epoch [76], Step [10/25], Loss: 0.0672\n",
            "Epoch [76], Step [20/25], Loss: 0.0792\n",
            "Epoch [76] Validation Loss: 0.0696\n",
            "Epoch [77], Step [10/25], Loss: 0.0564\n",
            "Epoch [77], Step [20/25], Loss: 0.0832\n",
            "Epoch [77] Validation Loss: 0.1904\n",
            "Epoch [78], Step [10/25], Loss: 0.0861\n",
            "Epoch [78], Step [20/25], Loss: 0.0754\n",
            "Epoch [78] Validation Loss: 0.0456\n",
            "✅ Saved best model at epoch 78 with val_loss=0.0456\n",
            "Epoch [79], Step [10/25], Loss: 0.0748\n",
            "Epoch [79], Step [20/25], Loss: 0.0666\n",
            "Epoch [79] Validation Loss: 0.0585\n",
            "Epoch [80], Step [10/25], Loss: 0.0613\n",
            "Epoch [80], Step [20/25], Loss: 0.0752\n",
            "Epoch [80] Validation Loss: 0.1086\n",
            "Epoch [81], Step [10/25], Loss: 0.0608\n",
            "Epoch [81], Step [20/25], Loss: 0.0674\n",
            "Epoch [81] Validation Loss: 0.0616\n",
            "Epoch [82], Step [10/25], Loss: 0.0626\n",
            "Epoch [82], Step [20/25], Loss: 0.0543\n",
            "Epoch [82] Validation Loss: 0.0422\n",
            "✅ Saved best model at epoch 82 with val_loss=0.0422\n",
            "Epoch [83], Step [10/25], Loss: 0.0518\n",
            "Epoch [83], Step [20/25], Loss: 0.0713\n",
            "Epoch [83] Validation Loss: 0.1256\n",
            "Epoch [84], Step [10/25], Loss: 0.0753\n",
            "Epoch [84], Step [20/25], Loss: 0.0859\n",
            "Epoch [84] Validation Loss: 0.0574\n",
            "Epoch [85], Step [10/25], Loss: 0.0725\n",
            "Epoch [85], Step [20/25], Loss: 0.0823\n",
            "Epoch [85] Validation Loss: 0.0493\n",
            "Epoch [86], Step [10/25], Loss: 0.0632\n",
            "Epoch [86], Step [20/25], Loss: 0.0627\n",
            "Epoch [86] Validation Loss: 0.0526\n",
            "Epoch [87], Step [10/25], Loss: 0.0481\n",
            "Epoch [87], Step [20/25], Loss: 0.0573\n",
            "Epoch [87] Validation Loss: 0.0415\n",
            "✅ Saved best model at epoch 87 with val_loss=0.0415\n",
            "Epoch [88], Step [10/25], Loss: 0.0563\n",
            "Epoch [88], Step [20/25], Loss: 0.0593\n",
            "Epoch [88] Validation Loss: 0.0355\n",
            "✅ Saved best model at epoch 88 with val_loss=0.0355\n",
            "Epoch [89], Step [10/25], Loss: 0.0741\n",
            "Epoch [89], Step [20/25], Loss: 0.0574\n",
            "Epoch [89] Validation Loss: 0.0528\n",
            "Epoch [90], Step [10/25], Loss: 0.0480\n",
            "Epoch [90], Step [20/25], Loss: 0.0607\n",
            "Epoch [90] Validation Loss: 0.0597\n",
            "Epoch [91], Step [10/25], Loss: 0.0542\n",
            "Epoch [91], Step [20/25], Loss: 0.0627\n",
            "Epoch [91] Validation Loss: 0.0818\n",
            "Epoch [92], Step [10/25], Loss: 0.0771\n",
            "Epoch [92], Step [20/25], Loss: 0.0583\n",
            "Epoch [92] Validation Loss: 0.0699\n",
            "Epoch [93], Step [10/25], Loss: 0.0880\n",
            "Epoch [93], Step [20/25], Loss: 0.0761\n",
            "Epoch [93] Validation Loss: 0.0862\n",
            "Epoch [94], Step [10/25], Loss: 0.0890\n",
            "Epoch [94], Step [20/25], Loss: 0.0674\n",
            "Epoch [94] Validation Loss: 0.0893\n",
            "Epoch [95], Step [10/25], Loss: 0.0674\n",
            "Epoch [95], Step [20/25], Loss: 0.0767\n",
            "Epoch [95] Validation Loss: 0.0881\n",
            "Epoch [96], Step [10/25], Loss: 0.0560\n",
            "Epoch [96], Step [20/25], Loss: 0.0561\n",
            "Epoch [96] Validation Loss: 0.0495\n",
            "Epoch [97], Step [10/25], Loss: 0.0561\n",
            "Epoch [97], Step [20/25], Loss: 0.0573\n",
            "Epoch [97] Validation Loss: 0.1024\n",
            "Epoch [98], Step [10/25], Loss: 0.0890\n",
            "Epoch [98], Step [20/25], Loss: 0.0974\n",
            "Epoch [98] Validation Loss: 0.0802\n",
            "Epoch [99], Step [10/25], Loss: 0.0742\n",
            "Epoch [99], Step [20/25], Loss: 0.0704\n",
            "Epoch [99] Validation Loss: 0.0655\n",
            "Epoch [100], Step [10/25], Loss: 0.0595\n",
            "Epoch [100], Step [20/25], Loss: 0.0628\n",
            "Epoch [100] Validation Loss: 0.0990\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_test():\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels, target_lengths in test_loader:\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "            target_lengths = target_lengths.to(DEVICE)\n",
        "\n",
        "            logits = model(images)\n",
        "            logits = logits.permute(2, 0, 1)\n",
        "            log_probs = logits.log_softmax(2)\n",
        "\n",
        "            input_lengths = torch.full(size=(images.size(0),),\n",
        "                                       fill_value=logits.size(0),\n",
        "                                       dtype=torch.long).to(DEVICE)\n",
        "\n",
        "            loss = criterion(log_probs, labels, input_lengths, target_lengths)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    return val_loss\n",
        "\n",
        "print(f\"Test Loss: {validate_test():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QcZyxMIaVdo",
        "outputId": "b4ebd740-d83e-407e-9613-3ef5608efe08"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode(logits, idx_to_char, blank_idx):\n",
        "    \"\"\"\n",
        "    logits: [T, N, C] tensor (log probs or raw logits)\n",
        "    idx_to_char: dictionary mapping int -> char\n",
        "    blank_idx: index of blank symbol\n",
        "    \"\"\"\n",
        "    preds = logits.argmax(2).permute(1, 0)   # [N, T]\n",
        "\n",
        "    results = []\n",
        "    for pred in preds:\n",
        "        string = \"\"\n",
        "        prev = None\n",
        "        for p in pred.cpu().numpy():\n",
        "            if p != prev and p != blank_idx:   # collapse + remove blank\n",
        "                string += idx_to_char[p]\n",
        "            prev = p\n",
        "        results.append(string)\n",
        "    return results\n",
        "\n",
        "def predict_image(model, image, dataset):\n",
        "    \"\"\"\n",
        "    model   : trained model\n",
        "    image   : tensor [3, 24, 94]\n",
        "    dataset : dataset object (for idx_to_char, blank index)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        image = image.unsqueeze(0).to(DEVICE)        # add batch dim [1, 3, 24, 94]\n",
        "        logits = model(image)                        # [N, C, T]\n",
        "        logits = logits.permute(2, 0, 1)             # [T, N, C]\n",
        "\n",
        "        preds = greedy_decode(logits, dataset.idx_to_char, dataset.char_to_idx['-'])\n",
        "        return preds[0]\n",
        "\n"
      ],
      "metadata": {
        "id": "ikQAubV7btCR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a sample from your test set\n",
        "i = 0\n",
        "img, _, _ = test_ds[i]\n",
        "print(\"Ground truth:\", test_ds.image_files[i])\n",
        "\n",
        "pred = predict_image(model, img, test_ds)\n",
        "print(\"Prediction  :\", pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoNJkgcDbtX_",
        "outputId": "2bfbbb6c-dac8-4bdb-90ea-fd07ede9e7ba"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground truth: AY021B.jpg\n",
            "Prediction  : AY021B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np # Import numpy\n",
        "\n",
        "def evaluate_and_save(model, dataset, output_dir=\"results\"):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = len(dataset)\n",
        "\n",
        "    # Prepare output folders\n",
        "    right_dir = os.path.join(output_dir, \"right\")\n",
        "    wrong_dir = os.path.join(output_dir, \"wrong\")\n",
        "    os.makedirs(right_dir, exist_ok=True)\n",
        "    os.makedirs(wrong_dir, exist_ok=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(total):\n",
        "            img, label_encoded, _ = dataset[i]\n",
        "\n",
        "            # Ground truth string\n",
        "            label_str = \"\".join(dataset.idx_to_char[idx.item()] for idx in label_encoded)\n",
        "\n",
        "            # Prediction\n",
        "            pred_str = predict_image(model, img, dataset)\n",
        "\n",
        "            # Convert tensor -> numpy for saving\n",
        "            if isinstance(img, torch.Tensor):\n",
        "                np_img = img.squeeze().cpu().numpy() * 255.0\n",
        "                np_img = np.transpose(np_img, (1, 2, 0)) # Transpose dimensions to (height, width, channels)\n",
        "                np_img = np_img.astype(\"uint8\")\n",
        "            else:\n",
        "                np_img = img\n",
        "\n",
        "            # Build filename: prediction_GT_index.png\n",
        "            filename = f\"{pred_str}_GT-{label_str}_{i}.png\"\n",
        "\n",
        "            if pred_str == label_str:\n",
        "                correct += 1\n",
        "                save_path = os.path.join(right_dir, filename)\n",
        "            else:\n",
        "                save_path = os.path.join(wrong_dir, filename)\n",
        "\n",
        "            # Save with OpenCV\n",
        "            cv2.imwrite(save_path, np_img)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "# Run\n",
        "\n",
        "test_acc = evaluate_and_save(model, test_ds, output_dir=\"predictions\")\n",
        "print(f\"Test Accuracy: {test_acc:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pu2Xi5xVdhT5",
        "outputId": "79f20279-43e1-45e9-8185-b1a1131c4183"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 89.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72ca9cf1",
        "outputId": "a1d114c0-5cd6-4d5c-f131-db1a9bd1f0d7"
      },
      "source": [
        "!zip -r /content/predictions.zip /content/predictions"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/predictions/ (stored 0%)\n",
            "  adding: content/predictions/wrong/ (stored 0%)\n",
            "  adding: content/predictions/wrong/AB8888_GT-AB88B8_59.png (stored 0%)\n",
            "  adding: content/predictions/wrong/AE0178_GT-AED178_27.png (stored 0%)\n",
            "  adding: content/predictions/wrong/A11111_GT-AJ0J22_46.png (deflated 1%)\n",
            "  adding: content/predictions/wrong/AKR0356_GT-KR0356_94.png (stored 0%)\n",
            "  adding: content/predictions/wrong/AY0083_GT-AY0D83_49.png (stored 0%)\n",
            "  adding: content/predictions/wrong/AP0992_GT-AP099X_82.png (stored 0%)\n",
            "  adding: content/predictions/wrong/APB2153_GT-PB2153_44.png (stored 0%)\n",
            "  adding: content/predictions/wrong/AW1V2U_GT-AW112U_48.png (stored 0%)\n",
            "  adding: content/predictions/wrong/KL2283_GT-KZ2283_86.png (stored 0%)\n",
            "  adding: content/predictions/wrong/AHL28K_GT-AH128K_57.png (stored 0%)\n",
            "  adding: content/predictions/wrong/A30CJ56_GT-A30J56_47.png (stored 0%)\n",
            "  adding: content/predictions/right/ (stored 0%)\n",
            "  adding: content/predictions/right/AER805_GT-AER805_81.png (stored 0%)\n",
            "  adding: content/predictions/right/AT5Y25_GT-AT5Y25_66.png (stored 0%)\n",
            "  adding: content/predictions/right/AW283F_GT-AW283F_61.png (stored 0%)\n",
            "  adding: content/predictions/right/A75556_GT-A75556_4.png (stored 0%)\n",
            "  adding: content/predictions/right/AN5735_GT-AN5735_20.png (stored 0%)\n",
            "  adding: content/predictions/right/A9J079_GT-A9J079_54.png (stored 0%)\n",
            "  adding: content/predictions/right/A53N00_GT-A53N00_9.png (stored 0%)\n",
            "  adding: content/predictions/right/AKK123_GT-AKK123_88.png (stored 0%)\n",
            "  adding: content/predictions/right/AVY501_GT-AVY501_19.png (stored 0%)\n",
            "  adding: content/predictions/right/ATU196_GT-ATU196_8.png (stored 0%)\n",
            "  adding: content/predictions/right/AL515L_GT-AL515L_53.png (stored 0%)\n",
            "  adding: content/predictions/right/A13W08_GT-A13W08_64.png (stored 0%)\n",
            "  adding: content/predictions/right/AHB399_GT-AHB399_29.png (stored 0%)\n",
            "  adding: content/predictions/right/AH858R_GT-AH858R_41.png (stored 0%)\n",
            "  adding: content/predictions/right/ASJ346_GT-ASJ346_97.png (stored 0%)\n",
            "  adding: content/predictions/right/AD028K_GT-AD028K_23.png (stored 0%)\n",
            "  adding: content/predictions/right/AZB220_GT-AZB220_96.png (stored 0%)\n",
            "  adding: content/predictions/right/A76737_GT-A76737_89.png (stored 0%)\n",
            "  adding: content/predictions/right/A8M636_GT-A8M636_38.png (stored 0%)\n",
            "  adding: content/predictions/right/ALJ620_GT-ALJ620_50.png (stored 0%)\n",
            "  adding: content/predictions/right/AUS029_GT-AUS029_3.png (stored 0%)\n",
            "  adding: content/predictions/right/A6T999_GT-A6T999_83.png (stored 0%)\n",
            "  adding: content/predictions/right/AT5Y57_GT-AT5Y57_43.png (stored 0%)\n",
            "  adding: content/predictions/right/A508X8_GT-A508X8_34.png (stored 0%)\n",
            "  adding: content/predictions/right/A30B03_GT-A30B03_18.png (stored 0%)\n",
            "  adding: content/predictions/right/AL9L09_GT-AL9L09_16.png (stored 0%)\n",
            "  adding: content/predictions/right/AQQ677_GT-AQQ677_55.png (stored 0%)\n",
            "  adding: content/predictions/right/ARW936_GT-ARW936_30.png (stored 0%)\n",
            "  adding: content/predictions/right/AX3Z77_GT-AX3Z77_32.png (stored 0%)\n",
            "  adding: content/predictions/right/D33U29_GT-D33U29_52.png (stored 0%)\n",
            "  adding: content/predictions/right/AW9V20_GT-AW9V20_62.png (stored 0%)\n",
            "  adding: content/predictions/right/AD0D89_GT-AD0D89_91.png (stored 0%)\n",
            "  adding: content/predictions/right/AHG124_GT-AHG124_56.png (stored 0%)\n",
            "  adding: content/predictions/right/AFM554_GT-AFM554_74.png (stored 0%)\n",
            "  adding: content/predictions/right/ARK558_GT-ARK558_98.png (stored 0%)\n",
            "  adding: content/predictions/right/AP201A_GT-AP201A_13.png (stored 0%)\n",
            "  adding: content/predictions/right/AXZ961_GT-AXZ961_33.png (stored 0%)\n",
            "  adding: content/predictions/right/AR5752_GT-AR5752_35.png (stored 0%)\n",
            "  adding: content/predictions/right/AV9G58_GT-AV9G58_31.png (stored 0%)\n",
            "  adding: content/predictions/right/AF513U_GT-AF513U_12.png (stored 0%)\n",
            "  adding: content/predictions/right/A051N0_GT-A051N0_67.png (stored 0%)\n",
            "  adding: content/predictions/right/A22U82_GT-A22U82_14.png (stored 0%)\n",
            "  adding: content/predictions/right/AY021B_GT-AY021B_0.png (stored 0%)\n",
            "  adding: content/predictions/right/AG4449_GT-AG4449_28.png (stored 0%)\n",
            "  adding: content/predictions/right/AH0M53_GT-AH0M53_87.png (stored 0%)\n",
            "  adding: content/predictions/right/AD9L71_GT-AD9L71_70.png (stored 0%)\n",
            "  adding: content/predictions/right/AGH510_GT-AGH510_85.png (stored 0%)\n",
            "  adding: content/predictions/right/AKS189_GT-AKS189_51.png (stored 0%)\n",
            "  adding: content/predictions/right/AT152M_GT-AT152M_71.png (stored 0%)\n",
            "  adding: content/predictions/right/AE9639_GT-AE9639_42.png (stored 0%)\n",
            "  adding: content/predictions/right/AL1R08_GT-AL1R08_65.png (stored 0%)\n",
            "  adding: content/predictions/right/AB766W_GT-AB766W_39.png (stored 0%)\n",
            "  adding: content/predictions/right/A390X1_GT-A390X1_37.png (stored 0%)\n",
            "  adding: content/predictions/right/A8R966_GT-A8R966_40.png (stored 0%)\n",
            "  adding: content/predictions/right/AL606G_GT-AL606G_10.png (stored 0%)\n",
            "  adding: content/predictions/right/AX702L_GT-AX702L_92.png (stored 0%)\n",
            "  adding: content/predictions/right/AV683V_GT-AV683V_75.png (stored 0%)\n",
            "  adding: content/predictions/right/AS0825_GT-AS0825_93.png (stored 0%)\n",
            "  adding: content/predictions/right/AGG512_GT-AGG512_7.png (stored 0%)\n",
            "  adding: content/predictions/right/AY605B_GT-AY605B_11.png (stored 0%)\n",
            "  adding: content/predictions/right/A0V788_GT-A0V788_26.png (stored 0%)\n",
            "  adding: content/predictions/right/A71920_GT-A71920_76.png (stored 0%)\n",
            "  adding: content/predictions/right/A57NT9_GT-A57NT9_25.png (stored 0%)\n",
            "  adding: content/predictions/right/AP9802_GT-AP9802_15.png (stored 0%)\n",
            "  adding: content/predictions/right/AY631Y_GT-AY631Y_6.png (stored 0%)\n",
            "  adding: content/predictions/right/M5G687_GT-M5G687_80.png (stored 0%)\n",
            "  adding: content/predictions/right/AP700J_GT-AP700J_84.png (stored 0%)\n",
            "  adding: content/predictions/right/AX788P_GT-AX788P_95.png (stored 0%)\n",
            "  adding: content/predictions/right/A256R2_GT-A256R2_78.png (stored 0%)\n",
            "  adding: content/predictions/right/HRE869_GT-HRE869_68.png (stored 0%)\n",
            "  adding: content/predictions/right/AY6P56_GT-AY6P56_72.png (stored 0%)\n",
            "  adding: content/predictions/right/AQF776_GT-AQF776_17.png (stored 0%)\n",
            "  adding: content/predictions/right/A71H99_GT-A71H99_69.png (stored 0%)\n",
            "  adding: content/predictions/right/AG017H_GT-AG017H_22.png (stored 0%)\n",
            "  adding: content/predictions/right/APA883_GT-APA883_45.png (stored 0%)\n",
            "  adding: content/predictions/right/A72G21_GT-A72G21_90.png (stored 0%)\n",
            "  adding: content/predictions/right/A63244_GT-A63244_99.png (stored 0%)\n",
            "  adding: content/predictions/right/A9S911_GT-A9S911_1.png (stored 0%)\n",
            "  adding: content/predictions/right/ASL612_GT-ASL612_2.png (stored 0%)\n",
            "  adding: content/predictions/right/AGK272_GT-AGK272_58.png (stored 0%)\n",
            "  adding: content/predictions/right/AC8H11_GT-AC8H11_21.png (stored 0%)\n",
            "  adding: content/predictions/right/A99788_GT-A99788_63.png (stored 0%)\n",
            "  adding: content/predictions/right/AF8381_GT-AF8381_36.png (stored 0%)\n",
            "  adding: content/predictions/right/AB921S_GT-AB921S_5.png (stored 0%)\n",
            "  adding: content/predictions/right/AXQ959_GT-AXQ959_79.png (stored 0%)\n",
            "  adding: content/predictions/right/AJV866_GT-AJV866_24.png (stored 0%)\n",
            "  adding: content/predictions/right/AL392W_GT-AL392W_73.png (stored 0%)\n",
            "  adding: content/predictions/right/A3X096_GT-A3X096_77.png (stored 0%)\n",
            "  adding: content/predictions/right/A82Z60_GT-A82Z60_60.png (stored 0%)\n"
          ]
        }
      ]
    }
  ]
}