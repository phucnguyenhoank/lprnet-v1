{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ce269e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train dataset:\n",
      "Images batch: (32, 24, 94, 3)\n",
      "Sample 1: [12 29  9 10  8 26 24], length: 7\n",
      "Sample 2: [14 27  1  5  1 32 26], length: 7\n",
      "Sample 3: [27 20 10 10  1  6], length: 6\n",
      "Sample 4: [20 31 34  8  1  4], length: 6\n",
      "Sample 5: [15 22  2  6  7 28 28], length: 7\n",
      "\n",
      "Validation dataset:\n",
      "Images batch: (32, 24, 94, 3)\n",
      "Sample 1: [ 2 27 11 16  1  6  1], length: 7\n",
      "Sample 2: [ 4  6  4 11 29 21  4  6], length: 8\n",
      "Sample 3: [ 5  2 20 11  4  5], length: 6\n",
      "Sample 4: [ 6  2  3  7 18 32 22], length: 7\n",
      "Sample 5: [ 7  5  1  3 32 18  3  6], length: 8\n",
      "\n",
      "Test dataset:\n",
      "Images batch: (32, 24, 94, 3)\n",
      "Sample 1: [ 2  1  4  4 19 28], length: 6\n",
      "Sample 2: [ 3  4  5  9 34 28  3  6], length: 8\n",
      "Sample 3: [ 4  5 15 24 10  2 10 10], length: 8\n",
      "Sample 4: [ 4  9  2 11 30 21  9  4], length: 8\n",
      "Sample 5: [ 6  6 29 17  6  4], length: 6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Constants\n",
    "CHARSET = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "NUM_CLASSES = len(CHARSET) + 1  # 36 chars + blank=0\n",
    "IMG_WIDTH = 94\n",
    "IMG_HEIGHT = 24\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Global lookup table (shift by +1, so blank=0)\n",
    "keys = tf.constant(list(CHARSET))\n",
    "values = tf.constant(list(range(1, len(CHARSET)+1)), dtype=tf.int32)  # 1..36\n",
    "char_to_num_table = tf.lookup.StaticHashTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(keys, values),\n",
    "    default_value=0\n",
    ")\n",
    "\n",
    "# Reverse table for decoding\n",
    "num_to_char_table = tf.lookup.StaticHashTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(values, keys),\n",
    "    default_value=''\n",
    ")\n",
    "\n",
    "# Extract label from filename\n",
    "def process_filename(file_path):\n",
    "    filename = tf.strings.split(file_path, os.sep)[-1]\n",
    "    filename = tf.strings.regex_replace(filename, r'\\.[^.]+$', '')   # drop extension\n",
    "    filename = tf.strings.regex_replace(filename, r'_.*$', '')       # drop suffix after \"_\"\n",
    "    return filename\n",
    "\n",
    "# Encode label as int sequence (no padding)\n",
    "def encode_label(label):\n",
    "    chars = tf.strings.unicode_split(label, 'UTF-8')\n",
    "    indices = tf.cast(char_to_num_table.lookup(chars), tf.int32)\n",
    "    return indices\n",
    "\n",
    "# Load and preprocess image\n",
    "def load_and_preprocess_image(file_path):\n",
    "    image = tf.io.read_file(file_path)\n",
    "    image = tf.image.decode_image(image, channels=3, expand_animations=False)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])\n",
    "    \n",
    "    label = process_filename(file_path)\n",
    "    encoded_label = encode_label(label)   # Ragged (variable length)\n",
    "    return image, encoded_label\n",
    "\n",
    "# Create dataset with ragged batching\n",
    "def create_dataset(folder_path, shuffle=True):\n",
    "    files = tf.data.Dataset.list_files(str(folder_path / '*.*'), shuffle=shuffle)\n",
    "    dataset = files.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.apply(tf.data.experimental.dense_to_ragged_batch(BATCH_SIZE))\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Load datasets\n",
    "train_dir = pathlib.Path(\"lprds/train\")\n",
    "val_dir = pathlib.Path(\"lprds/val\")\n",
    "test_dir = pathlib.Path(\"lprds/test\")\n",
    "\n",
    "train_ds = create_dataset(train_dir, shuffle=True)\n",
    "val_ds = create_dataset(val_dir, shuffle=False)\n",
    "test_ds = create_dataset(test_dir, shuffle=False)\n",
    "\n",
    "# Inspect dataset\n",
    "def inspect_dataset(dataset, name, num_samples=5):\n",
    "    print(f\"\\n{name} dataset:\")\n",
    "    for images, labels in dataset.take(1):\n",
    "        print(f\"Images batch: {images.shape}\")\n",
    "        for i in range(min(num_samples, BATCH_SIZE)):\n",
    "            print(f\"Sample {i+1}: {labels[i].numpy()}, length: {len(labels[i])}\")\n",
    "\n",
    "inspect_dataset(train_ds, \"Train\")\n",
    "inspect_dataset(val_ds, \"Validation\")\n",
    "inspect_dataset(test_ds, \"Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d55ff58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"LPRNet\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"LPRNet\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_60 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">92</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │ input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">92</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv2d_60[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ re_lu_56 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">92</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_12    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ re_lu_56[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sbb1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">13,408</span> │ max_pooling2d_12… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_13    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">88</span>,     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sbb1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sbb2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">88</span>,     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">51,392</span> │ max_pooling2d_13… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sbb3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">88</span>,     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">59,584</span> │ sbb2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_14    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">86</span>,     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sbb3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">86</span>,     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_14… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_73 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">83</span>,     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">262,400</span> │ dropout_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">83</span>,     │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ conv2d_73[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ re_lu_69 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">83</span>,     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">83</span>,     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ re_lu_69[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_74 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">123,173</span> │ dropout_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gc_flatten          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10508</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_74[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gc_fc (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,345,152</span> │ gc_flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gc_repeat           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">284</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ gc_fc[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gc_reshape          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>,     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ gc_repeat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gc_concat           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>,     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_74[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">165</span>)              │            │ gc_reshape[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gc_conv1x1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,142</span> │ gc_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m3\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_60 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m92\u001b[0m,    │      \u001b[38;5;34m1,792\u001b[0m │ input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m92\u001b[0m,    │        \u001b[38;5;34m256\u001b[0m │ conv2d_60[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ re_lu_56 (\u001b[38;5;33mReLU\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m92\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_12    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m90\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ re_lu_56[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sbb1 (\u001b[38;5;33mSequential\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m90\u001b[0m,    │     \u001b[38;5;34m13,408\u001b[0m │ max_pooling2d_12… │\n",
       "│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_13    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m88\u001b[0m,     │          \u001b[38;5;34m0\u001b[0m │ sbb1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sbb2 (\u001b[38;5;33mSequential\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m88\u001b[0m,     │     \u001b[38;5;34m51,392\u001b[0m │ max_pooling2d_13… │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sbb3 (\u001b[38;5;33mSequential\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m88\u001b[0m,     │     \u001b[38;5;34m59,584\u001b[0m │ sbb2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_14    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m86\u001b[0m,     │          \u001b[38;5;34m0\u001b[0m │ sbb3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m86\u001b[0m,     │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_14… │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_73 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m83\u001b[0m,     │    \u001b[38;5;34m262,400\u001b[0m │ dropout_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m83\u001b[0m,     │      \u001b[38;5;34m1,024\u001b[0m │ conv2d_73[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ re_lu_69 (\u001b[38;5;33mReLU\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m83\u001b[0m,     │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m83\u001b[0m,     │          \u001b[38;5;34m0\u001b[0m │ re_lu_69[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_74 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m71\u001b[0m, \u001b[38;5;34m37\u001b[0m) │    \u001b[38;5;34m123,173\u001b[0m │ dropout_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gc_flatten          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10508\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ conv2d_74[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mFlatten\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gc_fc (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │  \u001b[38;5;34m1,345,152\u001b[0m │ gc_flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gc_repeat           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m284\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ gc_fc[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mRepeatVector\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gc_reshape          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m71\u001b[0m,     │          \u001b[38;5;34m0\u001b[0m │ gc_repeat[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mReshape\u001b[0m)           │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gc_concat           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m71\u001b[0m,     │          \u001b[38;5;34m0\u001b[0m │ conv2d_74[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │ \u001b[38;5;34m165\u001b[0m)              │            │ gc_reshape[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gc_conv1x1 (\u001b[38;5;33mConv2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m71\u001b[0m, \u001b[38;5;34m37\u001b[0m) │      \u001b[38;5;34m6,142\u001b[0m │ gc_concat[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,864,323</span> (7.11 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,864,323\u001b[0m (7.11 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,861,443</span> (7.10 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,861,443\u001b[0m (7.10 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,880</span> (11.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,880\u001b[0m (11.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (1, 4, 71, 37)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def small_basic_block(Cout, name=None):\n",
    "    mid = Cout // 4\n",
    "    seq = models.Sequential(name=name)\n",
    "    seq.add(layers.Conv2D(mid, kernel_size=1, strides=1, padding=\"same\"))\n",
    "    seq.add(layers.BatchNormalization())\n",
    "    seq.add(layers.ReLU())\n",
    "    \n",
    "    seq.add(layers.Conv2D(mid, kernel_size=(3,1), strides=1, padding=\"same\")) # height=3\n",
    "    seq.add(layers.BatchNormalization())\n",
    "    seq.add(layers.ReLU())\n",
    "\n",
    "    seq.add(layers.Conv2D(mid, kernel_size=(1,3), strides=1, padding=\"same\")) # width=3\n",
    "    seq.add(layers.BatchNormalization())\n",
    "    seq.add(layers.ReLU())\n",
    "\n",
    "    seq.add(layers.Conv2D(Cout, kernel_size=1, strides=1, padding=\"same\"))\n",
    "    seq.add(layers.BatchNormalization())\n",
    "    seq.add(layers.ReLU())\n",
    "\n",
    "    return seq\n",
    "\n",
    "def global_context_block(x, num_classes, gc_dim=128, name=None):\n",
    "    \"\"\"Global context embedding as in LPRNet paper.\"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "\n",
    "    # Step 1: Global context vector from backbone\n",
    "    context = layers.Flatten(name=f\"{name}_flatten\")(x)             # (B, H*W*C)\n",
    "    context = layers.Dense(gc_dim, activation='relu', name=f\"{name}_fc\")(context)  # (B, gc_dim)\n",
    "\n",
    "    # Step 2: Tile back to spatial map\n",
    "    context = layers.RepeatVector(H * W, name=f\"{name}_repeat\")(context)  # (B, H*W, gc_dim)\n",
    "    context = layers.Reshape((H, W, gc_dim), name=f\"{name}_reshape\")(context)  # (B, H, W, gc_dim)\n",
    "\n",
    "    # Step 3: Concatenate with backbone features\n",
    "    x = layers.Concatenate(axis=-1, name=f\"{name}_concat\")([x, context])  # (B, H, W, C+gc_dim)\n",
    "\n",
    "    # Step 4: 1×1 Conv to adjust channels → num_classes\n",
    "    x = layers.Conv2D(num_classes, (1,1), strides=1, padding=\"same\", name=f\"{name}_conv1x1\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def LPRNet(num_classes=37, dropout_rate=0.5):\n",
    "    inputs = layers.Input(shape=(24,94,3), name=\"input\") # HxWxD\n",
    "\n",
    "    # Backbone\n",
    "    x = layers.Conv2D(64, (3,3), strides=1, padding=\"valid\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    x = layers.MaxPooling2D((3,3), strides=(1,1), padding=\"valid\")(x)\n",
    "    x = small_basic_block(128,\"sbb1\")(x)\n",
    "    \n",
    "    x = layers.MaxPooling2D((3,3), strides=(2,1), padding=\"valid\")(x) # 64? it should 128 instead?\n",
    "\n",
    "    x = small_basic_block(256,\"sbb2\")(x)\n",
    "    x = small_basic_block(256,\"sbb3\")(x)\n",
    "\n",
    "    x = layers.MaxPooling2D((3,3), strides=(2,1), padding=\"valid\")(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = layers.Conv2D(256,(1,4), strides=1, padding=\"valid\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = layers.Conv2D(num_classes,(1, 13), strides=1, padding=\"valid\")(x)\n",
    "    # Add global context\n",
    "    x = global_context_block(x, num_classes=num_classes, gc_dim=128, name=\"gc\")\n",
    "\n",
    "    return models.Model(inputs=inputs, outputs=x, name=\"LPRNet\")\n",
    "\n",
    "# ---- Test ----\n",
    "model = LPRNet(num_classes=37)\n",
    "model.summary()\n",
    "\n",
    "dummy = tf.random.normal((1,24,94,3))\n",
    "out = model(dummy)\n",
    "print(\"Output shape:\", out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5db6720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Assuming the LPRNet model is already defined as in your code.\n",
    "# Note: There's a typo in your model code - \"SmallBasicBlock\" should be \"small_basic_block\" (case-sensitive).\n",
    "# Correct it to small_basic_block(256, \"sbb2\")(x) and small_basic_block(256, \"sbb3\")(x)\n",
    "\n",
    "# Custom CTC loss function to handle variable label lengths\n",
    "def ctc_loss(y_true, y_pred):\n",
    "    \n",
    "    # Cast y_true to int32 if not already\n",
    "    labels = tf.cast(y_true, tf.int32).to_sparse()\n",
    "\n",
    "    # y_pred shape: (B, 4, 71, 37)\n",
    "    # Reduce mean over height to get (B, 71, 37) for CTC (time steps along width)\n",
    "    logits = tf.reduce_max(y_pred, axis=1)\n",
    "\n",
    "    # Compute dynamic batch size and time-steps from logits\n",
    "    batch = tf.shape(logits)[0]\n",
    "    time_steps = tf.shape(logits)[1]\n",
    "\n",
    "    # Create logit_length matching runtime batch size\n",
    "    logit_length = tf.fill([batch], time_steps)\n",
    "    logit_length = tf.cast(logit_length, tf.int32)\n",
    "\n",
    "    # Compute CTC loss using tf.nn.ctc_loss\n",
    "    loss_per_sample = tf.nn.ctc_loss(\n",
    "        labels=labels,\n",
    "        logits=logits,\n",
    "        label_length=None,\n",
    "        logit_length=logit_length,\n",
    "        logits_time_major=False,\n",
    "        blank_index=0  # Assuming blank is class 0, characters are 1-36\n",
    "    )\n",
    "    \n",
    "    # Return mean loss\n",
    "    return tf.reduce_mean(loss_per_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0463ee3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from checkpoint.\n",
      "Epoch 1/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689ms/step - loss: 4.0088\n",
      "Epoch 1: val_loss improved from None to 35.92142, saving model to lprnet_checkpoint.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 766ms/step - loss: 4.3943 - val_loss: 35.9214 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690ms/step - loss: 4.1089\n",
      "Epoch 2: val_loss improved from 35.92142 to 21.47130, saving model to lprnet_checkpoint.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 735ms/step - loss: 3.7790 - val_loss: 21.4713 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707ms/step - loss: 2.8633\n",
      "Epoch 3: val_loss improved from 21.47130 to 19.94461, saving model to lprnet_checkpoint.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 755ms/step - loss: 2.7074 - val_loss: 19.9446 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689ms/step - loss: 2.2841\n",
      "Epoch 4: val_loss did not improve from 19.94461\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 712ms/step - loss: 2.2758 - val_loss: 20.2536 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700ms/step - loss: 1.8656\n",
      "Epoch 5: val_loss improved from 19.94461 to 19.33331, saving model to lprnet_checkpoint.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 741ms/step - loss: 2.1021 - val_loss: 19.3333 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706ms/step - loss: 2.8154\n",
      "Epoch 6: val_loss did not improve from 19.33331\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 736ms/step - loss: 2.2364 - val_loss: 28.0023 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688ms/step - loss: 2.2648\n",
      "Epoch 7: val_loss improved from 19.33331 to 16.27261, saving model to lprnet_checkpoint.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 725ms/step - loss: 2.3594 - val_loss: 16.2726 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754ms/step - loss: 2.0472\n",
      "Epoch 8: val_loss did not improve from 16.27261\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 777ms/step - loss: 2.0383 - val_loss: 18.9568 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 638ms/step - loss: 2.1982\n",
      "Epoch 9: val_loss did not improve from 16.27261\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 660ms/step - loss: 2.3812 - val_loss: 19.4549 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644ms/step - loss: 1.8825\n",
      "Epoch 10: val_loss improved from 16.27261 to 14.48850, saving model to lprnet_checkpoint.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 685ms/step - loss: 2.0266 - val_loss: 14.4885 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660ms/step - loss: 1.8803\n",
      "Epoch 11: val_loss did not improve from 14.48850\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 682ms/step - loss: 1.9881 - val_loss: 15.3820 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708ms/step - loss: 1.4683\n",
      "Epoch 12: val_loss improved from 14.48850 to 13.18902, saving model to lprnet_checkpoint.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 776ms/step - loss: 1.4874 - val_loss: 13.1890 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685ms/step - loss: 1.2395\n",
      "Epoch 13: val_loss improved from 13.18902 to 11.53521, saving model to lprnet_checkpoint.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 723ms/step - loss: 1.1649 - val_loss: 11.5352 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698ms/step - loss: 0.8066\n",
      "Epoch 14: val_loss did not improve from 11.53521\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 721ms/step - loss: 0.9426 - val_loss: 14.0211 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 768ms/step - loss: 0.9456\n",
      "Epoch 15: val_loss improved from 11.53521 to 11.12948, saving model to lprnet_checkpoint.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 838ms/step - loss: 1.0543 - val_loss: 11.1295 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755ms/step - loss: 0.9414\n",
      "Epoch 16: val_loss did not improve from 11.12948\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 781ms/step - loss: 1.0735 - val_loss: 12.1865 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643ms/step - loss: 0.8331\n",
      "Epoch 17: val_loss did not improve from 11.12948\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 666ms/step - loss: 0.8408 - val_loss: 13.7413 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 638ms/step - loss: 1.1306\n",
      "Epoch 18: val_loss improved from 11.12948 to 11.11050, saving model to lprnet_checkpoint.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 680ms/step - loss: 1.0909 - val_loss: 11.1105 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661ms/step - loss: 1.2443\n",
      "Epoch 19: val_loss did not improve from 11.11050\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 684ms/step - loss: 1.2144 - val_loss: 12.4556 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675ms/step - loss: 1.0923\n",
      "Epoch 20: val_loss did not improve from 11.11050\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 701ms/step - loss: 1.0781 - val_loss: 12.2424 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707ms/step - loss: 1.0485\n",
      "Epoch 21: val_loss did not improve from 11.11050\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 735ms/step - loss: 1.0981 - val_loss: 13.2421 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694ms/step - loss: 1.0822\n",
      "Epoch 22: val_loss did not improve from 11.11050\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 717ms/step - loss: 1.1479 - val_loss: 14.0749 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706ms/step - loss: 0.9695\n",
      "Epoch 23: val_loss did not improve from 11.11050\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 731ms/step - loss: 1.2123 - val_loss: 13.8976 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716ms/step - loss: 0.8773\n",
      "Epoch 24: val_loss improved from 11.11050 to 8.58902, saving model to lprnet_checkpoint.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 758ms/step - loss: 0.8348 - val_loss: 8.5890 - learning_rate: 5.0000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685ms/step - loss: 0.6149\n",
      "Epoch 25: val_loss did not improve from 8.58902\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 708ms/step - loss: 0.6517 - val_loss: 9.0645 - learning_rate: 5.0000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647ms/step - loss: 0.4274\n",
      "Epoch 26: val_loss did not improve from 8.58902\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 670ms/step - loss: 0.3756 - val_loss: 9.4928 - learning_rate: 5.0000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654ms/step - loss: 0.2821\n",
      "Epoch 27: val_loss did not improve from 8.58902\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 676ms/step - loss: 0.2851 - val_loss: 8.7459 - learning_rate: 5.0000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728ms/step - loss: 0.3892\n",
      "Epoch 28: val_loss improved from 8.58902 to 8.00659, saving model to lprnet_checkpoint.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 766ms/step - loss: 0.2836 - val_loss: 8.0066 - learning_rate: 5.0000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 673ms/step - loss: 0.2597\n",
      "Epoch 29: val_loss improved from 8.00659 to 7.64673, saving model to lprnet_checkpoint.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 712ms/step - loss: 0.2412 - val_loss: 7.6467 - learning_rate: 5.0000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664ms/step - loss: 0.1899\n",
      "Epoch 30: val_loss did not improve from 7.64673\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 686ms/step - loss: 0.1807 - val_loss: 8.0072 - learning_rate: 5.0000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712ms/step - loss: 0.1783\n",
      "Epoch 31: val_loss did not improve from 7.64673\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 734ms/step - loss: 0.1947 - val_loss: 8.3132 - learning_rate: 5.0000e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646ms/step - loss: 0.1451\n",
      "Epoch 32: val_loss did not improve from 7.64673\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 668ms/step - loss: 0.1748 - val_loss: 8.2348 - learning_rate: 5.0000e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677ms/step - loss: 0.1528\n",
      "Epoch 33: val_loss did not improve from 7.64673\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 703ms/step - loss: 0.1497 - val_loss: 7.7422 - learning_rate: 5.0000e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718ms/step - loss: 0.1172\n",
      "Epoch 34: val_loss did not improve from 7.64673\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 755ms/step - loss: 0.1315 - val_loss: 7.9764 - learning_rate: 5.0000e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651ms/step - loss: 0.1568\n",
      "Epoch 35: val_loss did not improve from 7.64673\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 674ms/step - loss: 0.1684 - val_loss: 7.8492 - learning_rate: 2.5000e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718ms/step - loss: 0.1385\n",
      "Epoch 36: val_loss did not improve from 7.64673\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 743ms/step - loss: 0.1269 - val_loss: 7.7488 - learning_rate: 2.5000e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687ms/step - loss: 0.1057\n",
      "Epoch 37: val_loss did not improve from 7.64673\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 709ms/step - loss: 0.1255 - val_loss: 7.8346 - learning_rate: 2.5000e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 626ms/step - loss: 0.0855\n",
      "Epoch 38: val_loss did not improve from 7.64673\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 648ms/step - loss: 0.1031 - val_loss: 7.9810 - learning_rate: 2.5000e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647ms/step - loss: 0.0876\n",
      "Epoch 39: val_loss did not improve from 7.64673\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 673ms/step - loss: 0.0955 - val_loss: 7.8373 - learning_rate: 2.5000e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 627ms/step - loss: 0.1057\n",
      "Epoch 40: val_loss did not improve from 7.64673\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 649ms/step - loss: 0.1114 - val_loss: 8.1054 - learning_rate: 1.2500e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 626ms/step - loss: 0.1619\n",
      "Epoch 41: val_loss did not improve from 7.64673\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 648ms/step - loss: 0.1480 - val_loss: 7.8550 - learning_rate: 1.2500e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641ms/step - loss: 0.1595\n",
      "Epoch 42: val_loss did not improve from 7.64673\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 664ms/step - loss: 0.1584 - val_loss: 7.7722 - learning_rate: 1.2500e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 632ms/step - loss: 0.1128\n",
      "Epoch 43: val_loss did not improve from 7.64673\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 654ms/step - loss: 0.1119 - val_loss: 7.9951 - learning_rate: 1.2500e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 628ms/step - loss: 0.0803\n",
      "Epoch 44: val_loss did not improve from 7.64673\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 650ms/step - loss: 0.0973 - val_loss: 7.9742 - learning_rate: 1.2500e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669ms/step - loss: 0.1334\n",
      "Epoch 45: val_loss did not improve from 7.64673\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 690ms/step - loss: 0.1127 - val_loss: 7.8815 - learning_rate: 6.2500e-05\n",
      "Epoch 46/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 625ms/step - loss: 0.1021\n",
      "Epoch 46: val_loss did not improve from 7.64673\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 647ms/step - loss: 0.0983 - val_loss: 7.7551 - learning_rate: 6.2500e-05\n",
      "Epoch 47/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757ms/step - loss: 0.0906\n",
      "Epoch 47: val_loss improved from 7.64673 to 7.64598, saving model to lprnet_checkpoint.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 808ms/step - loss: 0.1023 - val_loss: 7.6460 - learning_rate: 6.2500e-05\n",
      "Epoch 48/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667ms/step - loss: 0.1067\n",
      "Epoch 48: val_loss improved from 7.64598 to 7.56232, saving model to lprnet_checkpoint.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 707ms/step - loss: 0.1094 - val_loss: 7.5623 - learning_rate: 6.2500e-05\n",
      "Epoch 49/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650ms/step - loss: 0.0930\n",
      "Epoch 49: val_loss improved from 7.56232 to 7.44585, saving model to lprnet_checkpoint.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 689ms/step - loss: 0.0952 - val_loss: 7.4458 - learning_rate: 6.2500e-05\n",
      "Epoch 50/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662ms/step - loss: 0.0658\n",
      "Epoch 50: val_loss improved from 7.44585 to 7.41826, saving model to lprnet_checkpoint.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 702ms/step - loss: 0.0981 - val_loss: 7.4183 - learning_rate: 6.2500e-05\n",
      "Epoch 51/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674ms/step - loss: 0.1039\n",
      "Epoch 51: val_loss improved from 7.41826 to 7.40349, saving model to lprnet_checkpoint.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 718ms/step - loss: 0.0821 - val_loss: 7.4035 - learning_rate: 6.2500e-05\n",
      "Epoch 52/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664ms/step - loss: 0.1124\n",
      "Epoch 52: val_loss improved from 7.40349 to 7.33811, saving model to lprnet_checkpoint.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 702ms/step - loss: 0.1095 - val_loss: 7.3381 - learning_rate: 6.2500e-05\n",
      "Epoch 53/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730ms/step - loss: 0.0979\n",
      "Epoch 53: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 773ms/step - loss: 0.0805 - val_loss: 7.3679 - learning_rate: 6.2500e-05\n",
      "Epoch 54/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707ms/step - loss: 0.0646\n",
      "Epoch 54: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 731ms/step - loss: 0.0740 - val_loss: 7.5751 - learning_rate: 6.2500e-05\n",
      "Epoch 55/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674ms/step - loss: 0.0643\n",
      "Epoch 55: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 697ms/step - loss: 0.0840 - val_loss: 7.7214 - learning_rate: 6.2500e-05\n",
      "Epoch 56/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668ms/step - loss: 0.0897\n",
      "Epoch 56: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 694ms/step - loss: 0.0806 - val_loss: 7.8352 - learning_rate: 6.2500e-05\n",
      "Epoch 57/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682ms/step - loss: 0.0626\n",
      "Epoch 57: val_loss did not improve from 7.33811\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 706ms/step - loss: 0.0812 - val_loss: 7.8448 - learning_rate: 6.2500e-05\n",
      "Epoch 58/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665ms/step - loss: 0.0545\n",
      "Epoch 58: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 688ms/step - loss: 0.0620 - val_loss: 7.8644 - learning_rate: 3.1250e-05\n",
      "Epoch 59/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 672ms/step - loss: 0.0603\n",
      "Epoch 59: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 695ms/step - loss: 0.0893 - val_loss: 7.8276 - learning_rate: 3.1250e-05\n",
      "Epoch 60/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661ms/step - loss: 0.0591\n",
      "Epoch 60: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 685ms/step - loss: 0.0779 - val_loss: 7.7861 - learning_rate: 3.1250e-05\n",
      "Epoch 61/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656ms/step - loss: 0.0713\n",
      "Epoch 61: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 680ms/step - loss: 0.0782 - val_loss: 7.8190 - learning_rate: 3.1250e-05\n",
      "Epoch 62/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656ms/step - loss: 0.0999\n",
      "Epoch 62: val_loss did not improve from 7.33811\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 680ms/step - loss: 0.0950 - val_loss: 7.8324 - learning_rate: 3.1250e-05\n",
      "Epoch 63/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656ms/step - loss: 0.0805\n",
      "Epoch 63: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 682ms/step - loss: 0.0762 - val_loss: 7.8124 - learning_rate: 1.5625e-05\n",
      "Epoch 64/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670ms/step - loss: 0.0983\n",
      "Epoch 64: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 693ms/step - loss: 0.0864 - val_loss: 7.7794 - learning_rate: 1.5625e-05\n",
      "Epoch 65/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 720ms/step - loss: 0.0964\n",
      "Epoch 65: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 764ms/step - loss: 0.0880 - val_loss: 7.7597 - learning_rate: 1.5625e-05\n",
      "Epoch 66/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709ms/step - loss: 0.0969\n",
      "Epoch 66: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 732ms/step - loss: 0.0976 - val_loss: 7.7146 - learning_rate: 1.5625e-05\n",
      "Epoch 67/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656ms/step - loss: 0.0710\n",
      "Epoch 67: val_loss did not improve from 7.33811\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 680ms/step - loss: 0.0593 - val_loss: 7.7112 - learning_rate: 1.5625e-05\n",
      "Epoch 68/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712ms/step - loss: 0.0867\n",
      "Epoch 68: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 755ms/step - loss: 0.0685 - val_loss: 7.7248 - learning_rate: 7.8125e-06\n",
      "Epoch 69/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 778ms/step - loss: 0.0685\n",
      "Epoch 69: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 822ms/step - loss: 0.0771 - val_loss: 7.7235 - learning_rate: 7.8125e-06\n",
      "Epoch 70/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696ms/step - loss: 0.0568\n",
      "Epoch 70: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 719ms/step - loss: 0.0643 - val_loss: 7.7300 - learning_rate: 7.8125e-06\n",
      "Epoch 71/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740ms/step - loss: 0.0613\n",
      "Epoch 71: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 767ms/step - loss: 0.0850 - val_loss: 7.7421 - learning_rate: 7.8125e-06\n",
      "Epoch 72/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737ms/step - loss: 0.0877\n",
      "Epoch 72: val_loss did not improve from 7.33811\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 762ms/step - loss: 0.0771 - val_loss: 7.7143 - learning_rate: 7.8125e-06\n",
      "Epoch 73/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686ms/step - loss: 0.0598\n",
      "Epoch 73: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 710ms/step - loss: 0.0728 - val_loss: 7.7246 - learning_rate: 3.9063e-06\n",
      "Epoch 74/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712ms/step - loss: 0.0902\n",
      "Epoch 74: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 735ms/step - loss: 0.1064 - val_loss: 7.7284 - learning_rate: 3.9063e-06\n",
      "Epoch 75/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710ms/step - loss: 0.0716\n",
      "Epoch 75: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 733ms/step - loss: 0.0735 - val_loss: 7.7203 - learning_rate: 3.9063e-06\n",
      "Epoch 76/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 671ms/step - loss: 0.0655\n",
      "Epoch 76: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 693ms/step - loss: 0.0801 - val_loss: 7.7290 - learning_rate: 3.9063e-06\n",
      "Epoch 77/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648ms/step - loss: 0.0567\n",
      "Epoch 77: val_loss did not improve from 7.33811\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 672ms/step - loss: 0.0698 - val_loss: 7.7326 - learning_rate: 3.9063e-06\n",
      "Epoch 78/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685ms/step - loss: 0.0607\n",
      "Epoch 78: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 707ms/step - loss: 0.0646 - val_loss: 7.7375 - learning_rate: 1.9531e-06\n",
      "Epoch 79/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651ms/step - loss: 0.0667\n",
      "Epoch 79: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 677ms/step - loss: 0.0689 - val_loss: 7.7408 - learning_rate: 1.9531e-06\n",
      "Epoch 80/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680ms/step - loss: 0.0467\n",
      "Epoch 80: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 703ms/step - loss: 0.0622 - val_loss: 7.7466 - learning_rate: 1.9531e-06\n",
      "Epoch 81/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668ms/step - loss: 0.0923\n",
      "Epoch 81: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 691ms/step - loss: 0.0707 - val_loss: 7.7422 - learning_rate: 1.9531e-06\n",
      "Epoch 82/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674ms/step - loss: 0.0789\n",
      "Epoch 82: val_loss did not improve from 7.33811\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 699ms/step - loss: 0.0727 - val_loss: 7.7426 - learning_rate: 1.9531e-06\n",
      "Epoch 83/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684ms/step - loss: 0.0615\n",
      "Epoch 83: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 708ms/step - loss: 0.0784 - val_loss: 7.7464 - learning_rate: 1.0000e-06\n",
      "Epoch 84/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686ms/step - loss: 0.0610\n",
      "Epoch 84: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 710ms/step - loss: 0.0643 - val_loss: 7.7507 - learning_rate: 1.0000e-06\n",
      "Epoch 85/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674ms/step - loss: 0.0490\n",
      "Epoch 85: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 717ms/step - loss: 0.0631 - val_loss: 7.7560 - learning_rate: 1.0000e-06\n",
      "Epoch 86/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675ms/step - loss: 0.0814\n",
      "Epoch 86: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 698ms/step - loss: 0.0715 - val_loss: 7.7569 - learning_rate: 1.0000e-06\n",
      "Epoch 87/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685ms/step - loss: 0.0640\n",
      "Epoch 87: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 707ms/step - loss: 0.0752 - val_loss: 7.7615 - learning_rate: 1.0000e-06\n",
      "Epoch 88/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674ms/step - loss: 0.0591\n",
      "Epoch 88: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 698ms/step - loss: 0.0643 - val_loss: 7.7603 - learning_rate: 1.0000e-06\n",
      "Epoch 89/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649ms/step - loss: 0.0839\n",
      "Epoch 89: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 672ms/step - loss: 0.0678 - val_loss: 7.7627 - learning_rate: 1.0000e-06\n",
      "Epoch 90/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651ms/step - loss: 0.0783\n",
      "Epoch 90: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 677ms/step - loss: 0.0657 - val_loss: 7.7638 - learning_rate: 1.0000e-06\n",
      "Epoch 91/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648ms/step - loss: 0.0853\n",
      "Epoch 91: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 671ms/step - loss: 0.0817 - val_loss: 7.7638 - learning_rate: 1.0000e-06\n",
      "Epoch 92/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649ms/step - loss: 0.0639\n",
      "Epoch 92: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 672ms/step - loss: 0.0733 - val_loss: 7.7624 - learning_rate: 1.0000e-06\n",
      "Epoch 93/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645ms/step - loss: 0.1238\n",
      "Epoch 93: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 667ms/step - loss: 0.1077 - val_loss: 7.7547 - learning_rate: 1.0000e-06\n",
      "Epoch 94/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645ms/step - loss: 0.1267\n",
      "Epoch 94: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 668ms/step - loss: 0.0843 - val_loss: 7.7552 - learning_rate: 1.0000e-06\n",
      "Epoch 95/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660ms/step - loss: 0.0625\n",
      "Epoch 95: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 682ms/step - loss: 0.0664 - val_loss: 7.7489 - learning_rate: 1.0000e-06\n",
      "Epoch 96/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665ms/step - loss: 0.0609\n",
      "Epoch 96: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 688ms/step - loss: 0.0743 - val_loss: 7.7427 - learning_rate: 1.0000e-06\n",
      "Epoch 97/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647ms/step - loss: 0.0856\n",
      "Epoch 97: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 670ms/step - loss: 0.0728 - val_loss: 7.7376 - learning_rate: 1.0000e-06\n",
      "Epoch 98/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662ms/step - loss: 0.0560\n",
      "Epoch 98: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 687ms/step - loss: 0.0686 - val_loss: 7.7439 - learning_rate: 1.0000e-06\n",
      "Epoch 99/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653ms/step - loss: 0.0511\n",
      "Epoch 99: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 676ms/step - loss: 0.0552 - val_loss: 7.7459 - learning_rate: 1.0000e-06\n",
      "Epoch 100/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676ms/step - loss: 0.0501\n",
      "Epoch 100: val_loss did not improve from 7.33811\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 701ms/step - loss: 0.0573 - val_loss: 7.7427 - learning_rate: 1.0000e-06\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 6.9258\n",
      "Test Loss: 6.925837993621826\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import optimizers, callbacks\n",
    "\n",
    "# Load the saved model if resuming, otherwise instantiate a new one\n",
    "try:\n",
    "    model = tf.keras.models.load_model(\n",
    "        \"lprnet_checkpoint.keras\",\n",
    "        custom_objects={\"ctc_loss\": ctc_loss},\n",
    "        compile=False\n",
    "    )\n",
    "    print(\"Loaded model from checkpoint.\")\n",
    "except:\n",
    "    print(\"No checkpoint found, creating new model.\")\n",
    "    model = LPRNet(num_classes=37, dropout_rate=0.5)  # Assuming LPRNet is defined\n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "    loss=ctc_loss\n",
    ")\n",
    "\n",
    "# Define callbacks\n",
    "checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "    filepath=\"lprnet_checkpoint.keras\",\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode=\"min\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "reduce_lr_callback = callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,  # Reduce learning rate by half\n",
    "    patience=5,  # Wait 5 epochs before reducing\n",
    "    min_lr=1e-6,  # Minimum learning rate\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model with callbacks\n",
    "history = model.fit(\n",
    "    train_ds,  # Assuming train_ds is defined\n",
    "    validation_data=val_ds,  # Assuming val_ds is defined\n",
    "    epochs=100,\n",
    "    initial_epoch=0,  # Set to the epoch you want to start from if resuming\n",
    "    callbacks=[checkpoint_callback, reduce_lr_callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate on test dataset\n",
    "test_loss = model.evaluate(test_ds)  # Assuming test_ds is defined\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "# Save the final model\n",
    "model.save(\"lprnet_model_final.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e302a3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted License Plate: MWWK908\n"
     ]
    }
   ],
   "source": [
    "# Character mapping (adjust based on your dataset)\n",
    "# Example: 0=blank, 1-10=digits 0-9, 11-36=A-Z\n",
    "char_map = {0: '', 1: '0', 2: '1', 3: '2', 4: '3', 5: '4', 6: '5', 7: '6', 8: '7', 9: '8', 10: '9',\n",
    "            11: 'A', 12: 'B', 13: 'C', 14: 'D', 15: 'E', 16: 'F', 17: 'G', 18: 'H', 19: 'I', 20: 'J',\n",
    "            21: 'K', 22: 'L', 23: 'M', 24: 'N', 25: 'O', 26: 'P', 27: 'Q', 28: 'R', 29: 'S', 30: 'T',\n",
    "            31: 'U', 32: 'V', 33: 'W', 34: 'X', 35: 'Y', 36: 'Z'}\n",
    "\n",
    "# Function to preprocess a single image\n",
    "def preprocess_image(image_path):\n",
    "    # Load image (assuming it's a file path; adjust if image is a numpy array)\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)  # Normalize to [0, 1]\n",
    "    img = tf.image.resize(img, [24, 94])  # Resize to match model input\n",
    "    img = tf.expand_dims(img, axis=0)  # Add batch dimension: (1, 24, 94, 3)\n",
    "    return img\n",
    "\n",
    "# Function to decode CTC output\n",
    "def decode_ctc_output(logits, char_map):\n",
    "    # Reduce mean over height to get (B, 71, 37)\n",
    "    logits = tf.reduce_max(logits, axis=1)  # Shape: (1, 71, 37)\n",
    "    \n",
    "    # CTC greedy decoder\n",
    "    decoded, _ = tf.nn.ctc_beam_search_decoder(\n",
    "        inputs=tf.transpose(logits, perm=[1, 0, 2]),  # Time-major: (71, 1, 37)\n",
    "        sequence_length=tf.ones([1], dtype=tf.int32) * logits.shape[1],\n",
    "        # blank_index=0\n",
    "        beam_width=10\n",
    "    )\n",
    "    \n",
    "    # Convert sparse tensor to dense and extract the sequence\n",
    "    decoded_sequence = tf.sparse.to_dense(decoded[0]).numpy()[0]\n",
    "    \n",
    "    # Map indices to characters\n",
    "    prediction = ''.join([char_map.get(int(idx), '') for idx in decoded_sequence])\n",
    "    return prediction\n",
    "\n",
    "# Function to predict license plate from an image\n",
    "def predict_license_plate(image_path, model_path=\"lprnet_checkpoint.keras\"):\n",
    "    # Load the model\n",
    "    model = tf.keras.models.load_model(\n",
    "        model_path,\n",
    "        custom_objects={\"ctc_loss\": ctc_loss},\n",
    "        compile=False\n",
    "    )\n",
    "    \n",
    "    # Preprocess the image\n",
    "    img = preprocess_image(image_path)\n",
    "    \n",
    "    # Get model prediction\n",
    "    logits = model(img)  # Shape: (1, 4, 71, 37)\n",
    "    \n",
    "    # Decode the prediction\n",
    "    prediction = decode_ctc_output(logits, char_map)\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example image path\n",
    "    image_path = r\"lprds\\test\\WANNE685.JPG\"  # Replace with actual image path\n",
    "    try:\n",
    "        predicted_plate = predict_license_plate(image_path, model_path=\"lprnet_checkpoint.keras\")\n",
    "        print(f\"Predicted License Plate: {predicted_plate}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting license plate: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4cf953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b744597",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lprnet-v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
